{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataAsTensor():\n",
    "    # Assume we only deal with 6 possible notes: C, D, E, F, G, and _\n",
    "    # correspond to a one hot array with value 1 at indexes 0, 1, 2, 3, 4, 5\n",
    "    # Tempo consists of 1, 2, 3, 4\n",
    "\n",
    "    k = torch.LongTensor([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\n",
    "    p1 = torch.LongTensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3])\n",
    "    p2 = torch.LongTensor([4, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5])\n",
    "    p3 = torch.LongTensor([5, 2, 5, 2, 5, 2, 5, 2, 5, 2, 5, 2])\n",
    "    p4 = torch.LongTensor([0, 5, 0, 5, 0, 5, 0, 5, 0, 5, 0, 5])\n",
    "    tempo = torch.LongTensor([1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4])\n",
    "    \n",
    "    # Turn notes into onehot numpy vectors\n",
    "    x1 = k[p1]\n",
    "    x2 = k[p2]\n",
    "    x3 = k[p3]\n",
    "    x4 = k[p4]\n",
    "    \n",
    "    return x1, x2, x3, x4, tempo\n",
    "\n",
    "# loadDataAsTensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x2, x3, x4, tempo, hidden):\n",
    "        temp = torch.cat((x2[0].float(), x3[0].float(), x4[0].float(), tempo.float(), hidden[0].float()), 0)\n",
    "        input_combined = temp.view(1, -1).float()\n",
    "        \n",
    "        hidden_i = self.i2h(input_combined)\n",
    "        hidden_i = self.tanh(hidden_i)\n",
    "\n",
    "        output = self.h2o(hidden_i)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden_i\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we are predicting p1. Hence the training data includes p2, p3, p4, and tempo\n",
    "\n",
    "def getTrainingDataTensor(idx, x1, x2, x3, x4, tempo):\n",
    "    x1_tensor = x1[idx, :].view(1, -1)\n",
    "    x2_tensor = x2[idx, :].view(1, -1)\n",
    "    x3_tensor = x3[idx, :].view(1, -1)\n",
    "    x4_tensor = x4[idx, :].view(1, -1)\n",
    "    tempo_tensor = torch.zeros(1).long()\n",
    "    tempo_tensor.fill_(tempo[idx]).view(1, -1)\n",
    "    \n",
    "    return x2_tensor, x3_tensor, x4_tensor, tempo_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTargetDataTensor(idx, x1):\n",
    "    x1_tensor = x1[idx, :]\n",
    "    \n",
    "    correct_index_as_tensor = (x1_tensor==1).nonzero()[0]\n",
    "    \n",
    "    return correct_index_as_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x2_tensor, x3_tensor, x4_tensor, tempo_tensor, target_tensor, hidden_tensor, optimizer):\n",
    "        \n",
    "    output, hidden_i = rnn(x2_tensor, x3_tensor, x4_tensor, tempo_tensor, hidden_tensor)\n",
    "\n",
    "    loss = F.cross_entropy(output, target_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "        \n",
    "    return output, hidden_i, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.LongTensor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/cs682project/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 0s (99 4%) loss: 1.7567\n",
      "output: tensor([[0.1323, 0.1915, 0.1700, 0.2020, 0.1465, 0.1577]], grad_fn=<SoftmaxBackward>)\n",
      "0m 1s (199 9%) loss: 1.7129\n",
      "output: tensor([[0.1300, 0.1987, 0.1791, 0.2466, 0.1178, 0.1278]], grad_fn=<SoftmaxBackward>)\n",
      "0m 3s (299 14%) loss: 1.6597\n",
      "output: tensor([[0.1249, 0.1998, 0.1815, 0.3013, 0.0920, 0.1004]], grad_fn=<SoftmaxBackward>)\n",
      "0m 6s (399 19%) loss: 1.5988\n",
      "output: tensor([[0.1175, 0.1949, 0.1761, 0.3648, 0.0701, 0.0766]], grad_fn=<SoftmaxBackward>)\n",
      "0m 10s (499 24%) loss: 1.5363\n",
      "output: tensor([[0.1090, 0.1864, 0.1637, 0.4308, 0.0528, 0.0573]], grad_fn=<SoftmaxBackward>)\n",
      "0m 14s (599 29%) loss: 1.4801\n",
      "output: tensor([[0.0999, 0.1794, 0.1469, 0.4909, 0.0400, 0.0429]], grad_fn=<SoftmaxBackward>)\n",
      "0m 19s (699 34%) loss: 1.4314\n",
      "output: tensor([[0.0864, 0.1798, 0.1269, 0.5440, 0.0306, 0.0323]], grad_fn=<SoftmaxBackward>)\n",
      "0m 24s (799 39%) loss: 1.3816\n",
      "output: tensor([[0.0603, 0.1919, 0.1014, 0.5992, 0.0233, 0.0238]], grad_fn=<SoftmaxBackward>)\n",
      "0m 30s (899 44%) loss: 1.3234\n",
      "output: tensor([[0.0307, 0.1999, 0.0705, 0.6651, 0.0170, 0.0168]], grad_fn=<SoftmaxBackward>)\n",
      "0m 37s (999 49%) loss: 1.2450\n",
      "output: tensor([[0.0159, 0.1597, 0.0462, 0.7548, 0.0120, 0.0116]], grad_fn=<SoftmaxBackward>)\n",
      "0m 45s (1099 54%) loss: 1.1618\n",
      "output: tensor([[0.0100, 0.0901, 0.0310, 0.8528, 0.0082, 0.0078]], grad_fn=<SoftmaxBackward>)\n",
      "0m 54s (1199 59%) loss: 1.1109\n",
      "output: tensor([[0.0072, 0.0450, 0.0218, 0.9151, 0.0057, 0.0054]], grad_fn=<SoftmaxBackward>)\n",
      "1m 3s (1299 64%) loss: 1.0871\n",
      "output: tensor([[0.0054, 0.0255, 0.0161, 0.9447, 0.0042, 0.0040]], grad_fn=<SoftmaxBackward>)\n",
      "1m 13s (1399 69%) loss: 1.0751\n",
      "output: tensor([[0.0043, 0.0168, 0.0126, 0.9599, 0.0033, 0.0031]], grad_fn=<SoftmaxBackward>)\n",
      "1m 24s (1499 74%) loss: 1.0680\n",
      "output: tensor([[0.0036, 0.0122, 0.0102, 0.9688, 0.0027, 0.0026]], grad_fn=<SoftmaxBackward>)\n",
      "1m 35s (1599 79%) loss: 1.0635\n",
      "output: tensor([[0.0031, 0.0094, 0.0085, 0.9745, 0.0023, 0.0022]], grad_fn=<SoftmaxBackward>)\n",
      "1m 46s (1699 84%) loss: 1.0603\n",
      "output: tensor([[0.0027, 0.0076, 0.0072, 0.9786, 0.0020, 0.0019]], grad_fn=<SoftmaxBackward>)\n",
      "1m 58s (1799 89%) loss: 1.0580\n",
      "output: tensor([[0.0024, 0.0063, 0.0063, 0.9815, 0.0018, 0.0017]], grad_fn=<SoftmaxBackward>)\n",
      "2m 10s (1899 94%) loss: 1.0562\n",
      "output: tensor([[0.0021, 0.0054, 0.0055, 0.9838, 0.0016, 0.0015]], grad_fn=<SoftmaxBackward>)\n",
      "2m 22s (1999 99%) loss: 1.0548\n",
      "output: tensor([[0.0019, 0.0047, 0.0049, 0.9856, 0.0015, 0.0014]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "input_size = 6*3+1\n",
    "hidden_size = 100\n",
    "output_size = 6\n",
    "sequence_size = 12\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "n_iters = 2000\n",
    "print_every = 100\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "hidden = rnn.initHidden()\n",
    "print(hidden.type())\n",
    "\n",
    "tx1, tx2, tx3, tx4, t_tempo = loadDataAsTensor()\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "ite = 0\n",
    "for n in range(n_iters):\n",
    "    if ite/12 >= 1:\n",
    "        ite = 0\n",
    "            \n",
    "    x2, x3, x4, tempo = getTrainingDataTensor(ite, tx1, tx2, tx3, tx4, t_tempo)\n",
    "    target = getTargetDataTensor(ite, tx1)\n",
    "\n",
    "    output, hidden, loss = train(x2, x3, x4, tempo, target, hidden, optimizer)\n",
    "    total_loss += loss\n",
    "    \n",
    "    if (n+1) % print_every == 0:\n",
    "            print('%s (%d %d%%) loss: %.4f' % (timeSince(start), n, n / n_iters * 100, loss))\n",
    "            print(\"output: {x}\".format(x=output))\n",
    "            \n",
    "    if (n+1) % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0\n",
    "    \n",
    "    ite += 1\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToNote(idx):\n",
    "    if idx == 0:\n",
    "        return 'C'\n",
    "    elif idx == 1:\n",
    "        return 'D'\n",
    "    elif idx == 2:\n",
    "        return 'E'\n",
    "    elif idx == 3:\n",
    "        return 'F'\n",
    "    elif idx == 4:\n",
    "        return 'G'\n",
    "    elif idx == '5':\n",
    "        return \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(length):\n",
    "    tx1, tx2, tx3, tx4, t_tempo = loadDataAsTensor()\n",
    "    print(t_tempo)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.initHidden()\n",
    "        \n",
    "        k = 0\n",
    "        for i in range(length):\n",
    "            if k / 12 >= 1:\n",
    "                k = 0\n",
    "            x2, x3, x4, tempo = getTrainingDataTensor(k, tx1, tx2, tx3, tx4, t_tempo)\n",
    "            output, hidden = rnn(x2, x3, x4, tempo, hidden)\n",
    "            \n",
    "            result = np.random.choice(range(6), p=output.numpy()[0])\n",
    "            print(convertToNote(result))\n",
    "            \n",
    "            k += 1\n",
    "            i += 1\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4])\n",
      "E\n",
      "D\n",
      "F\n",
      "F\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/cs682project/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
