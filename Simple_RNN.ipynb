{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla RNN implementation. Created by following parts of the implementation from: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "# print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_dim = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "weight_scale = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _onehot(size):\n",
    "        return np.zeros((size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Whh = np.random.randn(hidden_dim, hidden_dim) * weight_scale\n",
    "Whx = np.random.randn(hidden_dim, vocab_size) * weight_scale\n",
    "Wyh = np.random.randn(vocab_size, hidden_dim) * weight_scale\n",
    "bh = np.zeros((hidden_dim, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossPlease(X, y, h_prev):\n",
    "        h, x, yhat, out = {}, {}, {}, {}\n",
    "        loss = 0\n",
    "        h[-1] = h_prev\n",
    "        \n",
    "        # forward pass:\n",
    "        for i in range(len(X)):\n",
    "            # xt DIM = (D, 1)\n",
    "            x[i] = _onehot(vocab_size)\n",
    "            x[i][X[i]] = 1\n",
    "            \n",
    "            # h DIM = (H, 1)\n",
    "            h_value = np.dot(Whh, h[i-1]) + np.dot(Whx, x[i]) + bh\n",
    "            h[i] = np.tanh(h_value)\n",
    "\n",
    "            # y DIM = (D, 1)\n",
    "            yhat[i] = np.dot(Wyh, h[i]) + by\n",
    "            \n",
    "            # out DIM = (D, 1) -> normalized probabilities\n",
    "            out[i] = np.exp(yhat[i]) / np.sum(np.exp(yhat[i]))\n",
    "            loss += -np.log(out[i][y[i], 0])\n",
    "            \n",
    "#             print(\"loss: {x}\".format(x=loss))\n",
    "        \n",
    "        # backward pass:\n",
    "        dl_dWhh, dl_dWhx, dl_dWyh = np.zeros_like(Whh), np.zeros_like(Whx), np.zeros_like(Wyh)\n",
    "        dl_dbh, dl_dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "        dl_dhpassdown = np.zeros(h[0].shape)\n",
    "        \n",
    "        for j in reversed(range(len(X))):\n",
    "            # backprop through softmax\n",
    "            dout_j = np.copy(out[j])\n",
    "            dout_j[y[j]] -= 1\n",
    "            \n",
    "            dl_dWyh += np.dot(dout_j, h[j].T)\n",
    "            dl_dby += dout_j\n",
    "            \n",
    "            dl_dhj = np.dot(Wyh.T, dout_j) + dl_dhpassdown\n",
    "            dl_dtanh = (1 - h[i] * h[i]) * dl_dhj\n",
    "            \n",
    "            dl_dWhh += np.dot(dl_dtanh, h[j-1].T)\n",
    "            dl_dWhx += np.dot(dl_dtanh, x[j].T)\n",
    "            dl_dbh += dl_dtanh\n",
    "            dl_dhpassdown = np.dot(Whh.T, dl_dtanh)\n",
    "        \n",
    "        # Clip values to mitigate exploding gradients\n",
    "        for dparam in [dl_dWhh, dl_dWhx, dl_dWyh, dl_dbh, dl_dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "            \n",
    "        return loss, dl_dWhh, dl_dWhx, dl_dWyh, dl_dbh, dl_dby, h[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):        \n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Whx, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Wyh, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ell helol eeell eeoo  helol heol  helol eeool heel  eelooheeooleheelh heloo eeool  eell eelo  eeool heol heelo  eelolheeolh eelolheeoolhheooo eeoo leelll  eelohhelo  eeool  eoloheelll eelolhheoo  eell \n",
      "----\n",
      "----\n",
      " ehhelhhllhooohllohlheheheh hhoo hl loheeoloel l loeo leoo lolllohho lol hlholo  lohoh llleolhhehleoelhoe le e leoolellooolohl llllleeoohll l lhllll lh ollehelloleohlololeeeeoeeeleelolh llllh olhelh ol \n",
      "----\n",
      "----\n",
      " ollololhh ole hlell  ooll  lehellhoolllllllllo lllelellelll llellhllllloe hl lle lllll llhelll l he olllhll lleehlohhlloelhlllo e l ollel ll lllo  l e oll lllolh h leoehlole lle lellhe   loell lll ole \n",
      "----\n",
      "----\n",
      " ool hhlhehle le lh heeloo ol  lhllll le loheohehloee ooo he elleooh heehl ho hlh ool olhlhlhel eolehh llelehhoollhooehlhlelooelehh eol hh llllholh lllelool hlloooooehl  olhol oehloeholhlhleoleee heo   \n",
      "----\n",
      "----\n",
      " ll e   heh eoll  hholeell l llhooll leoohoool l  eloeleeloollo hol l o ho lloooeol hll oolh oo hohoeeoh ooh ooelhhh o helhe ooo olle lhoeo ol  ellle lhlhlelhl l lll eoleoeleolheo lelleolleolllh  llhlh \n",
      "----\n",
      "----\n",
      "  o e o elhhho e elhh oe h he l  helollelhle lhholho e  e  eo o e he o elohoo eo ho elo elhhhoel ho elellhe e hholhh elelo elheo e ellhholhee elhelhe  hl  oe lelel  ee  h h le o lheo lhllhlo o   lhe el \n",
      "----\n",
      "----\n",
      "  hhelhlhooelllloelel  hl o h eeloeoellhehhlheeleel llool h olhhlolh  lolhhooel leoel llh hlellh ellloel helhehh hhoell eelh he hhlllel o  oelhloeh lhoh oleoll olh hhhel oelheolheohhlloeleloelhlohhlhel \n",
      "----\n",
      "----\n",
      " o leheo ell loeo lheheoooeo lol l l loeeohlllllleeohhheohoehlellelloe e eeloeeo llleoo llohoehe lo eoelheh oeeoeeell eehllh ololhleohllollehlholllleeollo hlleloe h l loehool ll elol ll ohhehololelohoe \n",
      "----\n",
      "----\n",
      " he o lohhhlhholo ho e lo oellellello e olo eolhle lhe hlhelll lho o e o hel h o llelo elolo hoolo lo hlo olo loolhelolo le o hhlhlo o lle o ollhllo hhlhhe hhhhh o helele lo hel lo o lhellholhhhellhhho \n",
      "----\n",
      "----\n",
      " elhhlele hello o lhhlhollo lh ollhelhelholll lhelllo o hlo he o oollllhollo lhhhelllhelhehehelhehlhehehlh  helelo lhollo lheellllhelo o ellelhlolh lhellllhlhllhelo hello hellllo lo lo o helllheo o lo  \n",
      "----\n",
      "----\n",
      " oll llllle   llhleleo eollheeheol lolol lelo eoelolelhloohl e eloollloo lleeo  ollleh lllhellhho eoeoooho  lllloollehhlolehloe oeole   elelheheol h eeolllll eh lelohellh lloehl llleeoooo ooeollh llolh \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "            \n",
    "mWhx, mWhh, mWyh = np.zeros_like(Whx), np.zeros_like(Whh), np.zeros_like(Wyh)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "\n",
    "while True:\n",
    "    if p + seq_length + 1 >= len(data) or n ==0:\n",
    "        hprev = np.zeros((hidden_dim,1))\n",
    "        p=0\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "#     print(\"input: {x}\".format(x =inputs))\n",
    "#     print(\"targets: {x}\".format(x =targets))\n",
    "\n",
    "    if n % 100 == 0:\n",
    "        sample_text = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_text)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    loss, dWhh, dWhx, dWyh, dbh, dby, hprev = lossPlease(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Whx, Whh, Wyh, bh, by], \n",
    "                                                                [dWhx, dWhh, dWyh, dbh, dby], \n",
    "                                                                [mWhx, mWhh, mWyh, mbh, mby]):\n",
    "        \n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
