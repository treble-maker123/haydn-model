{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from util.helpers import *\n",
    "from util.run import train, validate\n",
    "from util.sample import sample\n",
    "from util.dataset import HaydnDataset, ChunksDataset\n",
    "from util.models import PitchEmbedModel, HarmonyModel, JudgeModel, NoteModel\n",
    "\n",
    "from music21 import converter\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "# number of instrument parts\n",
    "NUM_PARTS = 4\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset...\n",
      "Serialized scores found, loading...\n",
      "Scores loaded in 20.69 seconds.\n"
     ]
    }
   ],
   "source": [
    "# SETUP DATA\n",
    "\n",
    "SEQ_LEN = 32\n",
    "STRIDE = 2\n",
    "BATCH_SIZE = {\n",
    "    \"train\": 1024,\n",
    "    \"val\": 1024\n",
    "}\n",
    "LOADER_PARAMS = {\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": multiprocessing.cpu_count() - 2\n",
    "}\n",
    "TRANSFORMS = []\n",
    "# how mnay pieces to allocate to validation, note that pieces have different length of chunks, so \n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "SKIP_DATA = False\n",
    "\n",
    "if not SKIP_DATA:\n",
    "    haydn_dataset = HaydnDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 73 pieces and 819624 chunks in training set,and 8 pieces and 166933 chunks in validation set\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_DATA:\n",
    "    data_train = ChunksDataset(mode=\"train\",\n",
    "                               seq_len=SEQ_LEN, \n",
    "                               stride=STRIDE, \n",
    "                               dataset=haydn_dataset,\n",
    "                               transforms=TRANSFORMS,\n",
    "                               val_split=VALIDATION_SPLIT)\n",
    "    data_val = ChunksDataset(dataset=data_train.comp_set,\n",
    "                             transforms=TRANSFORMS)\n",
    "\n",
    "    loader_train = DataLoader(data_train,\n",
    "                              batch_size=BATCH_SIZE[\"train\"],\n",
    "                              **LOADER_PARAMS)\n",
    "    loader_val = DataLoader(data_val,\n",
    "                            batch_size=BATCH_SIZE[\"val\"],\n",
    "                            **LOADER_PARAMS)\n",
    "    \n",
    "    print(\"There are {} pieces and {} chunks in training set,\".format(len(data_train.dataset), len(data_train)) +\n",
    "          \"and {} pieces and {} chunks in validation set\".format(len(data_val.dataset), len(data_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "\n",
    "# number of epochs to run\n",
    "NUM_EPOCHS = 20\n",
    "# number of dimensions for the embedded pitch vectors\n",
    "EMBED_DIM = 5\n",
    "# dimension of the rhythm\n",
    "RHYTHM_DIM = 1\n",
    "# the total number of pitches plus rest\n",
    "PITCH_VOCAB_SIZE = 140\n",
    "# parameters for the optimizers\n",
    "OPTIM_PARAMS = {\n",
    "    \"lr\": 1e-2,\n",
    "    \"weight_decay\": 0.0\n",
    "}\n",
    "\n",
    "# weights applied to each of the loss functions\n",
    "# forward pitch\n",
    "fp_loss = 0.5\n",
    "# backward pitch\n",
    "bp_loss = 0.3\n",
    "# harmony pitch\n",
    "hp_loss = 0.3\n",
    "# foward rhythm\n",
    "fr_loss = 1.0\n",
    "# judge\n",
    "j_loss = 1.0\n",
    "# part\n",
    "p_loss = 0.1\n",
    "\n",
    "LOSS_WEIGHTS = [fp_loss, bp_loss, hp_loss, fr_loss, j_loss, p_loss]\n",
    "\n",
    "print(\"Hyperparameter loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS AND OPTIMIZERS\n",
    "\n",
    "SKIP_MODELS = False\n",
    "\n",
    "if not SKIP_MODELS:\n",
    "    model_names = [\"forward_\", \"backward_\", \"harmony_\", \"judge_\"]\n",
    "\n",
    "    models = {\n",
    "        \"pitch_embed\": PitchEmbedModel(vocab_size=PITCH_VOCAB_SIZE,\n",
    "                                       embed_dim=EMBED_DIM)\n",
    "    }\n",
    "    optims = {}\n",
    "\n",
    "    for i in range(NUM_PARTS):\n",
    "        note_input_dim = EMBED_DIM + RHYTHM_DIM\n",
    "        note_hidden_dim = 256\n",
    "        note_num_layers = 1\n",
    "        models[model_names[0] + str(i)] = NoteModel(note_input_dim, \n",
    "                                                    note_hidden_dim,\n",
    "                                                    batch_size=BATCH_SIZE['train'],\n",
    "                                                    num_layers=note_num_layers,\n",
    "                                                    vocab_size=PITCH_VOCAB_SIZE)\n",
    "\n",
    "        models[model_names[1] + str(i)] = NoteModel(note_input_dim, \n",
    "                                                    note_hidden_dim,\n",
    "                                                    batch_size=BATCH_SIZE['train'],\n",
    "                                                    num_layers=note_num_layers,\n",
    "                                                    vocab_size=PITCH_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "        harmony_input_shape = (NUM_PARTS, EMBED_DIM + NUM_PARTS)\n",
    "        harmony_hidden_dim = 4 # should be less than 9\n",
    "        models[model_names[2] + str(i)] = HarmonyModel(input_shape=harmony_input_shape,\n",
    "                                                       vocab_size=PITCH_VOCAB_SIZE,\n",
    "                                                       hidden_dim=harmony_hidden_dim)\n",
    "\n",
    "\n",
    "        judge_input_shape = (NUM_PARTS - 1, EMBED_DIM)\n",
    "        judge_hidden_dim = 128\n",
    "        output_dim = PITCH_VOCAB_SIZE\n",
    "        models[model_names[3] + str(i)] = JudgeModel(judge_input_shape,\n",
    "                                                     judge_hidden_dim,\n",
    "                                                     output_dim)\n",
    "\n",
    "        # jointly optimize all of the params, so weights can be assigned to different loss.\n",
    "        embed_params = list(models[\"pitch_embed\"].parameters())\n",
    "        forward_params = list(models[model_names[0] + str(i)].parameters())\n",
    "        backward_params = list(models[model_names[1] + str(i)].parameters())\n",
    "        harmony_params = list(models[model_names[2] + str(i)].parameters())\n",
    "        judge_params = list(models[model_names[3] + str(i)].parameters())\n",
    "        optims[i] = optim.Adam(forward_params + backward_params +\n",
    "                               harmony_params + judge_params, \n",
    "                               **OPTIM_PARAMS)\n",
    "\n",
    "    # send all models to the appropriate device\n",
    "    for key in models:\n",
    "        models[key].to(device=device)\n",
    "        \n",
    "    print(\"Models loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS BLOCK CLEAR GPU CACHE\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN LOOP\n",
    "\n",
    "SKIP_TRAIN = False\n",
    "\n",
    "if not SKIP_TRAIN:\n",
    "    train_stats = []\n",
    "    val_stats = []\n",
    "    saved_models = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"EPOCH {}\".format(epoch))\n",
    "        print(\"-----------\")\n",
    "        stats, models = train(models, optims, loader_train, \n",
    "                              model_names=model_names, \n",
    "                              loss_weights=LOSS_WEIGHTS,\n",
    "                              device=device,\n",
    "                              print_iter=100)\n",
    "\n",
    "        stats, models = validate(models, loader_val,\n",
    "                                 model_names=model_names,\n",
    "                                 device=device,\n",
    "                                 print_iter=100)\n",
    "\n",
    "        print(\"-----------\")\n",
    "        print(\"Completed epoch {}.\".format(epoch))\n",
    "        print(\"\")\n",
    "        train_stats.append(stats)\n",
    "        val_stats.append(stats)\n",
    "        saved_models.append(copy.deepcopy(models))\n",
    "\n",
    "\n",
    "    print(\"Training completed! Saving files.\")\n",
    "\n",
    "    # create a folder to store all of the stats and models\n",
    "    mkdir(OUTPUT_PATH)\n",
    "    stats_file_name = get_formatted_time() + \"_\" + get_unique_id() + \".stat\"\n",
    "    stats_file_path = OUTPUT_PATH + \"/\" + stats_file_name\n",
    "    models_file_name = get_formatted_time() + \"_\" + get_unique_id() + \".models\"\n",
    "    models_file_path = OUTPUT_PATH + \"/\" + models_file_name\n",
    "\n",
    "    with open(stats_file_path, \"wb\") as file:\n",
    "        pickle.dump((train_stats, val_stats), file)\n",
    "    with open(models_file_path, \"wb\") as file:\n",
    "        pickle.dump(saved_models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLING\n",
    "\n",
    "if not \"models\" in vars() or models is None or len(models) < 2:\n",
    "    mkdir(OUTPUT_PATH)\n",
    "    # file location, update to point to the correct file\n",
    "    MODELS_FILE_PATH = OUTPUT_PATH + \"/12-09_03-06-47_04D328.models\"\n",
    "    # which epoch's models to use\n",
    "    NTH_EPOCH = 0\n",
    "    \n",
    "    with open(MODELS_FILE_PATH, \"rb\") as file:\n",
    "        saved_models = pickle.load(file)\n",
    "        models = saved_models[NTH_EPOCH]\n",
    "        \n",
    "# how many ticks to sample, 16 ticks ~ 1 measure of music\n",
    "NUM_TICKS_TO_SAMPLE = 256\n",
    "# number of iterations to repeat the sampling process, one iteration\n",
    "# will run for NUM_PARTS * NUM_TICKS_TO_SAMPLE times.\n",
    "NUM_REPEATS = 1\n",
    "        \n",
    "output = sample(models, \n",
    "                num_parts=NUM_PARTS,\n",
    "                num_ticks=NUM_TICKS_TO_SAMPLE,\n",
    "                num_dims=PITCH_VOCAB_SIZE,\n",
    "                seq_len=SEQ_LEN,\n",
    "                num_repeats=NUM_REPEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = haydn_dataset.matrix_to_score(output)\n",
    "\n",
    "SAVING = True\n",
    "\n",
    "if SAVING:\n",
    "    file_name = get_unique_id() + \"_\" + get_formatted_time() + \".pgz\"\n",
    "    mkdir(SAMPLE_PATH)\n",
    "    output_path = SAMPLE_PATH + \"/\" + file_name\n",
    "    converter.freeze(score, fp=output_path)\n",
    "    \n",
    "# converter.thaw(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
