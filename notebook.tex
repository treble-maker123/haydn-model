
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{runbook}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{copy}
        \PY{k+kn}{import} \PY{n+nn}{pickle}
        \PY{k+kn}{import} \PY{n+nn}{multiprocessing}
        
        \PY{k+kn}{import} \PY{n+nn}{torch}
        \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
        \PY{k+kn}{import} \PY{n+nn}{torchvision}\PY{n+nn}{.}\PY{n+nn}{transforms} \PY{k}{as} \PY{n+nn}{T}
        \PY{k+kn}{from} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{utils}\PY{n+nn}{.}\PY{n+nn}{data} \PY{k}{import} \PY{n}{DataLoader}
        
        \PY{k+kn}{from} \PY{n+nn}{util}\PY{n+nn}{.}\PY{n+nn}{helpers} \PY{k}{import} \PY{o}{*}
        \PY{k+kn}{from} \PY{n+nn}{util}\PY{n+nn}{.}\PY{n+nn}{run} \PY{k}{import} \PY{n}{train}\PY{p}{,} \PY{n}{validate}
        \PY{k+kn}{from} \PY{n+nn}{util}\PY{n+nn}{.}\PY{n+nn}{sample} \PY{k}{import} \PY{n}{sample}
        \PY{k+kn}{from} \PY{n+nn}{util}\PY{n+nn}{.}\PY{n+nn}{dataset} \PY{k}{import} \PY{n}{HaydnDataset}\PY{p}{,} \PY{n}{ChunksDataset}
        \PY{k+kn}{from} \PY{n+nn}{util}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{PitchEmbedModel}\PY{p}{,} \PY{n}{HarmonyModel}\PY{p}{,} \PY{n}{JudgeModel}\PY{p}{,} \PY{n}{NoteModel}
        
        \PY{k+kn}{from} \PY{n+nn}{music21} \PY{k}{import} \PY{n}{converter}
        
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k}{import} \PY{n}{pyplot} \PY{k}{as} \PY{n}{plt}
        
        \PY{o}{\PYZpc{}}\PY{k}{load\PYZus{}ext} autoreload
        \PY{o}{\PYZpc{}}\PY{k}{autoreload} 2
        
        \PY{k}{if} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{is\PYZus{}available}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cuda}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{n}{torch}\PY{o}{.}\PY{n}{set\PYZus{}default\PYZus{}tensor\PYZus{}type}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{torch.cuda.FloatTensor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cpu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            
        \PY{c+c1}{\PYZsh{} number of instrument parts}
        \PY{n}{NUM\PYZus{}PARTS} \PY{o}{=} \PY{l+m+mi}{4}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        
        \PY{n}{RUN\PYZus{}ID} \PY{o}{=} \PY{n}{get\PYZus{}unique\PYZus{}id}\PY{p}{(}\PY{p}{)}
        \PY{n}{RUN\PYZus{}TIME} \PY{o}{=} \PY{n}{get\PYZus{}formatted\PYZus{}time}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
cuda

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{} LOAD HAYDN DATASET}
        
        \PY{n}{SKIP\PYZus{}DATA} \PY{o}{=} \PY{k+kc}{False}
        
        \PY{k}{if} \PY{o+ow}{not} \PY{n}{SKIP\PYZus{}DATA}\PY{p}{:}
            \PY{n}{haydn\PYZus{}dataset} \PY{o}{=} \PY{n}{HaydnDataset}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Building dataset{\ldots}
Serialized scores found, loading{\ldots}
Scores loaded in 19.81 seconds.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} SETUP DATA LOADERs}
        
        \PY{n}{SEQ\PYZus{}LEN} \PY{o}{=} \PY{l+m+mi}{32}
        \PY{n}{STRIDE} \PY{o}{=} \PY{l+m+mi}{2}
        \PY{n}{BATCH\PYZus{}SIZE} \PY{o}{=} \PY{p}{\PYZob{}}
            \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{1024}\PY{p}{,}
            \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{val}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{1024}
        \PY{p}{\PYZcb{}}
        \PY{n}{LOADER\PYZus{}PARAMS} \PY{o}{=} \PY{p}{\PYZob{}}
            \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shuffle}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{k+kc}{True}\PY{p}{,}
            \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{num\PYZus{}workers}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{multiprocessing}\PY{o}{.}\PY{n}{cpu\PYZus{}count}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{2}
        \PY{p}{\PYZcb{}}
        \PY{n}{TRANSFORMS} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{c+c1}{\PYZsh{} how mnay pieces to allocate to validation, note that pieces have different length of chunks, so }
        \PY{n}{VALIDATION\PYZus{}SPLIT} \PY{o}{=} \PY{l+m+mf}{0.1}
        
        \PY{k}{if} \PY{o+ow}{not} \PY{n}{SKIP\PYZus{}DATA}\PY{p}{:}
            \PY{n}{data\PYZus{}train} \PY{o}{=} \PY{n}{ChunksDataset}\PY{p}{(}\PY{n}{mode}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                                       \PY{n}{seq\PYZus{}len}\PY{o}{=}\PY{n}{SEQ\PYZus{}LEN}\PY{p}{,} 
                                       \PY{n}{stride}\PY{o}{=}\PY{n}{STRIDE}\PY{p}{,} 
                                       \PY{n}{dataset}\PY{o}{=}\PY{n}{haydn\PYZus{}dataset}\PY{p}{,}
                                       \PY{n}{transforms}\PY{o}{=}\PY{n}{TRANSFORMS}\PY{p}{,}
                                       \PY{n}{val\PYZus{}split}\PY{o}{=}\PY{n}{VALIDATION\PYZus{}SPLIT}\PY{p}{)}
            \PY{n}{data\PYZus{}val} \PY{o}{=} \PY{n}{ChunksDataset}\PY{p}{(}\PY{n}{dataset}\PY{o}{=}\PY{n}{data\PYZus{}train}\PY{o}{.}\PY{n}{comp\PYZus{}set}\PY{p}{,}
                                     \PY{n}{transforms}\PY{o}{=}\PY{n}{TRANSFORMS}\PY{p}{)}
        
            \PY{n}{loader\PYZus{}train} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{data\PYZus{}train}\PY{p}{,}
                                      \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{BATCH\PYZus{}SIZE}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
                                      \PY{o}{*}\PY{o}{*}\PY{n}{LOADER\PYZus{}PARAMS}\PY{p}{)}
            \PY{n}{loader\PYZus{}val} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{data\PYZus{}val}\PY{p}{,}
                                    \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{BATCH\PYZus{}SIZE}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{val}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
                                    \PY{o}{*}\PY{o}{*}\PY{n}{LOADER\PYZus{}PARAMS}\PY{p}{)}
            
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{There are }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ pieces and }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ chunks in training set,}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}train}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}train}\PY{p}{)}\PY{p}{)} \PY{o}{+}
                  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{and }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ pieces and }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{ chunks in validation set}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}val}\PY{o}{.}\PY{n}{dataset}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{data\PYZus{}val}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
There are 73 pieces and 793169 chunks in training set,and 8 pieces and 219869 chunks in validation set

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} HYPERPARAMETERS}
        
        \PY{c+c1}{\PYZsh{} number of epochs to run}
        \PY{n}{NUM\PYZus{}EPOCHS} \PY{o}{=} \PY{l+m+mi}{20}
        \PY{c+c1}{\PYZsh{} number of dimensions for the embedded pitch vectors}
        \PY{n}{EMBED\PYZus{}DIM} \PY{o}{=} \PY{l+m+mi}{5}
        \PY{c+c1}{\PYZsh{} dimension of the rhythm}
        \PY{n}{RHYTHM\PYZus{}DIM} \PY{o}{=} \PY{l+m+mi}{1}
        \PY{c+c1}{\PYZsh{} the total number of pitches plus rest}
        \PY{n}{PITCH\PYZus{}VOCAB\PYZus{}SIZE} \PY{o}{=} \PY{l+m+mi}{140}
        \PY{c+c1}{\PYZsh{} parameters for the optimizers}
        \PY{n}{OPTIM\PYZus{}PARAMS} \PY{o}{=} \PY{p}{\PYZob{}}
            \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lr}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mf}{1e\PYZhy{}2}\PY{p}{,}
            \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{weight\PYZus{}decay}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mf}{0.0}
        \PY{p}{\PYZcb{}}
        
        \PY{c+c1}{\PYZsh{} weights applied to each of the loss functions}
        \PY{c+c1}{\PYZsh{} forward pitch}
        \PY{n}{fp\PYZus{}loss} \PY{o}{=} \PY{l+m+mf}{0.5}
        \PY{c+c1}{\PYZsh{} backward pitch}
        \PY{n}{bp\PYZus{}loss} \PY{o}{=} \PY{l+m+mf}{0.3}
        \PY{c+c1}{\PYZsh{} harmony pitch}
        \PY{n}{hp\PYZus{}loss} \PY{o}{=} \PY{l+m+mf}{0.3}
        \PY{c+c1}{\PYZsh{} foward rhythm}
        \PY{n}{fr\PYZus{}loss} \PY{o}{=} \PY{l+m+mf}{1.0}
        \PY{c+c1}{\PYZsh{} judge}
        \PY{n}{j\PYZus{}loss} \PY{o}{=} \PY{l+m+mf}{1.0}
        \PY{c+c1}{\PYZsh{} part}
        \PY{n}{p\PYZus{}loss} \PY{o}{=} \PY{l+m+mf}{0.1}
        
        \PY{n}{LOSS\PYZus{}WEIGHTS} \PY{o}{=} \PY{p}{[}\PY{n}{fp\PYZus{}loss}\PY{p}{,} \PY{n}{bp\PYZus{}loss}\PY{p}{,} \PY{n}{hp\PYZus{}loss}\PY{p}{,} \PY{n}{fr\PYZus{}loss}\PY{p}{,} \PY{n}{j\PYZus{}loss}\PY{p}{,} \PY{n}{p\PYZus{}loss}\PY{p}{]}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Hyperparameter loaded.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Hyperparameter loaded.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} MODELS AND OPTIMIZERS}
        
        \PY{n}{SKIP\PYZus{}MODELS} \PY{o}{=} \PY{k+kc}{False}
        
        \PY{k}{if} \PY{o+ow}{not} \PY{n}{SKIP\PYZus{}MODELS}\PY{p}{:}
            \PY{n}{model\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{forward\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{backward\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{harmony\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{judge\PYZus{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        
            \PY{n}{models} \PY{o}{=} \PY{p}{\PYZob{}}
                \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pitch\PYZus{}embed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{PitchEmbedModel}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{o}{=}\PY{n}{PITCH\PYZus{}VOCAB\PYZus{}SIZE}\PY{p}{,}
                                               \PY{n}{embed\PYZus{}dim}\PY{o}{=}\PY{n}{EMBED\PYZus{}DIM}\PY{p}{)}
            \PY{p}{\PYZcb{}}
            \PY{n}{optims} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
        
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{NUM\PYZus{}PARTS}\PY{p}{)}\PY{p}{:}
                \PY{n}{note\PYZus{}input\PYZus{}dim} \PY{o}{=} \PY{n}{EMBED\PYZus{}DIM} \PY{o}{+} \PY{n}{RHYTHM\PYZus{}DIM}
                \PY{n}{note\PYZus{}hidden\PYZus{}dim} \PY{o}{=} \PY{l+m+mi}{256}
                \PY{n}{note\PYZus{}num\PYZus{}layers} \PY{o}{=} \PY{l+m+mi}{1}
                \PY{n}{models}\PY{p}{[}\PY{n}{model\PYZus{}names}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{NoteModel}\PY{p}{(}\PY{n}{note\PYZus{}input\PYZus{}dim}\PY{p}{,} 
                                                            \PY{n}{note\PYZus{}hidden\PYZus{}dim}\PY{p}{,}
                                                            \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{BATCH\PYZus{}SIZE}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                                                            \PY{n}{num\PYZus{}layers}\PY{o}{=}\PY{n}{note\PYZus{}num\PYZus{}layers}\PY{p}{,}
                                                            \PY{n}{vocab\PYZus{}size}\PY{o}{=}\PY{n}{PITCH\PYZus{}VOCAB\PYZus{}SIZE}\PY{p}{)}
        
                \PY{n}{models}\PY{p}{[}\PY{n}{model\PYZus{}names}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{NoteModel}\PY{p}{(}\PY{n}{note\PYZus{}input\PYZus{}dim}\PY{p}{,} 
                                                            \PY{n}{note\PYZus{}hidden\PYZus{}dim}\PY{p}{,}
                                                            \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{BATCH\PYZus{}SIZE}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                                                            \PY{n}{num\PYZus{}layers}\PY{o}{=}\PY{n}{note\PYZus{}num\PYZus{}layers}\PY{p}{,}
                                                            \PY{n}{vocab\PYZus{}size}\PY{o}{=}\PY{n}{PITCH\PYZus{}VOCAB\PYZus{}SIZE}\PY{p}{)}
        
        
                \PY{n}{harmony\PYZus{}input\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{n}{NUM\PYZus{}PARTS}\PY{p}{,} \PY{n}{EMBED\PYZus{}DIM} \PY{o}{+} \PY{n}{NUM\PYZus{}PARTS}\PY{p}{)}
                \PY{n}{harmony\PYZus{}hidden\PYZus{}dim} \PY{o}{=} \PY{l+m+mi}{4} \PY{c+c1}{\PYZsh{} should be less than 9}
                \PY{n}{models}\PY{p}{[}\PY{n}{model\PYZus{}names}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{HarmonyModel}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{harmony\PYZus{}input\PYZus{}shape}\PY{p}{,}
                                                               \PY{n}{vocab\PYZus{}size}\PY{o}{=}\PY{n}{PITCH\PYZus{}VOCAB\PYZus{}SIZE}\PY{p}{,}
                                                               \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{n}{harmony\PYZus{}hidden\PYZus{}dim}\PY{p}{)}
        
        
                \PY{n}{judge\PYZus{}input\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{n}{NUM\PYZus{}PARTS} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{EMBED\PYZus{}DIM}\PY{p}{)}
                \PY{n}{judge\PYZus{}hidden\PYZus{}dim} \PY{o}{=} \PY{l+m+mi}{128}
                \PY{n}{output\PYZus{}dim} \PY{o}{=} \PY{n}{PITCH\PYZus{}VOCAB\PYZus{}SIZE}
                \PY{n}{models}\PY{p}{[}\PY{n}{model\PYZus{}names}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{JudgeModel}\PY{p}{(}\PY{n}{judge\PYZus{}input\PYZus{}shape}\PY{p}{,}
                                                             \PY{n}{judge\PYZus{}hidden\PYZus{}dim}\PY{p}{,}
                                                             \PY{n}{output\PYZus{}dim}\PY{p}{)}
        
                \PY{c+c1}{\PYZsh{} jointly optimize all of the params, so weights can be assigned to different loss.}
                \PY{n}{embed\PYZus{}params} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{models}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pitch\PYZus{}embed}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                \PY{n}{forward\PYZus{}params} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{models}\PY{p}{[}\PY{n}{model\PYZus{}names}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                \PY{n}{backward\PYZus{}params} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{models}\PY{p}{[}\PY{n}{model\PYZus{}names}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                \PY{n}{harmony\PYZus{}params} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{models}\PY{p}{[}\PY{n}{model\PYZus{}names}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                \PY{n}{judge\PYZus{}params} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{models}\PY{p}{[}\PY{n}{model\PYZus{}names}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                \PY{n}{optims}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{forward\PYZus{}params} \PY{o}{+} \PY{n}{backward\PYZus{}params} \PY{o}{+}
                                       \PY{n}{harmony\PYZus{}params} \PY{o}{+} \PY{n}{judge\PYZus{}params}\PY{p}{,} 
                                       \PY{o}{*}\PY{o}{*}\PY{n}{OPTIM\PYZus{}PARAMS}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} send all models to the appropriate device}
            \PY{k}{for} \PY{n}{key} \PY{o+ow}{in} \PY{n}{models}\PY{p}{:}
                \PY{n}{models}\PY{p}{[}\PY{n}{key}\PY{p}{]}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{)}
                
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Models loaded.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Models loaded.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} RUN THIS BLOCK CLEAR GPU CACHE}
        \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{empty\PYZus{}cache}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} TRAIN LOOP}
        
        \PY{n}{SKIP\PYZus{}TRAIN} \PY{o}{=} \PY{k+kc}{False}
        
        \PY{k}{if} \PY{o+ow}{not} \PY{n}{SKIP\PYZus{}TRAIN}\PY{p}{:}
            \PY{n}{train\PYZus{}stats} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{val\PYZus{}stats} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{saved\PYZus{}models} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        
            \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{NUM\PYZus{}EPOCHS}\PY{p}{)}\PY{p}{:}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{EPOCH }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}\PY{p}{)}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                \PY{n}{train\PYZus{}stat}\PY{p}{,} \PY{n}{models} \PY{o}{=} \PY{n}{train}\PY{p}{(}\PY{n}{models}\PY{p}{,} \PY{n}{optims}\PY{p}{,} \PY{n}{loader\PYZus{}train}\PY{p}{,} 
                                      \PY{n}{model\PYZus{}names}\PY{o}{=}\PY{n}{model\PYZus{}names}\PY{p}{,} 
                                      \PY{n}{loss\PYZus{}weights}\PY{o}{=}\PY{n}{LOSS\PYZus{}WEIGHTS}\PY{p}{,}
                                      \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,}
                                      \PY{n}{print\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
        
                \PY{n}{val\PYZus{}stat}\PY{p}{,} \PY{n}{models} \PY{o}{=} \PY{n}{validate}\PY{p}{(}\PY{n}{models}\PY{p}{,} \PY{n}{loader\PYZus{}val}\PY{p}{,}
                                         \PY{n}{model\PYZus{}names}\PY{o}{=}\PY{n}{model\PYZus{}names}\PY{p}{,}
                                         \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,}
                                         \PY{n}{print\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
        
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Completed epoch }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}\PY{p}{)}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                \PY{n}{train\PYZus{}stats}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{train\PYZus{}stat}\PY{p}{)}
                \PY{n}{val\PYZus{}stats}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{val\PYZus{}stat}\PY{p}{)}
                \PY{n}{saved\PYZus{}models}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{copy}\PY{o}{.}\PY{n}{deepcopy}\PY{p}{(}\PY{n}{models}\PY{p}{)}\PY{p}{)}
        
        
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training completed! Saving files.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} create a folder to store all of the stats and models}
            \PY{n}{mkdir}\PY{p}{(}\PY{n}{OUTPUT\PYZus{}PATH}\PY{p}{)}
            \PY{n}{stats\PYZus{}file\PYZus{}name} \PY{o}{=} \PY{n}{RUN\PYZus{}TIME} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{RUN\PYZus{}ID} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.stat}\PY{l+s+s2}{\PYZdq{}}
            \PY{n}{stats\PYZus{}file\PYZus{}path} \PY{o}{=} \PY{n}{OUTPUT\PYZus{}PATH} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{stats\PYZus{}file\PYZus{}name}
            \PY{n}{models\PYZus{}file\PYZus{}name} \PY{o}{=} \PY{n}{RUN\PYZus{}TIME} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{RUN\PYZus{}ID} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.models}\PY{l+s+s2}{\PYZdq{}}
            \PY{n}{models\PYZus{}file\PYZus{}path} \PY{o}{=} \PY{n}{OUTPUT\PYZus{}PATH} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{models\PYZus{}file\PYZus{}name}
        
            \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{stats\PYZus{}file\PYZus{}path}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{wb}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{k}{as} \PY{n}{file}\PY{p}{:}
                \PY{n}{pickle}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{p}{(}\PY{n}{train\PYZus{}stats}\PY{p}{,} \PY{n}{val\PYZus{}stats}\PY{p}{)}\PY{p}{,} \PY{n}{file}\PY{p}{)}
            \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{models\PYZus{}file\PYZus{}path}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{wb}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{k}{as} \PY{n}{file}\PY{p}{:}
                \PY{n}{pickle}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{n}{saved\PYZus{}models}\PY{p}{,} \PY{n}{file}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
EPOCH 0
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 4.95140/0.00\%, bp\_loss: 4.95615/0.00\%, hp\_loss: 5.01104/1.00\%, j\_loss: 5.84864/0.00\%, 
		fr\_loss: 0.50101/45.00\%, p\_loss: 1.13502/54.00\%, 
		total weighted loss: 11.92901
	Part 2 - fp\_loss: 4.93666/0.00\%, bp\_loss: 4.95593/0.00\%, hp\_loss: 5.21532/0.00\%, j\_loss: 6.12076/0.00\%, 
		fr\_loss: 0.49879/53.00\%, p\_loss: 1.22066/50.00\%, 
		total weighted loss: 12.26133
	Part 3 - fp\_loss: 4.98336/0.00\%, bp\_loss: 4.91575/1.00\%, hp\_loss: 5.01843/0.00\%, j\_loss: 6.51066/0.00\%, 
		fr\_loss: 0.50110/53.00\%, p\_loss: 2.00827/0.00\%, 
		total weighted loss: 12.68452
	Part 4 - fp\_loss: 4.91120/0.00\%, bp\_loss: 4.95009/0.00\%, hp\_loss: 4.94248/0.00\%, j\_loss: 5.46928/0.00\%, 
		fr\_loss: 0.50266/47.00\%, p\_loss: 1.44329/9.00\%, 
		total weighted loss: 11.53964
	Training time elapsed: 1.19 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.96870/48.00\%, bp\_loss: 3.13267/24.00\%, hp\_loss: 3.11988/24.00\%, j\_loss: 2.38730/44.00\%, 
		fr\_loss: 0.45711/54.00\%, p\_loss: 0.00150/100.00\%, 
		total weighted loss: 5.70468
	Part 2 - fp\_loss: 1.92614/50.00\%, bp\_loss: 2.90178/28.00\%, hp\_loss: 2.89999/28.00\%, j\_loss: 2.31987/48.00\%, 
		fr\_loss: 0.35440/64.00\%, p\_loss: 0.00134/100.00\%, 
		total weighted loss: 5.37800
	Part 3 - fp\_loss: 1.74535/58.00\%, bp\_loss: 2.67688/37.00\%, hp\_loss: 2.56875/38.00\%, j\_loss: 2.01602/56.00\%, 
		fr\_loss: 0.31272/68.00\%, p\_loss: 0.00269/100.00\%, 
		total weighted loss: 4.77537
	Part 4 - fp\_loss: 1.93756/57.00\%, bp\_loss: 2.85234/32.00\%, hp\_loss: 2.74203/33.00\%, j\_loss: 2.13135/55.00\%, 
		fr\_loss: 0.26834/73.00\%, p\_loss: 0.00134/100.00\%, 
		total weighted loss: 5.04691
	Training time elapsed: 38.78 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.86119/45.00\%, bp\_loss: 2.94614/22.00\%, hp\_loss: 3.13066/21.00\%, j\_loss: 2.32394/44.00\%, 
		fr\_loss: 0.41608/58.00\%, p\_loss: 0.00069/100.00\%, 
		total weighted loss: 5.49372
	Part 2 - fp\_loss: 1.66959/54.00\%, bp\_loss: 2.68692/31.00\%, hp\_loss: 2.79704/31.00\%, j\_loss: 2.07822/53.00\%, 
		fr\_loss: 0.25664/75.00\%, p\_loss: 0.00059/100.00\%, 
		total weighted loss: 4.81490
	Part 3 - fp\_loss: 1.52298/59.00\%, bp\_loss: 2.61002/36.00\%, hp\_loss: 2.43499/37.00\%, j\_loss: 1.83791/59.00\%, 
		fr\_loss: 0.25375/74.00\%, p\_loss: 0.00109/100.00\%, 
		total weighted loss: 4.36676
	Part 4 - fp\_loss: 1.58695/59.00\%, bp\_loss: 2.73012/29.00\%, hp\_loss: 2.68291/30.00\%, j\_loss: 1.98849/58.00\%, 
		fr\_loss: 0.21742/78.00\%, p\_loss: 0.00071/100.00\%, 
		total weighted loss: 4.62336
	Training time elapsed: 76.29 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.74434/50.00\%, bp\_loss: 2.80716/23.00\%, hp\_loss: 3.08666/22.00\%, j\_loss: 2.17331/49.00\%, 
		fr\_loss: 0.45207/54.00\%, p\_loss: 0.00052/100.00\%, 
		total weighted loss: 5.26575
	Part 2 - fp\_loss: 1.59718/56.00\%, bp\_loss: 2.63560/30.00\%, hp\_loss: 2.82005/30.00\%, j\_loss: 1.99254/54.00\%, 
		fr\_loss: 0.22753/77.00\%, p\_loss: 0.00039/100.00\%, 
		total weighted loss: 4.65539
	Part 3 - fp\_loss: 1.54537/58.00\%, bp\_loss: 2.49520/35.00\%, hp\_loss: 2.46862/37.00\%, j\_loss: 1.83874/58.00\%, 
		fr\_loss: 0.23566/76.00\%, p\_loss: 0.00066/100.00\%, 
		total weighted loss: 4.33630
	Part 4 - fp\_loss: 1.55193/58.00\%, bp\_loss: 2.56182/31.00\%, hp\_loss: 2.66897/31.00\%, j\_loss: 1.94378/58.00\%, 
		fr\_loss: 0.19536/80.00\%, p\_loss: 0.00053/100.00\%, 
		total weighted loss: 4.48440
	Training time elapsed: 113.76 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.64941/53.00\%, bp\_loss: 2.80227/22.00\%, hp\_loss: 3.13299/20.00\%, j\_loss: 2.05370/53.00\%, 
		fr\_loss: 0.25136/75.00\%, p\_loss: 0.00032/100.00\%, 
		total weighted loss: 4.91038
	Part 2 - fp\_loss: 1.40717/59.00\%, bp\_loss: 2.50271/32.00\%, hp\_loss: 2.78544/32.00\%, j\_loss: 1.81288/59.00\%, 
		fr\_loss: 0.23322/77.00\%, p\_loss: 0.00031/100.00\%, 
		total weighted loss: 4.33616
	Part 3 - fp\_loss: 1.45149/61.00\%, bp\_loss: 2.50543/33.00\%, hp\_loss: 2.54665/35.00\%, j\_loss: 1.75664/61.00\%, 
		fr\_loss: 0.21823/78.00\%, p\_loss: 0.00047/100.00\%, 
		total weighted loss: 4.21629
	Part 4 - fp\_loss: 1.44711/61.00\%, bp\_loss: 2.58502/28.00\%, hp\_loss: 2.67673/29.00\%, j\_loss: 1.83028/60.00\%, 
		fr\_loss: 0.18924/81.00\%, p\_loss: 0.00048/100.00\%, 
		total weighted loss: 4.32165
	Training time elapsed: 151.23 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.63133/52.00\%, bp\_loss: 2.66006/25.00\%, hp\_loss: 3.09211/22.00\%, j\_loss: 2.05970/52.00\%, 
		fr\_loss: 0.25703/74.00\%, p\_loss: 0.00024/100.00\%, 
		total weighted loss: 4.85806
	Part 2 - fp\_loss: 1.52057/57.00\%, bp\_loss: 2.42600/32.00\%, hp\_loss: 2.76126/31.00\%, j\_loss: 1.91905/57.00\%, 
		fr\_loss: 0.23727/76.00\%, p\_loss: 0.00024/100.00\%, 
		total weighted loss: 4.47280
	Part 3 - fp\_loss: 1.37693/62.00\%, bp\_loss: 2.35437/36.00\%, hp\_loss: 2.45780/37.00\%, j\_loss: 1.70715/62.00\%, 
		fr\_loss: 0.20465/79.00\%, p\_loss: 0.00039/100.00\%, 
		total weighted loss: 4.04395
	Part 4 - fp\_loss: 1.43517/61.00\%, bp\_loss: 2.44372/33.00\%, hp\_loss: 2.67323/31.00\%, j\_loss: 1.81976/61.00\%, 
		fr\_loss: 0.20719/79.00\%, p\_loss: 0.00040/100.00\%, 
		total weighted loss: 4.27966
	Training time elapsed: 188.70 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.57223/55.00\%, bp\_loss: 2.73895/21.00\%, hp\_loss: 3.15352/19.00\%, j\_loss: 1.98187/55.00\%, 
		fr\_loss: 0.24319/75.00\%, p\_loss: 0.00021/100.00\%, 
		total weighted loss: 4.77894
	Part 2 - fp\_loss: 1.50901/58.00\%, bp\_loss: 2.43469/32.00\%, hp\_loss: 2.79271/31.00\%, j\_loss: 1.87632/57.00\%, 
		fr\_loss: 0.22277/78.00\%, p\_loss: 0.00021/100.00\%, 
		total weighted loss: 4.42183
	Part 3 - fp\_loss: 1.33561/65.00\%, bp\_loss: 2.34715/36.00\%, hp\_loss: 2.51725/35.00\%, j\_loss: 1.63283/65.00\%, 
		fr\_loss: 0.23148/76.00\%, p\_loss: 0.00028/100.00\%, 
		total weighted loss: 3.99147
	Part 4 - fp\_loss: 1.37577/63.00\%, bp\_loss: 2.40597/32.00\%, hp\_loss: 2.63588/31.00\%, j\_loss: 1.72488/62.00\%, 
		fr\_loss: 0.18429/81.00\%, p\_loss: 0.00034/100.00\%, 
		total weighted loss: 4.10964
	Training time elapsed: 226.17 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.57803/53.00\%, bp\_loss: 2.68228/23.00\%, hp\_loss: 3.14906/21.00\%, j\_loss: 2.05286/53.00\%, 
		fr\_loss: 0.23302/76.00\%, p\_loss: 0.00017/100.00\%, 
		total weighted loss: 4.82431
	Part 2 - fp\_loss: 1.39778/62.00\%, bp\_loss: 2.33823/33.00\%, hp\_loss: 2.76035/30.00\%, j\_loss: 1.73089/61.00\%, 
		fr\_loss: 0.23465/76.00\%, p\_loss: 0.00018/100.00\%, 
		total weighted loss: 4.19403
	Part 3 - fp\_loss: 1.41384/61.00\%, bp\_loss: 2.24747/38.00\%, hp\_loss: 2.47295/36.00\%, j\_loss: 1.74974/60.00\%, 
		fr\_loss: 0.22739/77.00\%, p\_loss: 0.00028/100.00\%, 
		total weighted loss: 4.10020
	Part 4 - fp\_loss: 1.29938/63.00\%, bp\_loss: 2.34733/33.00\%, hp\_loss: 2.61347/31.00\%, j\_loss: 1.71634/63.00\%, 
		fr\_loss: 0.17329/83.00\%, p\_loss: 0.00034/100.00\%, 
		total weighted loss: 4.02760
	Training time elapsed: 263.65 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.55931/54.00\%, bp\_loss: 2.58743/26.00\%, hp\_loss: 3.06939/23.00\%, j\_loss: 1.99734/53.00\%, 
		fr\_loss: 0.23358/76.00\%, p\_loss: 0.00014/100.00\%, 
		total weighted loss: 4.70764
	Part 2 - fp\_loss: 1.40415/61.00\%, bp\_loss: 2.31568/36.00\%, hp\_loss: 2.73078/31.00\%, j\_loss: 1.76151/61.00\%, 
		fr\_loss: 0.22088/78.00\%, p\_loss: 0.00016/100.00\%, 
		total weighted loss: 4.19842
	Part 3 - fp\_loss: 1.31887/65.00\%, bp\_loss: 2.19094/39.00\%, hp\_loss: 2.50751/36.00\%, j\_loss: 1.61455/65.00\%, 
		fr\_loss: 0.21918/78.00\%, p\_loss: 0.00022/100.00\%, 
		total weighted loss: 3.90273
	Part 4 - fp\_loss: 1.24543/65.00\%, bp\_loss: 2.32291/35.00\%, hp\_loss: 2.59819/31.00\%, j\_loss: 1.60857/65.00\%, 
		fr\_loss: 0.18865/81.00\%, p\_loss: 0.00035/100.00\%, 
		total weighted loss: 3.89630
	Training time elapsed: 301.14 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.76405/50.00\%, bp\_loss: 2.64477/29.00\%, hp\_loss: 3.06525/27.00\%, j\_loss: 2.19397/49.00\%, 
		fr\_loss: 0.26840/73.00\%, p\_loss: 0.00013/100.00\%, 
		total weighted loss: 9.93656
	Part 2 - fp\_loss: 1.36882/62.00\%, bp\_loss: 2.44437/42.00\%, hp\_loss: 2.56996/41.00\%, j\_loss: 1.67554/62.00\%, 
		fr\_loss: 0.27388/72.00\%, p\_loss: 0.00013/100.00\%, 
		total weighted loss: 8.33270
	Part 3 - fp\_loss: 1.11345/70.00\%, bp\_loss: 2.02400/47.00\%, hp\_loss: 2.05699/50.00\%, j\_loss: 1.38840/70.00\%, 
		fr\_loss: 0.21304/79.00\%, p\_loss: 0.00019/100.00\%, 
		total weighted loss: 6.79608
	Part 4 - fp\_loss: 1.18719/67.00\%, bp\_loss: 2.21174/41.00\%, hp\_loss: 2.23994/43.00\%, j\_loss: 1.46462/68.00\%, 
		fr\_loss: 0.21429/78.00\%, p\_loss: 0.00027/100.00\%, 
		total weighted loss: 7.31805
	`Validation time elapsed: 0.68 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.69039/53.00\%, bp\_loss: 2.54699/31.00\%, hp\_loss: 3.07074/27.00\%, j\_loss: 2.05071/52.00\%, 
		fr\_loss: 0.23390/76.00\%, p\_loss: 0.00012/100.00\%, 
		total weighted loss: 9.59285
	Part 2 - fp\_loss: 1.33287/61.00\%, bp\_loss: 2.54015/42.00\%, hp\_loss: 2.55442/42.00\%, j\_loss: 1.68801/61.00\%, 
		fr\_loss: 0.23209/76.00\%, p\_loss: 0.00012/100.00\%, 
		total weighted loss: 8.34765
	Part 3 - fp\_loss: 1.05345/71.00\%, bp\_loss: 1.99479/50.00\%, hp\_loss: 1.95706/52.00\%, j\_loss: 1.34106/70.00\%, 
		fr\_loss: 0.22730/77.00\%, p\_loss: 0.00017/100.00\%, 
		total weighted loss: 6.57383
	Part 4 - fp\_loss: 1.12178/69.00\%, bp\_loss: 2.10491/44.00\%, hp\_loss: 2.14335/47.00\%, j\_loss: 1.41754/69.00\%, 
		fr\_loss: 0.21512/78.00\%, p\_loss: 0.00025/100.00\%, 
		total weighted loss: 7.00295
	`Validation time elapsed: 9.50 seconds
`
-----------
Completed epoch 0.

EPOCH 1
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.55166/54.00\%, bp\_loss: 2.67152/25.00\%, hp\_loss: 3.13517/21.00\%, j\_loss: 1.99812/54.00\%, 
		fr\_loss: 0.22769/77.00\%, p\_loss: 0.00014/100.00\%, 
		total weighted loss: 4.74366
	Part 2 - fp\_loss: 1.45770/59.00\%, bp\_loss: 2.53632/32.00\%, hp\_loss: 2.78503/29.00\%, j\_loss: 1.80967/58.00\%, 
		fr\_loss: 0.22114/78.00\%, p\_loss: 0.00018/100.00\%, 
		total weighted loss: 4.35608
	Part 3 - fp\_loss: 1.32908/65.00\%, bp\_loss: 2.29434/37.00\%, hp\_loss: 2.49063/36.00\%, j\_loss: 1.64166/65.00\%, 
		fr\_loss: 0.20214/79.00\%, p\_loss: 0.00022/100.00\%, 
		total weighted loss: 3.94385
	Part 4 - fp\_loss: 1.24198/66.00\%, bp\_loss: 2.37257/33.00\%, hp\_loss: 2.66155/32.00\%, j\_loss: 1.57963/66.00\%, 
		fr\_loss: 0.17811/82.00\%, p\_loss: 0.00036/100.00\%, 
		total weighted loss: 3.88900
	Training time elapsed: 0.98 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.55761/53.00\%, bp\_loss: 2.64264/24.00\%, hp\_loss: 3.08167/22.00\%, j\_loss: 1.98443/52.00\%, 
		fr\_loss: 0.22267/77.00\%, p\_loss: 0.00012/100.00\%, 
		total weighted loss: 4.70321
	Part 2 - fp\_loss: 1.50116/58.00\%, bp\_loss: 2.34145/33.00\%, hp\_loss: 2.81137/28.00\%, j\_loss: 1.78896/58.00\%, 
		fr\_loss: 0.24033/76.00\%, p\_loss: 0.00014/100.00\%, 
		total weighted loss: 4.32573
	Part 3 - fp\_loss: 1.30669/65.00\%, bp\_loss: 2.19674/38.00\%, hp\_loss: 2.51866/36.00\%, j\_loss: 1.58123/65.00\%, 
		fr\_loss: 0.22303/77.00\%, p\_loss: 0.00016/100.00\%, 
		total weighted loss: 3.87223
	Part 4 - fp\_loss: 1.37227/63.00\%, bp\_loss: 2.33580/34.00\%, hp\_loss: 2.68138/33.00\%, j\_loss: 1.66586/63.00\%, 
		fr\_loss: 0.17762/82.00\%, p\_loss: 0.00039/100.00\%, 
		total weighted loss: 4.03481
	Training time elapsed: 38.47 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.50131/54.00\%, bp\_loss: 2.58303/25.00\%, hp\_loss: 3.10477/22.00\%, j\_loss: 1.94216/54.00\%, 
		fr\_loss: 0.20176/79.00\%, p\_loss: 0.00011/100.00\%, 
		total weighted loss: 4.60092
	Part 2 - fp\_loss: 1.38143/61.00\%, bp\_loss: 2.30945/35.00\%, hp\_loss: 2.80931/30.00\%, j\_loss: 1.66994/61.00\%, 
		fr\_loss: 0.18068/82.00\%, p\_loss: 0.00011/100.00\%, 
		total weighted loss: 4.07698
	Part 3 - fp\_loss: 1.28169/65.00\%, bp\_loss: 2.11817/40.00\%, hp\_loss: 2.49455/36.00\%, j\_loss: 1.60772/64.00\%, 
		fr\_loss: 0.18198/81.00\%, p\_loss: 0.00013/100.00\%, 
		total weighted loss: 3.81437
	Part 4 - fp\_loss: 1.33927/64.00\%, bp\_loss: 2.29860/35.00\%, hp\_loss: 2.67154/30.00\%, j\_loss: 1.65958/63.00\%, 
		fr\_loss: 0.16151/83.00\%, p\_loss: 0.00031/100.00\%, 
		total weighted loss: 3.98180
	Training time elapsed: 75.93 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.54232/55.00\%, bp\_loss: 2.53601/26.00\%, hp\_loss: 3.07372/23.00\%, j\_loss: 1.95337/55.00\%, 
		fr\_loss: 0.22781/77.00\%, p\_loss: 0.00010/100.00\%, 
		total weighted loss: 4.63527
	Part 2 - fp\_loss: 1.35014/62.00\%, bp\_loss: 2.21599/37.00\%, hp\_loss: 2.73076/31.00\%, j\_loss: 1.66766/62.00\%, 
		fr\_loss: 0.19312/81.00\%, p\_loss: 0.00013/100.00\%, 
		total weighted loss: 4.01990
	Part 3 - fp\_loss: 1.27223/65.00\%, bp\_loss: 2.07455/41.00\%, hp\_loss: 2.42003/36.00\%, j\_loss: 1.53002/65.00\%, 
		fr\_loss: 0.18755/81.00\%, p\_loss: 0.00011/100.00\%, 
		total weighted loss: 3.70207
	Part 4 - fp\_loss: 1.24319/65.00\%, bp\_loss: 2.26208/36.00\%, hp\_loss: 2.65853/29.00\%, j\_loss: 1.57121/65.00\%, 
		fr\_loss: 0.15299/85.00\%, p\_loss: 0.00036/100.00\%, 
		total weighted loss: 3.82202
	Training time elapsed: 113.40 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.52417/56.00\%, bp\_loss: 2.49026/26.00\%, hp\_loss: 3.02403/23.00\%, j\_loss: 1.90096/55.00\%, 
		fr\_loss: 0.21852/78.00\%, p\_loss: 0.00009/100.00\%, 
		total weighted loss: 4.53586
	Part 2 - fp\_loss: 1.42130/59.00\%, bp\_loss: 2.23055/36.00\%, hp\_loss: 2.71680/31.00\%, j\_loss: 1.72297/58.00\%, 
		fr\_loss: 0.20363/79.00\%, p\_loss: 0.00014/100.00\%, 
		total weighted loss: 4.12146
	Part 3 - fp\_loss: 1.27161/66.00\%, bp\_loss: 2.07101/43.00\%, hp\_loss: 2.41626/38.00\%, j\_loss: 1.43763/66.00\%, 
		fr\_loss: 0.21944/78.00\%, p\_loss: 0.00010/100.00\%, 
		total weighted loss: 3.63907
	Part 4 - fp\_loss: 1.29328/64.00\%, bp\_loss: 2.31314/34.00\%, hp\_loss: 2.67875/30.00\%, j\_loss: 1.64132/63.00\%, 
		fr\_loss: 0.17718/82.00\%, p\_loss: 0.00025/100.00\%, 
		total weighted loss: 3.96274
	Training time elapsed: 150.85 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.59390/52.00\%, bp\_loss: 2.51588/27.00\%, hp\_loss: 3.05540/23.00\%, j\_loss: 2.02669/52.00\%, 
		fr\_loss: 0.20322/79.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 4.69825
	Part 2 - fp\_loss: 1.26410/62.00\%, bp\_loss: 2.25892/36.00\%, hp\_loss: 2.77653/28.00\%, j\_loss: 1.60215/62.00\%, 
		fr\_loss: 0.19006/80.00\%, p\_loss: 0.00011/100.00\%, 
		total weighted loss: 3.93490
	Part 3 - fp\_loss: 1.28112/64.00\%, bp\_loss: 2.19266/38.00\%, hp\_loss: 2.42617/35.00\%, j\_loss: 1.59064/64.00\%, 
		fr\_loss: 0.20422/79.00\%, p\_loss: 0.00009/100.00\%, 
		total weighted loss: 3.82108
	Part 4 - fp\_loss: 1.18036/69.00\%, bp\_loss: 2.26961/36.00\%, hp\_loss: 2.61922/34.00\%, j\_loss: 1.39254/70.00\%, 
		fr\_loss: 0.14614/85.00\%, p\_loss: 0.00034/100.00\%, 
		total weighted loss: 3.59554
	Training time elapsed: 188.29 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.56330/54.00\%, bp\_loss: 2.48457/28.00\%, hp\_loss: 3.10102/22.00\%, j\_loss: 1.97132/54.00\%, 
		fr\_loss: 0.21754/78.00\%, p\_loss: 0.00007/100.00\%, 
		total weighted loss: 4.64619
	Part 2 - fp\_loss: 1.37487/61.00\%, bp\_loss: 2.21006/38.00\%, hp\_loss: 2.73229/31.00\%, j\_loss: 1.69594/61.00\%, 
		fr\_loss: 0.22789/77.00\%, p\_loss: 0.00008/100.00\%, 
		total weighted loss: 4.09398
	Part 3 - fp\_loss: 1.28436/66.00\%, bp\_loss: 2.16687/37.00\%, hp\_loss: 2.44380/36.00\%, j\_loss: 1.54839/65.00\%, 
		fr\_loss: 0.19831/80.00\%, p\_loss: 0.00008/100.00\%, 
		total weighted loss: 3.77210
	Part 4 - fp\_loss: 1.17099/69.00\%, bp\_loss: 2.12132/41.00\%, hp\_loss: 2.63162/32.00\%, j\_loss: 1.37487/69.00\%, 
		fr\_loss: 0.17186/82.00\%, p\_loss: 0.00025/100.00\%, 
		total weighted loss: 3.55813
	Training time elapsed: 225.75 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.58248/54.00\%, bp\_loss: 2.53452/26.00\%, hp\_loss: 3.15999/19.00\%, j\_loss: 1.95126/54.00\%, 
		fr\_loss: 0.20875/79.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 4.65961
	Part 2 - fp\_loss: 1.34376/63.00\%, bp\_loss: 2.41288/32.00\%, hp\_loss: 2.77521/29.00\%, j\_loss: 1.63308/63.00\%, 
		fr\_loss: 0.20208/79.00\%, p\_loss: 0.00008/100.00\%, 
		total weighted loss: 4.06347
	Part 3 - fp\_loss: 1.26064/65.00\%, bp\_loss: 2.12987/38.00\%, hp\_loss: 2.46435/36.00\%, j\_loss: 1.55596/65.00\%, 
		fr\_loss: 0.18092/81.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 3.74547
	Part 4 - fp\_loss: 1.26697/66.00\%, bp\_loss: 1.79169/49.00\%, hp\_loss: 2.58902/32.00\%, j\_loss: 1.27817/69.00\%, 
		fr\_loss: 0.15940/84.00\%, p\_loss: 0.00023/100.00\%, 
		total weighted loss: 3.38528
	Training time elapsed: 263.21 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.51739/54.00\%, bp\_loss: 2.50763/26.00\%, hp\_loss: 3.12532/20.00\%, j\_loss: 1.95720/54.00\%, 
		fr\_loss: 0.20335/79.00\%, p\_loss: 0.00007/100.00\%, 
		total weighted loss: 4.60914
	Part 2 - fp\_loss: 1.39388/61.00\%, bp\_loss: 2.34629/32.00\%, hp\_loss: 2.76012/29.00\%, j\_loss: 1.67438/61.00\%, 
		fr\_loss: 0.21093/79.00\%, p\_loss: 0.00010/100.00\%, 
		total weighted loss: 4.11418
	Part 3 - fp\_loss: 1.17366/69.00\%, bp\_loss: 2.21456/38.00\%, hp\_loss: 2.43277/36.00\%, j\_loss: 1.39445/69.00\%, 
		fr\_loss: 0.19630/80.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 3.57179
	Part 4 - fp\_loss: 1.19569/67.00\%, bp\_loss: 1.72691/53.00\%, hp\_loss: 2.64320/31.00\%, j\_loss: 1.17951/70.00\%, 
		fr\_loss: 0.15601/84.00\%, p\_loss: 0.00026/100.00\%, 
		total weighted loss: 3.24442
	Training time elapsed: 300.65 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.58057/56.00\%, bp\_loss: 2.45172/31.00\%, hp\_loss: 3.07881/26.00\%, j\_loss: 1.99106/56.00\%, 
		fr\_loss: 0.23989/75.00\%, p\_loss: 0.00008/100.00\%, 
		total weighted loss: 9.34212
	Part 2 - fp\_loss: 1.32441/62.00\%, bp\_loss: 2.20975/40.00\%, hp\_loss: 2.54935/42.00\%, j\_loss: 1.60566/62.00\%, 
		fr\_loss: 0.24452/75.00\%, p\_loss: 0.00008/100.00\%, 
		total weighted loss: 7.93376
	Part 3 - fp\_loss: 1.08507/70.00\%, bp\_loss: 1.97755/48.00\%, hp\_loss: 2.02642/50.00\%, j\_loss: 1.31401/70.00\%, 
		fr\_loss: 0.19931/79.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 6.60242
	Part 4 - fp\_loss: 1.17389/68.00\%, bp\_loss: 2.22163/42.00\%, hp\_loss: 2.22579/43.00\%, j\_loss: 1.47706/64.00\%, 
		fr\_loss: 0.19893/80.00\%, p\_loss: 0.00013/100.00\%, 
		total weighted loss: 7.29743
	`Validation time elapsed: 0.72 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.67898/53.00\%, bp\_loss: 2.53049/29.00\%, hp\_loss: 3.05231/27.00\%, j\_loss: 2.04875/53.00\%, 
		fr\_loss: 0.23495/76.00\%, p\_loss: 0.00008/100.00\%, 
		total weighted loss: 9.54556
	Part 2 - fp\_loss: 1.27713/63.00\%, bp\_loss: 2.25366/41.00\%, hp\_loss: 2.55110/43.00\%, j\_loss: 1.65348/63.00\%, 
		fr\_loss: 0.23867/76.00\%, p\_loss: 0.00007/100.00\%, 
		total weighted loss: 7.97410
	Part 3 - fp\_loss: 1.14491/66.00\%, bp\_loss: 1.99485/50.00\%, hp\_loss: 1.90454/52.00\%, j\_loss: 1.47510/66.00\%, 
		fr\_loss: 0.23516/76.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 6.75461
	Part 4 - fp\_loss: 1.22228/68.00\%, bp\_loss: 1.92200/48.00\%, hp\_loss: 2.18923/45.00\%, j\_loss: 1.26921/69.00\%, 
		fr\_loss: 0.23198/76.00\%, p\_loss: 0.00017/100.00\%, 
		total weighted loss: 6.83487
	`Validation time elapsed: 9.52 seconds
`
-----------
Completed epoch 1.

EPOCH 2
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.48091/57.00\%, bp\_loss: 2.49037/26.00\%, hp\_loss: 3.12634/20.00\%, j\_loss: 1.88681/56.00\%, 
		fr\_loss: 0.19770/80.00\%, p\_loss: 0.00007/100.00\%, 
		total weighted loss: 4.50999
	Part 2 - fp\_loss: 1.35412/63.00\%, bp\_loss: 2.25357/37.00\%, hp\_loss: 2.77205/29.00\%, j\_loss: 1.60845/62.00\%, 
		fr\_loss: 0.20254/79.00\%, p\_loss: 0.00008/100.00\%, 
		total weighted loss: 3.99575
	Part 3 - fp\_loss: 1.27540/64.00\%, bp\_loss: 2.09356/39.00\%, hp\_loss: 2.46551/37.00\%, j\_loss: 1.56186/64.00\%, 
		fr\_loss: 0.19221/81.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 3.75950
	Part 4 - fp\_loss: 1.26338/64.00\%, bp\_loss: 2.14234/41.00\%, hp\_loss: 2.63522/32.00\%, j\_loss: 1.48053/62.00\%, 
		fr\_loss: 0.16782/83.00\%, p\_loss: 0.00028/100.00\%, 
		total weighted loss: 3.71334
	Training time elapsed: 1.04 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.48210/58.00\%, bp\_loss: 2.43162/30.00\%, hp\_loss: 3.05306/24.00\%, j\_loss: 1.87457/57.00\%, 
		fr\_loss: 0.19272/81.00\%, p\_loss: 0.00007/100.00\%, 
		total weighted loss: 4.45375
	Part 2 - fp\_loss: 1.32433/62.00\%, bp\_loss: 2.17256/36.00\%, hp\_loss: 2.75422/29.00\%, j\_loss: 1.66079/62.00\%, 
		fr\_loss: 0.17904/82.00\%, p\_loss: 0.00008/100.00\%, 
		total weighted loss: 3.98004
	Part 3 - fp\_loss: 1.17731/68.00\%, bp\_loss: 2.10674/42.00\%, hp\_loss: 2.47266/36.00\%, j\_loss: 1.42976/67.00\%, 
		fr\_loss: 0.17569/82.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 3.56793
	Part 4 - fp\_loss: 1.20508/66.00\%, bp\_loss: 1.65492/53.00\%, hp\_loss: 2.65411/31.00\%, j\_loss: 1.24857/68.00\%, 
		fr\_loss: 0.13411/86.00\%, p\_loss: 0.00018/100.00\%, 
		total weighted loss: 3.27794
	Training time elapsed: 38.27 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.45917/56.00\%, bp\_loss: 2.45701/28.00\%, hp\_loss: 3.09743/22.00\%, j\_loss: 1.85491/56.00\%, 
		fr\_loss: 0.19569/80.00\%, p\_loss: 0.00007/100.00\%, 
		total weighted loss: 4.44653
	Part 2 - fp\_loss: 1.34178/62.00\%, bp\_loss: 2.16286/40.00\%, hp\_loss: 2.67100/33.00\%, j\_loss: 1.60443/62.00\%, 
		fr\_loss: 0.20411/79.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 3.92959
	Part 3 - fp\_loss: 1.27239/65.00\%, bp\_loss: 1.92444/45.00\%, hp\_loss: 2.41787/38.00\%, j\_loss: 1.45453/66.00\%, 
		fr\_loss: 0.21519/78.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 3.60862
	Part 4 - fp\_loss: 1.21480/67.00\%, bp\_loss: 1.39450/63.00\%, hp\_loss: 2.55857/33.00\%, j\_loss: 0.99190/73.00\%, 
		fr\_loss: 0.16400/83.00\%, p\_loss: 0.00021/100.00\%, 
		total weighted loss: 2.94924
	Training time elapsed: 75.46 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.41424/58.00\%, bp\_loss: 2.41705/29.00\%, hp\_loss: 3.11178/21.00\%, j\_loss: 1.84638/57.00\%, 
		fr\_loss: 0.17914/82.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 4.39129
	Part 2 - fp\_loss: 1.32608/63.00\%, bp\_loss: 2.10868/39.00\%, hp\_loss: 2.78485/30.00\%, j\_loss: 1.59659/63.00\%, 
		fr\_loss: 0.19908/80.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 3.92677
	Part 3 - fp\_loss: 1.23130/66.00\%, bp\_loss: 1.95957/43.00\%, hp\_loss: 2.46828/36.00\%, j\_loss: 1.39802/67.00\%, 
		fr\_loss: 0.18479/81.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 3.52682
	Part 4 - fp\_loss: 1.20309/67.00\%, bp\_loss: 1.50550/61.00\%, hp\_loss: 2.68377/31.00\%, j\_loss: 1.03043/73.00\%, 
		fr\_loss: 0.18376/81.00\%, p\_loss: 0.00019/100.00\%, 
		total weighted loss: 3.07254
	Training time elapsed: 112.65 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.54105/55.00\%, bp\_loss: 2.44301/28.00\%, hp\_loss: 3.07433/21.00\%, j\_loss: 1.85721/54.00\%, 
		fr\_loss: 0.20096/80.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 4.48390
	Part 2 - fp\_loss: 1.30221/64.00\%, bp\_loss: 2.13205/36.00\%, hp\_loss: 2.71224/30.00\%, j\_loss: 1.58583/64.00\%, 
		fr\_loss: 0.18195/81.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 3.87217
	Part 3 - fp\_loss: 1.29262/67.00\%, bp\_loss: 1.85927/48.00\%, hp\_loss: 2.44329/38.00\%, j\_loss: 1.30571/68.00\%, 
		fr\_loss: 0.20004/79.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 3.44283
	Part 4 - fp\_loss: 1.23770/65.00\%, bp\_loss: 1.19804/69.00\%, hp\_loss: 2.62674/31.00\%, j\_loss: 0.89209/74.00\%, 
		fr\_loss: 0.17072/83.00\%, p\_loss: 0.00013/100.00\%, 
		total weighted loss: 2.82911
	Training time elapsed: 149.84 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.45744/56.00\%, bp\_loss: 2.33790/33.00\%, hp\_loss: 3.06843/22.00\%, j\_loss: 1.75782/56.00\%, 
		fr\_loss: 0.21641/78.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 4.32485
	Part 2 - fp\_loss: 1.31958/62.00\%, bp\_loss: 2.17083/36.00\%, hp\_loss: 2.70064/32.00\%, j\_loss: 1.62165/62.00\%, 
		fr\_loss: 0.17227/82.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 3.91516
	Part 3 - fp\_loss: 1.20554/68.00\%, bp\_loss: 1.76409/49.00\%, hp\_loss: 2.42628/38.00\%, j\_loss: 1.27071/68.00\%, 
		fr\_loss: 0.17508/82.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 3.30567
	Part 4 - fp\_loss: 1.17961/67.00\%, bp\_loss: 1.30742/65.00\%, hp\_loss: 2.65692/31.00\%, j\_loss: 0.93914/76.00\%, 
		fr\_loss: 0.15942/84.00\%, p\_loss: 0.00017/100.00\%, 
		total weighted loss: 2.87768
	Training time elapsed: 187.06 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.48445/56.00\%, bp\_loss: 2.27096/36.00\%, hp\_loss: 3.08728/23.00\%, j\_loss: 1.73679/56.00\%, 
		fr\_loss: 0.20511/79.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 4.29161
	Part 2 - fp\_loss: 1.37901/62.00\%, bp\_loss: 2.12244/40.00\%, hp\_loss: 2.76037/28.00\%, j\_loss: 1.56805/61.00\%, 
		fr\_loss: 0.18016/82.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 3.90256
	Part 3 - fp\_loss: 1.22818/66.00\%, bp\_loss: 1.71511/53.00\%, hp\_loss: 2.30621/38.00\%, j\_loss: 1.24065/67.00\%, 
		fr\_loss: 0.16376/83.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 3.22489
	Part 4 - fp\_loss: 1.23417/68.00\%, bp\_loss: 1.16375/69.00\%, hp\_loss: 2.58547/33.00\%, j\_loss: 0.88785/77.00\%, 
		fr\_loss: 0.15982/84.00\%, p\_loss: 0.00014/100.00\%, 
		total weighted loss: 2.78953
	Training time elapsed: 224.22 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.42495/58.00\%, bp\_loss: 2.39155/35.00\%, hp\_loss: 3.06061/25.00\%, j\_loss: 1.72036/58.00\%, 
		fr\_loss: 0.19829/80.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 4.26677
	Part 2 - fp\_loss: 1.29075/64.00\%, bp\_loss: 2.03702/41.00\%, hp\_loss: 2.77442/30.00\%, j\_loss: 1.50081/63.00\%, 
		fr\_loss: 0.18348/81.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 3.77310
	Part 3 - fp\_loss: 1.19055/68.00\%, bp\_loss: 1.74456/52.00\%, hp\_loss: 2.44683/38.00\%, j\_loss: 1.22340/70.00\%, 
		fr\_loss: 0.16757/83.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 3.24367
	Part 4 - fp\_loss: 1.18822/67.00\%, bp\_loss: 1.09437/69.00\%, hp\_loss: 2.70115/30.00\%, j\_loss: 0.84188/80.00\%, 
		fr\_loss: 0.15507/84.00\%, p\_loss: 0.00011/100.00\%, 
		total weighted loss: 2.72973
	Training time elapsed: 261.42 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.47476/57.00\%, bp\_loss: 2.11828/44.00\%, hp\_loss: 3.04444/22.00\%, j\_loss: 1.60423/60.00\%, 
		fr\_loss: 0.20616/79.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 4.09658
	Part 2 - fp\_loss: 1.32241/63.00\%, bp\_loss: 1.99544/43.00\%, hp\_loss: 2.74799/31.00\%, j\_loss: 1.49169/62.00\%, 
		fr\_loss: 0.19492/80.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 3.77085
	Part 3 - fp\_loss: 1.28380/65.00\%, bp\_loss: 1.81652/50.00\%, hp\_loss: 2.44616/37.00\%, j\_loss: 1.33273/66.00\%, 
		fr\_loss: 0.18959/81.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.44303
	Part 4 - fp\_loss: 1.19343/67.00\%, bp\_loss: 1.46722/64.00\%, hp\_loss: 2.61699/34.00\%, j\_loss: 0.92641/72.00\%, 
		fr\_loss: 0.14681/85.00\%, p\_loss: 0.00016/100.00\%, 
		total weighted loss: 2.89521
	Training time elapsed: 298.62 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.59968/53.00\%, bp\_loss: 2.09663/42.00\%, hp\_loss: 3.08142/25.00\%, j\_loss: 1.81430/56.00\%, 
		fr\_loss: 0.23318/76.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 8.82524
	Part 2 - fp\_loss: 1.37661/59.00\%, bp\_loss: 1.93570/46.00\%, hp\_loss: 2.58080/42.00\%, j\_loss: 1.60721/59.00\%, 
		fr\_loss: 0.24338/75.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 7.74373
	Part 3 - fp\_loss: 1.10689/69.00\%, bp\_loss: 1.82677/52.00\%, hp\_loss: 1.99077/51.00\%, j\_loss: 1.29432/67.00\%, 
		fr\_loss: 0.18743/81.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 6.40620
	Part 4 - fp\_loss: 1.22289/66.00\%, bp\_loss: 2.02382/45.00\%, hp\_loss: 2.23357/44.00\%, j\_loss: 1.51628/59.00\%, 
		fr\_loss: 0.22656/77.00\%, p\_loss: 0.00010/100.00\%, 
		total weighted loss: 7.22321
	`Validation time elapsed: 0.73 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.65690/52.00\%, bp\_loss: 2.25222/41.00\%, hp\_loss: 3.10828/25.00\%, j\_loss: 1.86567/55.00\%, 
		fr\_loss: 0.25368/75.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 9.13679
	Part 2 - fp\_loss: 1.41730/59.00\%, bp\_loss: 2.13311/41.00\%, hp\_loss: 2.64123/38.00\%, j\_loss: 1.68831/59.00\%, 
		fr\_loss: 0.25081/74.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 8.13081
	Part 3 - fp\_loss: 1.12899/69.00\%, bp\_loss: 1.72787/57.00\%, hp\_loss: 1.95578/52.00\%, j\_loss: 1.15743/71.00\%, 
		fr\_loss: 0.20485/79.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 6.17494
	Part 4 - fp\_loss: 1.14473/68.00\%, bp\_loss: 1.50343/62.00\%, hp\_loss: 2.11013/47.00\%, j\_loss: 0.98612/69.00\%, 
		fr\_loss: 0.19752/80.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 5.94200
	`Validation time elapsed: 9.53 seconds
`
-----------
Completed epoch 2.

EPOCH 3
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.41573/57.00\%, bp\_loss: 2.00048/42.00\%, hp\_loss: 3.08046/22.00\%, j\_loss: 1.62663/60.00\%, 
		fr\_loss: 0.19387/80.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 4.05265
	Part 2 - fp\_loss: 1.29495/63.00\%, bp\_loss: 1.90879/45.00\%, hp\_loss: 2.78125/29.00\%, j\_loss: 1.47284/62.00\%, 
		fr\_loss: 0.19399/80.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 3.72132
	Part 3 - fp\_loss: 1.18675/68.00\%, bp\_loss: 1.77029/53.00\%, hp\_loss: 2.41738/37.00\%, j\_loss: 1.24402/70.00\%, 
		fr\_loss: 0.17878/82.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 3.27248
	Part 4 - fp\_loss: 1.20902/66.00\%, bp\_loss: 1.86328/51.00\%, hp\_loss: 2.65157/31.00\%, j\_loss: 1.33074/66.00\%, 
		fr\_loss: 0.13856/86.00\%, p\_loss: 0.00019/100.00\%, 
		total weighted loss: 3.42828
	Training time elapsed: 0.99 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.48234/58.00\%, bp\_loss: 2.09235/43.00\%, hp\_loss: 3.08260/22.00\%, j\_loss: 1.57185/61.00\%, 
		fr\_loss: 0.21146/78.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 4.07697
	Part 2 - fp\_loss: 1.28777/64.00\%, bp\_loss: 2.00844/44.00\%, hp\_loss: 2.74995/32.00\%, j\_loss: 1.43912/63.00\%, 
		fr\_loss: 0.18005/82.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 3.69057
	Part 3 - fp\_loss: 1.19617/68.00\%, bp\_loss: 1.70997/52.00\%, hp\_loss: 2.43747/37.00\%, j\_loss: 1.20720/68.00\%, 
		fr\_loss: 0.16989/83.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.21941
	Part 4 - fp\_loss: 1.16102/68.00\%, bp\_loss: 1.31427/65.00\%, hp\_loss: 2.60512/31.00\%, j\_loss: 0.85929/75.00\%, 
		fr\_loss: 0.15278/84.00\%, p\_loss: 0.00011/100.00\%, 
		total weighted loss: 2.76841
	Training time elapsed: 37.42 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.44461/57.00\%, bp\_loss: 1.95360/45.00\%, hp\_loss: 3.05307/23.00\%, j\_loss: 1.59362/60.00\%, 
		fr\_loss: 0.19835/80.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 4.01628
	Part 2 - fp\_loss: 1.32421/63.00\%, bp\_loss: 1.90114/46.00\%, hp\_loss: 2.71010/31.00\%, j\_loss: 1.45850/62.00\%, 
		fr\_loss: 0.18999/81.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 3.69397
	Part 3 - fp\_loss: 1.33730/62.00\%, bp\_loss: 1.87931/50.00\%, hp\_loss: 2.52951/34.00\%, j\_loss: 1.38950/65.00\%, 
		fr\_loss: 0.18890/81.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.56969
	Part 4 - fp\_loss: 1.14507/69.00\%, bp\_loss: 1.30807/67.00\%, hp\_loss: 2.65297/31.00\%, j\_loss: 0.83882/76.00\%, 
		fr\_loss: 0.14213/85.00\%, p\_loss: 0.00012/100.00\%, 
		total weighted loss: 2.74181
	Training time elapsed: 73.87 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.45904/57.00\%, bp\_loss: 1.97554/45.00\%, hp\_loss: 3.10478/22.00\%, j\_loss: 1.56671/62.00\%, 
		fr\_loss: 0.18353/81.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 4.00385
	Part 2 - fp\_loss: 1.32559/62.00\%, bp\_loss: 1.77327/52.00\%, hp\_loss: 2.78649/29.00\%, j\_loss: 1.40670/63.00\%, 
		fr\_loss: 0.19358/80.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 3.63100
	Part 3 - fp\_loss: 1.17545/68.00\%, bp\_loss: 1.68396/53.00\%, hp\_loss: 2.45057/37.00\%, j\_loss: 1.20255/70.00\%, 
		fr\_loss: 0.16399/84.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.19462
	Part 4 - fp\_loss: 1.18243/69.00\%, bp\_loss: 1.28327/68.00\%, hp\_loss: 2.61369/33.00\%, j\_loss: 0.81737/75.00\%, 
		fr\_loss: 0.16190/84.00\%, p\_loss: 0.00015/100.00\%, 
		total weighted loss: 2.73959
	Training time elapsed: 110.29 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.36650/60.00\%, bp\_loss: 1.90017/48.00\%, hp\_loss: 3.08875/22.00\%, j\_loss: 1.44694/63.00\%, 
		fr\_loss: 0.19034/80.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.81721
	Part 2 - fp\_loss: 1.16583/67.00\%, bp\_loss: 1.86812/48.00\%, hp\_loss: 2.71265/32.00\%, j\_loss: 1.26375/66.00\%, 
		fr\_loss: 0.17168/83.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 3.39258
	Part 3 - fp\_loss: 1.19005/68.00\%, bp\_loss: 1.68788/56.00\%, hp\_loss: 2.35386/39.00\%, j\_loss: 1.11776/70.00\%, 
		fr\_loss: 0.17712/82.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.10243
	Part 4 - fp\_loss: 1.17069/67.00\%, bp\_loss: 1.08376/72.00\%, hp\_loss: 2.58753/34.00\%, j\_loss: 0.77133/78.00\%, 
		fr\_loss: 0.14504/85.00\%, p\_loss: 0.00014/100.00\%, 
		total weighted loss: 2.60312
	Training time elapsed: 146.71 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.37057/61.00\%, bp\_loss: 2.07986/44.00\%, hp\_loss: 3.03408/23.00\%, j\_loss: 1.48769/63.00\%, 
		fr\_loss: 0.17993/81.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.88709
	Part 2 - fp\_loss: 1.28232/64.00\%, bp\_loss: 2.05630/47.00\%, hp\_loss: 2.69678/31.00\%, j\_loss: 1.42191/63.00\%, 
		fr\_loss: 0.17333/82.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 3.66233
	Part 3 - fp\_loss: 1.21810/66.00\%, bp\_loss: 1.58977/59.00\%, hp\_loss: 2.40134/38.00\%, j\_loss: 1.15631/70.00\%, 
		fr\_loss: 0.17840/82.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.14110
	Part 4 - fp\_loss: 1.11248/68.00\%, bp\_loss: 1.32873/66.00\%, hp\_loss: 2.54220/34.00\%, j\_loss: 0.93613/73.00\%, 
		fr\_loss: 0.15091/85.00\%, p\_loss: 0.00013/100.00\%, 
		total weighted loss: 2.80458
	Training time elapsed: 183.09 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.43006/60.00\%, bp\_loss: 1.87286/49.00\%, hp\_loss: 3.08458/22.00\%, j\_loss: 1.42956/64.00\%, 
		fr\_loss: 0.21384/79.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.84566
	Part 2 - fp\_loss: 1.28922/64.00\%, bp\_loss: 2.28271/37.00\%, hp\_loss: 2.75321/30.00\%, j\_loss: 1.52905/64.00\%, 
		fr\_loss: 0.19341/81.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 3.87784
	Part 3 - fp\_loss: 1.25986/65.00\%, bp\_loss: 1.44581/62.00\%, hp\_loss: 2.40593/35.00\%, j\_loss: 1.07985/69.00\%, 
		fr\_loss: 0.16606/83.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.03135
	Part 4 - fp\_loss: 1.20844/67.00\%, bp\_loss: 1.06758/73.00\%, hp\_loss: 2.55363/35.00\%, j\_loss: 0.73365/79.00\%, 
		fr\_loss: 0.16533/83.00\%, p\_loss: 0.00012/100.00\%, 
		total weighted loss: 2.58958
	Training time elapsed: 219.50 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.39433/60.00\%, bp\_loss: 1.83028/51.00\%, hp\_loss: 3.08764/22.00\%, j\_loss: 1.41810/65.00\%, 
		fr\_loss: 0.17694/82.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.76758
	Part 2 - fp\_loss: 1.32205/61.00\%, bp\_loss: 2.41314/35.00\%, hp\_loss: 2.71997/30.00\%, j\_loss: 1.66354/62.00\%, 
		fr\_loss: 0.17949/81.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 4.04399
	Part 3 - fp\_loss: 1.21328/67.00\%, bp\_loss: 1.40880/62.00\%, hp\_loss: 2.47118/34.00\%, j\_loss: 1.05485/70.00\%, 
		fr\_loss: 0.16425/83.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.98974
	Part 4 - fp\_loss: 1.14530/67.00\%, bp\_loss: 1.51644/62.00\%, hp\_loss: 2.58761/34.00\%, j\_loss: 0.96223/72.00\%, 
		fr\_loss: 0.16044/83.00\%, p\_loss: 0.00009/100.00\%, 
		total weighted loss: 2.92654
	Training time elapsed: 255.95 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.38022/61.00\%, bp\_loss: 1.76369/52.00\%, hp\_loss: 3.04990/23.00\%, j\_loss: 1.35863/63.00\%, 
		fr\_loss: 0.17341/82.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.66623
	Part 2 - fp\_loss: 1.29340/64.00\%, bp\_loss: 2.07448/43.00\%, hp\_loss: 2.71940/32.00\%, j\_loss: 1.44749/63.00\%, 
		fr\_loss: 0.18715/81.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 3.71950
	Part 3 - fp\_loss: 1.14900/67.00\%, bp\_loss: 1.53503/60.00\%, hp\_loss: 2.38568/38.00\%, j\_loss: 1.03250/69.00\%, 
		fr\_loss: 0.15135/85.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.93456
	Part 4 - fp\_loss: 1.15296/66.00\%, bp\_loss: 1.32979/67.00\%, hp\_loss: 2.55591/33.00\%, j\_loss: 0.89122/74.00\%, 
		fr\_loss: 0.15286/84.00\%, p\_loss: 0.00011/100.00\%, 
		total weighted loss: 2.78629
	Training time elapsed: 292.32 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.61699/54.00\%, bp\_loss: 1.89273/50.00\%, hp\_loss: 3.08000/28.00\%, j\_loss: 1.72121/56.00\%, 
		fr\_loss: 0.22955/77.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 8.54050
	Part 2 - fp\_loss: 1.43458/60.00\%, bp\_loss: 2.16363/45.00\%, hp\_loss: 2.56214/41.00\%, j\_loss: 1.62262/60.00\%, 
		fr\_loss: 0.24354/75.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 8.02655
	Part 3 - fp\_loss: 1.09732/69.00\%, bp\_loss: 1.77264/52.00\%, hp\_loss: 2.01864/50.00\%, j\_loss: 1.34582/63.00\%, 
		fr\_loss: 0.20503/79.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 6.43947
	Part 4 - fp\_loss: 1.22522/68.00\%, bp\_loss: 2.04567/49.00\%, hp\_loss: 2.21022/44.00\%, j\_loss: 1.31017/61.00\%, 
		fr\_loss: 0.21345/78.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 7.00479
	`Validation time elapsed: 0.73 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.70294/54.00\%, bp\_loss: 2.18401/47.00\%, hp\_loss: 3.13981/24.00\%, j\_loss: 1.69980/57.00\%, 
		fr\_loss: 0.23159/77.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 8.95816
	Part 2 - fp\_loss: 1.29496/63.00\%, bp\_loss: 2.20092/40.00\%, hp\_loss: 2.57718/42.00\%, j\_loss: 1.55554/62.00\%, 
		fr\_loss: 0.23030/77.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 7.85892
	Part 3 - fp\_loss: 1.03749/71.00\%, bp\_loss: 1.53547/64.00\%, hp\_loss: 1.99865/50.00\%, j\_loss: 0.96445/75.00\%, 
		fr\_loss: 0.17980/82.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 5.71587
	Part 4 - fp\_loss: 1.18480/67.00\%, bp\_loss: 1.55336/61.00\%, hp\_loss: 2.20256/43.00\%, j\_loss: 0.98227/71.00\%, 
		fr\_loss: 0.20432/79.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 6.12736
	`Validation time elapsed: 9.53 seconds
`
-----------
Completed epoch 3.

EPOCH 4
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.33995/61.00\%, bp\_loss: 1.69065/54.00\%, hp\_loss: 3.12339/22.00\%, j\_loss: 1.39204/63.00\%, 
		fr\_loss: 0.18497/81.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.69121
	Part 2 - fp\_loss: 1.31434/64.00\%, bp\_loss: 2.12024/44.00\%, hp\_loss: 2.68261/33.00\%, j\_loss: 1.47452/63.00\%, 
		fr\_loss: 0.19241/80.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.76496
	Part 3 - fp\_loss: 1.07142/71.00\%, bp\_loss: 1.63043/57.00\%, hp\_loss: 2.41804/37.00\%, j\_loss: 1.13437/69.00\%, 
		fr\_loss: 0.16060/84.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.04523
	Part 4 - fp\_loss: 1.14300/67.00\%, bp\_loss: 1.69919/60.00\%, hp\_loss: 2.54226/33.00\%, j\_loss: 1.02346/70.00\%, 
		fr\_loss: 0.14367/85.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 3.01106
	Training time elapsed: 1.01 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.40901/57.00\%, bp\_loss: 2.55557/34.00\%, hp\_loss: 3.05494/21.00\%, j\_loss: 1.74590/51.00\%, 
		fr\_loss: 0.20399/79.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 4.33754
	Part 2 - fp\_loss: 1.20991/64.00\%, bp\_loss: 2.10327/44.00\%, hp\_loss: 2.73244/32.00\%, j\_loss: 1.46680/64.00\%, 
		fr\_loss: 0.14429/85.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 3.66676
	Part 3 - fp\_loss: 1.11229/68.00\%, bp\_loss: 1.33630/65.00\%, hp\_loss: 2.53888/34.00\%, j\_loss: 0.98416/75.00\%, 
		fr\_loss: 0.14922/85.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.85208
	Part 4 - fp\_loss: 1.12012/70.00\%, bp\_loss: 1.38345/66.00\%, hp\_loss: 2.70029/31.00\%, j\_loss: 0.86892/77.00\%, 
		fr\_loss: 0.13544/86.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 2.78955
	Training time elapsed: 38.39 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.29560/62.00\%, bp\_loss: 1.86801/52.00\%, hp\_loss: 2.96088/26.00\%, j\_loss: 1.34533/64.00\%, 
		fr\_loss: 0.16691/83.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.60871
	Part 2 - fp\_loss: 1.20658/65.00\%, bp\_loss: 1.99171/46.00\%, hp\_loss: 2.73782/31.00\%, j\_loss: 1.32156/66.00\%, 
		fr\_loss: 0.16473/83.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.50845
	Part 3 - fp\_loss: 1.14337/68.00\%, bp\_loss: 1.32718/66.00\%, hp\_loss: 2.37451/38.00\%, j\_loss: 0.92369/71.00\%, 
		fr\_loss: 0.16348/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.76937
	Part 4 - fp\_loss: 1.14534/67.00\%, bp\_loss: 1.20741/69.00\%, hp\_loss: 2.58252/33.00\%, j\_loss: 0.76802/79.00\%, 
		fr\_loss: 0.14714/85.00\%, p\_loss: 0.00007/100.00\%, 
		total weighted loss: 2.62482
	Training time elapsed: 75.81 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.44071/56.00\%, bp\_loss: 1.79082/53.00\%, hp\_loss: 3.02245/22.00\%, j\_loss: 1.43849/62.00\%, 
		fr\_loss: 0.18645/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.78928
	Part 2 - fp\_loss: 1.28714/63.00\%, bp\_loss: 2.02055/44.00\%, hp\_loss: 2.79104/29.00\%, j\_loss: 1.41809/63.00\%, 
		fr\_loss: 0.17362/82.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.67876
	Part 3 - fp\_loss: 1.18558/68.00\%, bp\_loss: 1.25520/67.00\%, hp\_loss: 2.44003/36.00\%, j\_loss: 0.91916/75.00\%, 
		fr\_loss: 0.14447/85.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.76499
	Part 4 - fp\_loss: 1.19590/66.00\%, bp\_loss: 1.17803/71.00\%, hp\_loss: 2.64951/32.00\%, j\_loss: 0.85125/77.00\%, 
		fr\_loss: 0.14111/86.00\%, p\_loss: 0.00011/100.00\%, 
		total weighted loss: 2.73858
	Training time elapsed: 113.23 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.40420/59.00\%, bp\_loss: 2.15591/47.00\%, hp\_loss: 3.12289/22.00\%, j\_loss: 1.46527/61.00\%, 
		fr\_loss: 0.16766/83.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.91866
	Part 2 - fp\_loss: 1.26703/65.00\%, bp\_loss: 1.81720/50.00\%, hp\_loss: 2.75464/30.00\%, j\_loss: 1.26776/65.00\%, 
		fr\_loss: 0.17400/82.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.44683
	Part 3 - fp\_loss: 1.19003/68.00\%, bp\_loss: 1.23620/68.00\%, hp\_loss: 2.46512/36.00\%, j\_loss: 0.90420/75.00\%, 
		fr\_loss: 0.15813/84.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.76775
	Part 4 - fp\_loss: 1.11693/68.00\%, bp\_loss: 1.22700/68.00\%, hp\_loss: 2.63455/32.00\%, j\_loss: 0.79718/80.00\%, 
		fr\_loss: 0.15256/84.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 2.66667
	Training time elapsed: 150.61 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.38883/60.00\%, bp\_loss: 2.13380/48.00\%, hp\_loss: 3.16989/20.00\%, j\_loss: 1.45539/62.00\%, 
		fr\_loss: 0.18647/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.92739
	Part 2 - fp\_loss: 1.28947/61.00\%, bp\_loss: 1.92812/48.00\%, hp\_loss: 2.78709/29.00\%, j\_loss: 1.37969/63.00\%, 
		fr\_loss: 0.18633/81.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.62533
	Part 3 - fp\_loss: 1.11182/71.00\%, bp\_loss: 1.19997/69.00\%, hp\_loss: 2.46015/37.00\%, j\_loss: 0.82206/77.00\%, 
		fr\_loss: 0.18269/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.65869
	Part 4 - fp\_loss: 1.18199/68.00\%, bp\_loss: 1.23831/67.00\%, hp\_loss: 2.66995/31.00\%, j\_loss: 0.86799/74.00\%, 
		fr\_loss: 0.14738/85.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 2.77885
	Training time elapsed: 188.00 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.27175/62.00\%, bp\_loss: 1.72381/54.00\%, hp\_loss: 3.04147/22.00\%, j\_loss: 1.25442/68.00\%, 
		fr\_loss: 0.17903/82.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.49891
	Part 2 - fp\_loss: 1.33400/64.00\%, bp\_loss: 2.09828/44.00\%, hp\_loss: 2.79018/28.00\%, j\_loss: 1.37820/66.00\%, 
		fr\_loss: 0.17740/82.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.68915
	Part 3 - fp\_loss: 1.21017/67.00\%, bp\_loss: 1.23151/69.00\%, hp\_loss: 2.47169/35.00\%, j\_loss: 0.87850/75.00\%, 
		fr\_loss: 0.14991/84.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.74446
	Part 4 - fp\_loss: 1.10870/68.00\%, bp\_loss: 1.42192/67.00\%, hp\_loss: 2.67375/30.00\%, j\_loss: 0.83765/75.00\%, 
		fr\_loss: 0.13972/86.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 2.76043
	Training time elapsed: 225.38 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.46154/57.00\%, bp\_loss: 1.77056/54.00\%, hp\_loss: 3.04683/23.00\%, j\_loss: 1.46566/62.00\%, 
		fr\_loss: 0.20572/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.84737
	Part 2 - fp\_loss: 1.27486/63.00\%, bp\_loss: 1.87231/50.00\%, hp\_loss: 2.66070/32.00\%, j\_loss: 1.37377/63.00\%, 
		fr\_loss: 0.17960/82.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.55070
	Part 3 - fp\_loss: 1.16155/68.00\%, bp\_loss: 1.22112/70.00\%, hp\_loss: 2.42601/36.00\%, j\_loss: 0.90015/77.00\%, 
		fr\_loss: 0.16667/83.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.74173
	Part 4 - fp\_loss: 1.18123/66.00\%, bp\_loss: 1.29256/66.00\%, hp\_loss: 2.66709/32.00\%, j\_loss: 0.88963/75.00\%, 
		fr\_loss: 0.14868/85.00\%, p\_loss: 0.00007/100.00\%, 
		total weighted loss: 2.81683
	Training time elapsed: 262.79 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.38630/59.00\%, bp\_loss: 2.31555/43.00\%, hp\_loss: 3.12409/21.00\%, j\_loss: 1.55509/61.00\%, 
		fr\_loss: 0.17168/82.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 4.05182
	Part 2 - fp\_loss: 1.30741/62.00\%, bp\_loss: 1.73282/53.00\%, hp\_loss: 2.80155/28.00\%, j\_loss: 1.33900/66.00\%, 
		fr\_loss: 0.17739/82.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.53041
	Part 3 - fp\_loss: 1.18006/68.00\%, bp\_loss: 1.17859/69.00\%, hp\_loss: 2.40725/37.00\%, j\_loss: 0.91341/74.00\%, 
		fr\_loss: 0.16491/83.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.74411
	Part 4 - fp\_loss: 1.13933/68.00\%, bp\_loss: 1.19835/68.00\%, hp\_loss: 2.65000/31.00\%, j\_loss: 0.81276/76.00\%, 
		fr\_loss: 0.15553/84.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 2.69247
	Training time elapsed: 300.16 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.68672/51.00\%, bp\_loss: 2.06051/46.00\%, hp\_loss: 2.98979/29.00\%, j\_loss: 1.84600/52.00\%, 
		fr\_loss: 0.24156/75.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 8.82460
	Part 2 - fp\_loss: 1.37166/61.00\%, bp\_loss: 2.37527/42.00\%, hp\_loss: 2.59049/41.00\%, j\_loss: 1.70210/58.00\%, 
		fr\_loss: 0.23189/77.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 8.27142
	Part 3 - fp\_loss: 1.09988/69.00\%, bp\_loss: 1.64514/52.00\%, hp\_loss: 1.99331/51.00\%, j\_loss: 1.37611/62.00\%, 
		fr\_loss: 0.18940/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 6.30385
	Part 4 - fp\_loss: 1.19579/68.00\%, bp\_loss: 2.14927/47.00\%, hp\_loss: 2.26797/42.00\%, j\_loss: 1.39228/55.00\%, 
		fr\_loss: 0.19818/80.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 7.20354
	`Validation time elapsed: 0.71 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.77144/49.00\%, bp\_loss: 2.35391/43.00\%, hp\_loss: 2.98923/28.00\%, j\_loss: 1.86807/51.00\%, 
		fr\_loss: 0.25678/74.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 9.23944
	Part 2 - fp\_loss: 1.28178/61.00\%, bp\_loss: 1.87399/51.00\%, hp\_loss: 2.53419/42.00\%, j\_loss: 1.41500/63.00\%, 
		fr\_loss: 0.23131/77.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 7.33629
	Part 3 - fp\_loss: 1.20509/67.00\%, bp\_loss: 1.34261/67.00\%, hp\_loss: 2.04564/48.00\%, j\_loss: 1.01789/72.00\%, 
		fr\_loss: 0.22143/77.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 5.83267
	Part 4 - fp\_loss: 1.18597/69.00\%, bp\_loss: 1.35769/65.00\%, hp\_loss: 2.22374/46.00\%, j\_loss: 0.91564/71.00\%, 
		fr\_loss: 0.19188/80.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 5.87498
	`Validation time elapsed: 9.50 seconds
`
-----------
Completed epoch 4.

EPOCH 5
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.36007/60.00\%, bp\_loss: 2.06719/44.00\%, hp\_loss: 3.13062/21.00\%, j\_loss: 1.58020/62.00\%, 
		fr\_loss: 0.18721/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 4.00679
	Part 2 - fp\_loss: 1.24144/65.00\%, bp\_loss: 2.36589/41.00\%, hp\_loss: 2.74759/29.00\%, j\_loss: 1.53634/62.00\%, 
		fr\_loss: 0.17122/83.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.86232
	Part 3 - fp\_loss: 1.08603/71.00\%, bp\_loss: 1.53641/56.00\%, hp\_loss: 2.41113/37.00\%, j\_loss: 1.15276/66.00\%, 
		fr\_loss: 0.14564/85.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.02567
	Part 4 - fp\_loss: 1.03310/71.00\%, bp\_loss: 1.60379/62.00\%, hp\_loss: 2.61723/34.00\%, j\_loss: 0.94391/73.00\%, 
		fr\_loss: 0.13405/86.00\%, p\_loss: 0.00007/100.00\%, 
		total weighted loss: 2.86082
	Training time elapsed: 1.07 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.34423/59.00\%, bp\_loss: 1.93648/53.00\%, hp\_loss: 3.06448/22.00\%, j\_loss: 1.36779/63.00\%, 
		fr\_loss: 0.18490/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.72509
	Part 2 - fp\_loss: 1.25456/64.00\%, bp\_loss: 1.89996/48.00\%, hp\_loss: 2.79917/27.00\%, j\_loss: 1.35666/64.00\%, 
		fr\_loss: 0.16293/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.55661
	Part 3 - fp\_loss: 1.12753/68.00\%, bp\_loss: 1.23168/68.00\%, hp\_loss: 2.44926/37.00\%, j\_loss: 0.89301/74.00\%, 
		fr\_loss: 0.15051/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.71158
	Part 4 - fp\_loss: 1.08097/70.00\%, bp\_loss: 1.44406/65.00\%, hp\_loss: 2.61006/32.00\%, j\_loss: 0.84362/75.00\%, 
		fr\_loss: 0.12498/87.00\%, p\_loss: 0.00007/100.00\%, 
		total weighted loss: 2.72533
	Training time elapsed: 38.50 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.29199/62.00\%, bp\_loss: 1.87104/55.00\%, hp\_loss: 3.11556/21.00\%, j\_loss: 1.29263/65.00\%, 
		fr\_loss: 0.15375/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.58835
	Part 2 - fp\_loss: 1.25720/63.00\%, bp\_loss: 1.80697/51.00\%, hp\_loss: 2.76236/30.00\%, j\_loss: 1.31883/64.00\%, 
		fr\_loss: 0.18414/81.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.50238
	Part 3 - fp\_loss: 1.18871/69.00\%, bp\_loss: 1.12187/72.00\%, hp\_loss: 2.40516/37.00\%, j\_loss: 0.93917/78.00\%, 
		fr\_loss: 0.16669/83.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.75833
	Part 4 - fp\_loss: 1.06693/70.00\%, bp\_loss: 1.24700/69.00\%, hp\_loss: 2.63327/31.00\%, j\_loss: 0.74786/80.00\%, 
		fr\_loss: 0.13149/86.00\%, p\_loss: 0.00008/100.00\%, 
		total weighted loss: 2.57691
	Training time elapsed: 75.95 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.36072/60.00\%, bp\_loss: 1.84973/53.00\%, hp\_loss: 3.05698/22.00\%, j\_loss: 1.35604/64.00\%, 
		fr\_loss: 0.15612/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.66453
	Part 2 - fp\_loss: 1.26480/65.00\%, bp\_loss: 1.73269/54.00\%, hp\_loss: 2.72484/31.00\%, j\_loss: 1.25586/66.00\%, 
		fr\_loss: 0.16823/83.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.39374
	Part 3 - fp\_loss: 1.18584/69.00\%, bp\_loss: 1.15027/70.00\%, hp\_loss: 2.38234/37.00\%, j\_loss: 0.87400/76.00\%, 
		fr\_loss: 0.16796/83.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.69467
	Part 4 - fp\_loss: 1.09108/69.00\%, bp\_loss: 1.26833/68.00\%, hp\_loss: 2.59520/30.00\%, j\_loss: 0.80119/78.00\%, 
		fr\_loss: 0.14243/85.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 2.64822
	Training time elapsed: 113.34 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.40168/59.00\%, bp\_loss: 1.79420/55.00\%, hp\_loss: 3.06238/22.00\%, j\_loss: 1.29952/67.00\%, 
		fr\_loss: 0.18069/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.63802
	Part 2 - fp\_loss: 1.27846/64.00\%, bp\_loss: 1.61815/58.00\%, hp\_loss: 2.69670/32.00\%, j\_loss: 1.21653/65.00\%, 
		fr\_loss: 0.18922/81.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.33944
	Part 3 - fp\_loss: 1.23539/65.00\%, bp\_loss: 1.14669/71.00\%, hp\_loss: 2.45442/36.00\%, j\_loss: 0.84489/77.00\%, 
		fr\_loss: 0.16891/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.71183
	Part 4 - fp\_loss: 1.11641/68.00\%, bp\_loss: 1.26061/69.00\%, hp\_loss: 2.56006/34.00\%, j\_loss: 0.80758/77.00\%, 
		fr\_loss: 0.16875/83.00\%, p\_loss: 0.00007/100.00\%, 
		total weighted loss: 2.68074
	Training time elapsed: 150.75 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.36696/61.00\%, bp\_loss: 2.16280/48.00\%, hp\_loss: 3.04044/22.00\%, j\_loss: 1.51019/58.00\%, 
		fr\_loss: 0.17351/82.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.92815
	Part 2 - fp\_loss: 1.46040/61.00\%, bp\_loss: 1.71183/54.00\%, hp\_loss: 2.70540/31.00\%, j\_loss: 1.36249/61.00\%, 
		fr\_loss: 0.20798/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.62584
	Part 3 - fp\_loss: 1.14095/68.00\%, bp\_loss: 1.27554/69.00\%, hp\_loss: 2.45269/36.00\%, j\_loss: 0.89455/76.00\%, 
		fr\_loss: 0.15425/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.73775
	Part 4 - fp\_loss: 1.14808/70.00\%, bp\_loss: 1.32190/66.00\%, hp\_loss: 2.61405/31.00\%, j\_loss: 0.78905/76.00\%, 
		fr\_loss: 0.15105/84.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 2.69493
	Training time elapsed: 188.18 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.39493/58.00\%, bp\_loss: 1.73657/55.00\%, hp\_loss: 3.06656/22.00\%, j\_loss: 1.34397/65.00\%, 
		fr\_loss: 0.18599/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.66837
	Part 2 - fp\_loss: 2.11636/44.00\%, bp\_loss: 1.71980/54.00\%, hp\_loss: 2.75664/29.00\%, j\_loss: 1.78400/55.00\%, 
		fr\_loss: 0.30735/68.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 4.49246
	Part 3 - fp\_loss: 1.14212/68.00\%, bp\_loss: 1.50395/60.00\%, hp\_loss: 2.43232/38.00\%, j\_loss: 1.03220/71.00\%, 
		fr\_loss: 0.15929/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.94343
	Part 4 - fp\_loss: 1.18857/68.00\%, bp\_loss: 1.16318/69.00\%, hp\_loss: 2.67631/32.00\%, j\_loss: 0.80256/76.00\%, 
		fr\_loss: 0.15992/84.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 2.70862
	Training time elapsed: 225.62 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.40442/62.00\%, bp\_loss: 1.96201/54.00\%, hp\_loss: 3.01666/23.00\%, j\_loss: 1.34061/63.00\%, 
		fr\_loss: 0.17545/82.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.71187
	Part 2 - fp\_loss: 1.96276/51.00\%, bp\_loss: 1.50957/62.00\%, hp\_loss: 2.68910/31.00\%, j\_loss: 1.41931/62.00\%, 
		fr\_loss: 0.26253/74.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.92282
	Part 3 - fp\_loss: 1.16729/68.00\%, bp\_loss: 1.09388/71.00\%, hp\_loss: 2.39846/36.00\%, j\_loss: 0.75903/78.00\%, 
		fr\_loss: 0.15120/85.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.54158
	Part 4 - fp\_loss: 1.15500/67.00\%, bp\_loss: 1.33842/65.00\%, hp\_loss: 2.58732/30.00\%, j\_loss: 0.90255/75.00\%, 
		fr\_loss: 0.13668/86.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 2.79446
	Training time elapsed: 263.05 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.34860/60.00\%, bp\_loss: 1.67266/58.00\%, hp\_loss: 3.06298/22.00\%, j\_loss: 1.22450/68.00\%, 
		fr\_loss: 0.17656/82.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.49605
	Part 2 - fp\_loss: 1.84780/54.00\%, bp\_loss: 1.63946/58.00\%, hp\_loss: 2.73095/30.00\%, j\_loss: 1.42282/62.00\%, 
		fr\_loss: 0.26728/73.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.92513
	Part 3 - fp\_loss: 1.12633/69.00\%, bp\_loss: 1.06410/72.00\%, hp\_loss: 2.39184/37.00\%, j\_loss: 0.78098/78.00\%, 
		fr\_loss: 0.13852/86.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.51944
	Part 4 - fp\_loss: 1.19013/69.00\%, bp\_loss: 1.24872/68.00\%, hp\_loss: 2.61884/31.00\%, j\_loss: 0.75339/80.00\%, 
		fr\_loss: 0.14794/85.00\%, p\_loss: 0.00006/100.00\%, 
		total weighted loss: 2.65667
	Training time elapsed: 300.45 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.73759/51.00\%, bp\_loss: 2.05511/45.00\%, hp\_loss: 3.11502/26.00\%, j\_loss: 1.84734/55.00\%, 
		fr\_loss: 0.21262/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 8.96770
	Part 2 - fp\_loss: 1.64926/58.00\%, bp\_loss: 2.13224/45.00\%, hp\_loss: 2.55332/43.00\%, j\_loss: 1.69386/48.00\%, 
		fr\_loss: 0.28250/72.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 8.31119
	Part 3 - fp\_loss: 1.11119/70.00\%, bp\_loss: 1.57120/58.00\%, hp\_loss: 1.95801/50.00\%, j\_loss: 1.15395/67.00\%, 
		fr\_loss: 0.19940/80.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 5.99377
	Part 4 - fp\_loss: 1.13952/70.00\%, bp\_loss: 2.17161/53.00\%, hp\_loss: 2.22931/45.00\%, j\_loss: 1.33182/58.00\%, 
		fr\_loss: 0.21602/78.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 7.08830
	`Validation time elapsed: 0.72 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.68967/52.00\%, bp\_loss: 2.00967/50.00\%, hp\_loss: 3.05677/26.00\%, j\_loss: 1.68194/58.00\%, 
		fr\_loss: 0.24024/76.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 8.67832
	Part 2 - fp\_loss: 1.67539/56.00\%, bp\_loss: 1.78122/56.00\%, hp\_loss: 2.57458/41.00\%, j\_loss: 1.51318/58.00\%, 
		fr\_loss: 0.32263/67.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 7.86702
	Part 3 - fp\_loss: 1.16154/66.00\%, bp\_loss: 1.26855/70.00\%, hp\_loss: 1.99197/50.00\%, j\_loss: 0.90571/74.00\%, 
		fr\_loss: 0.20440/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 5.53218
	Part 4 - fp\_loss: 1.16455/69.00\%, bp\_loss: 1.33759/68.00\%, hp\_loss: 2.20929/44.00\%, j\_loss: 0.77556/79.00\%, 
		fr\_loss: 0.20129/80.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 5.68832
	`Validation time elapsed: 9.52 seconds
`
-----------
Completed epoch 5.

EPOCH 6
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.30876/61.00\%, bp\_loss: 1.85111/50.00\%, hp\_loss: 3.02380/23.00\%, j\_loss: 1.47796/63.00\%, 
		fr\_loss: 0.18690/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.78172
	Part 2 - fp\_loss: 1.86593/50.00\%, bp\_loss: 2.00150/48.00\%, hp\_loss: 2.76107/29.00\%, j\_loss: 1.72270/50.00\%, 
		fr\_loss: 0.28097/72.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 4.36541
	Part 3 - fp\_loss: 1.24553/66.00\%, bp\_loss: 1.29966/64.00\%, hp\_loss: 2.47025/35.00\%, j\_loss: 0.97029/75.00\%, 
		fr\_loss: 0.15308/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.87711
	Part 4 - fp\_loss: 1.28526/65.00\%, bp\_loss: 1.66907/63.00\%, hp\_loss: 2.63007/32.00\%, j\_loss: 1.09005/69.00\%, 
		fr\_loss: 0.15700/84.00\%, p\_loss: 0.00007/100.00\%, 
		total weighted loss: 3.17943
	Training time elapsed: 1.02 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.33797/62.00\%, bp\_loss: 1.58428/59.00\%, hp\_loss: 3.06772/23.00\%, j\_loss: 1.13961/70.00\%, 
		fr\_loss: 0.17521/82.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.37941
	Part 2 - fp\_loss: 2.07737/47.00\%, bp\_loss: 1.59969/60.00\%, hp\_loss: 2.67804/32.00\%, j\_loss: 1.56949/61.00\%, 
		fr\_loss: 0.31144/69.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 4.20294
	Part 3 - fp\_loss: 1.16482/70.00\%, bp\_loss: 1.17463/71.00\%, hp\_loss: 2.41832/36.00\%, j\_loss: 0.80785/77.00\%, 
		fr\_loss: 0.16063/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.62878
	Part 4 - fp\_loss: 1.06353/69.00\%, bp\_loss: 1.18512/70.00\%, hp\_loss: 2.58084/33.00\%, j\_loss: 0.73721/80.00\%, 
		fr\_loss: 0.13234/87.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 2.53111
	Training time elapsed: 37.75 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.39324/59.00\%, bp\_loss: 1.90236/54.00\%, hp\_loss: 3.05131/22.00\%, j\_loss: 1.35110/64.00\%, 
		fr\_loss: 0.17866/82.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.71248
	Part 2 - fp\_loss: 2.15528/47.00\%, bp\_loss: 1.59773/58.00\%, hp\_loss: 2.71408/30.00\%, j\_loss: 1.65133/59.00\%, 
		fr\_loss: 0.34041/66.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 4.36293
	Part 3 - fp\_loss: 1.15954/68.00\%, bp\_loss: 1.02552/72.00\%, hp\_loss: 2.49213/35.00\%, j\_loss: 0.78342/77.00\%, 
		fr\_loss: 0.13323/87.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.55172
	Part 4 - fp\_loss: 1.14603/68.00\%, bp\_loss: 1.11751/71.00\%, hp\_loss: 2.66185/32.00\%, j\_loss: 0.74303/78.00\%, 
		fr\_loss: 0.15616/84.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 2.60602
	Training time elapsed: 74.48 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.33027/59.00\%, bp\_loss: 1.76166/54.00\%, hp\_loss: 3.08351/22.00\%, j\_loss: 1.42117/63.00\%, 
		fr\_loss: 0.16042/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.70028
	Part 2 - fp\_loss: 2.60686/37.00\%, bp\_loss: 1.53386/62.00\%, hp\_loss: 2.79184/29.00\%, j\_loss: 1.69369/60.00\%, 
		fr\_loss: 0.40019/59.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 4.69502
	Part 3 - fp\_loss: 1.22549/66.00\%, bp\_loss: 1.07717/74.00\%, hp\_loss: 2.42349/35.00\%, j\_loss: 0.77072/78.00\%, 
		fr\_loss: 0.16607/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.59973
	Part 4 - fp\_loss: 1.24637/66.00\%, bp\_loss: 1.12974/71.00\%, hp\_loss: 2.57930/34.00\%, j\_loss: 0.83162/79.00\%, 
		fr\_loss: 0.17855/82.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 2.74607
	Training time elapsed: 111.17 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.39401/59.00\%, bp\_loss: 1.63174/55.00\%, hp\_loss: 3.12382/20.00\%, j\_loss: 1.30054/68.00\%, 
		fr\_loss: 0.18121/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.60543
	Part 2 - fp\_loss: 2.08452/47.00\%, bp\_loss: 1.46763/62.00\%, hp\_loss: 2.77664/27.00\%, j\_loss: 1.50740/62.00\%, 
		fr\_loss: 0.37695/62.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 4.19989
	Part 3 - fp\_loss: 1.10781/70.00\%, bp\_loss: 1.03465/76.00\%, hp\_loss: 2.48109/36.00\%, j\_loss: 0.72459/80.00\%, 
		fr\_loss: 0.14633/85.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.47954
	Part 4 - fp\_loss: 1.18445/68.00\%, bp\_loss: 1.06715/73.00\%, hp\_loss: 2.70290/31.00\%, j\_loss: 0.66503/81.00\%, 
		fr\_loss: 0.16745/83.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 2.55572
	Training time elapsed: 147.97 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.37958/60.00\%, bp\_loss: 1.75285/56.00\%, hp\_loss: 3.06724/23.00\%, j\_loss: 1.28597/64.00\%, 
		fr\_loss: 0.20367/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.62546
	Part 2 - fp\_loss: 2.06681/50.00\%, bp\_loss: 1.42542/64.00\%, hp\_loss: 2.73503/31.00\%, j\_loss: 1.35758/64.00\%, 
		fr\_loss: 0.35843/64.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.99756
	Part 3 - fp\_loss: 1.12617/68.00\%, bp\_loss: 0.95790/75.00\%, hp\_loss: 2.40248/38.00\%, j\_loss: 0.74405/79.00\%, 
		fr\_loss: 0.14751/85.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.46275
	Part 4 - fp\_loss: 1.13457/68.00\%, bp\_loss: 1.23098/68.00\%, hp\_loss: 2.58242/35.00\%, j\_loss: 0.77675/78.00\%, 
		fr\_loss: 0.16440/83.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 2.65246
	Training time elapsed: 184.68 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.41908/58.00\%, bp\_loss: 1.55040/59.00\%, hp\_loss: 3.06629/21.00\%, j\_loss: 1.22543/69.00\%, 
		fr\_loss: 0.17092/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.49091
	Part 2 - fp\_loss: 2.05382/48.00\%, bp\_loss: 1.43350/63.00\%, hp\_loss: 2.69657/30.00\%, j\_loss: 1.49543/63.00\%, 
		fr\_loss: 0.34295/65.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 4.10431
	Part 3 - fp\_loss: 1.14857/67.00\%, bp\_loss: 1.08266/73.00\%, hp\_loss: 2.44921/36.00\%, j\_loss: 0.79365/80.00\%, 
		fr\_loss: 0.13984/85.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.56733
	Part 4 - fp\_loss: 1.22606/67.00\%, bp\_loss: 1.79696/58.00\%, hp\_loss: 2.70738/29.00\%, j\_loss: 1.09525/70.00\%, 
		fr\_loss: 0.13876/86.00\%, p\_loss: 0.00005/100.00\%, 
		total weighted loss: 3.19836
	Training time elapsed: 221.42 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.35906/60.00\%, bp\_loss: 1.50684/60.00\%, hp\_loss: 3.04849/22.00\%, j\_loss: 1.22765/70.00\%, 
		fr\_loss: 0.17374/82.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.44752
	Part 2 - fp\_loss: 2.01661/50.00\%, bp\_loss: 1.35053/65.00\%, hp\_loss: 2.72888/30.00\%, j\_loss: 1.33552/65.00\%, 
		fr\_loss: 0.31299/69.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.88064
	Part 3 - fp\_loss: 1.12966/68.00\%, bp\_loss: 0.98623/75.00\%, hp\_loss: 2.30721/39.00\%, j\_loss: 0.74864/79.00\%, 
		fr\_loss: 0.15295/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.45445
	Part 4 - fp\_loss: 1.15998/67.00\%, bp\_loss: 1.18027/72.00\%, hp\_loss: 2.59504/33.00\%, j\_loss: 0.70012/80.00\%, 
		fr\_loss: 0.14972/85.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 2.56242
	Training time elapsed: 258.13 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.43518/56.00\%, bp\_loss: 1.49299/59.00\%, hp\_loss: 3.08243/21.00\%, j\_loss: 1.29593/67.00\%, 
		fr\_loss: 0.18828/80.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.57443
	Part 2 - fp\_loss: 1.99997/48.00\%, bp\_loss: 1.52773/60.00\%, hp\_loss: 2.77088/30.00\%, j\_loss: 1.60126/60.00\%, 
		fr\_loss: 0.31331/69.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 4.20414
	Part 3 - fp\_loss: 1.16564/69.00\%, bp\_loss: 1.01476/74.00\%, hp\_loss: 2.38919/38.00\%, j\_loss: 0.74741/78.00\%, 
		fr\_loss: 0.16180/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.51321
	Part 4 - fp\_loss: 1.19147/66.00\%, bp\_loss: 1.18067/69.00\%, hp\_loss: 2.62665/32.00\%, j\_loss: 0.84264/78.00\%, 
		fr\_loss: 0.14682/85.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 2.72739
	Training time elapsed: 294.83 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.70655/53.00\%, bp\_loss: 1.90952/51.00\%, hp\_loss: 3.09823/25.00\%, j\_loss: 1.73171/56.00\%, 
		fr\_loss: 0.25513/74.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 8.70115
	Part 2 - fp\_loss: 1.79096/50.00\%, bp\_loss: 1.93365/48.00\%, hp\_loss: 2.56773/42.00\%, j\_loss: 1.93664/48.00\%, 
		fr\_loss: 0.31393/69.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 8.54292
	Part 3 - fp\_loss: 1.07476/72.00\%, bp\_loss: 1.57463/61.00\%, hp\_loss: 1.96815/51.00\%, j\_loss: 1.04307/68.00\%, 
		fr\_loss: 0.19594/80.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 5.85656
	Part 4 - fp\_loss: 1.16389/66.00\%, bp\_loss: 1.96812/51.00\%, hp\_loss: 2.24856/44.00\%, j\_loss: 1.36298/61.00\%, 
		fr\_loss: 0.21757/78.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 6.96115
	`Validation time elapsed: 0.74 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.75527/52.00\%, bp\_loss: 1.66667/56.00\%, hp\_loss: 3.04146/28.00\%, j\_loss: 1.59464/58.00\%, 
		fr\_loss: 0.27020/72.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 8.32825
	Part 2 - fp\_loss: 1.70533/55.00\%, bp\_loss: 1.88647/55.00\%, hp\_loss: 2.57002/41.00\%, j\_loss: 1.72924/55.00\%, 
		fr\_loss: 0.33590/67.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 8.22697
	Part 3 - fp\_loss: 1.12171/69.00\%, bp\_loss: 1.25837/70.00\%, hp\_loss: 1.90949/52.00\%, j\_loss: 0.82903/75.00\%, 
		fr\_loss: 0.19935/80.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 5.31795
	Part 4 - fp\_loss: 1.09492/68.00\%, bp\_loss: 1.55533/63.00\%, hp\_loss: 2.14836/47.00\%, j\_loss: 0.94744/71.00\%, 
		fr\_loss: 0.21295/78.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 5.95904
	`Validation time elapsed: 9.52 seconds
`
-----------
Completed epoch 6.

EPOCH 7
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.41097/59.00\%, bp\_loss: 1.54405/59.00\%, hp\_loss: 3.07698/21.00\%, j\_loss: 1.33418/65.00\%, 
		fr\_loss: 0.16837/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.59434
	Part 2 - fp\_loss: 1.96750/50.00\%, bp\_loss: 1.93549/48.00\%, hp\_loss: 2.73934/29.00\%, j\_loss: 1.83658/50.00\%, 
		fr\_loss: 0.29436/70.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 4.51714
	Part 3 - fp\_loss: 1.11194/69.00\%, bp\_loss: 1.38061/65.00\%, hp\_loss: 2.41034/36.00\%, j\_loss: 0.93834/71.00\%, 
		fr\_loss: 0.14775/85.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.77935
	Part 4 - fp\_loss: 1.15486/67.00\%, bp\_loss: 1.59165/58.00\%, hp\_loss: 2.56416/33.00\%, j\_loss: 1.06002/71.00\%, 
		fr\_loss: 0.15497/84.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 3.03916
	Training time elapsed: 1.06 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.34388/59.00\%, bp\_loss: 1.46087/60.00\%, hp\_loss: 3.07882/20.00\%, j\_loss: 1.10572/73.00\%, 
		fr\_loss: 0.15881/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.29838
	Part 2 - fp\_loss: 1.91613/50.00\%, bp\_loss: 1.46071/64.00\%, hp\_loss: 2.71019/31.00\%, j\_loss: 1.39507/65.00\%, 
		fr\_loss: 0.29707/71.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.90148
	Part 3 - fp\_loss: 1.11529/69.00\%, bp\_loss: 1.00296/74.00\%, hp\_loss: 2.41864/36.00\%, j\_loss: 0.71578/80.00\%, 
		fr\_loss: 0.15061/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.45051
	Part 4 - fp\_loss: 1.13818/68.00\%, bp\_loss: 1.26552/68.00\%, hp\_loss: 2.61660/32.00\%, j\_loss: 0.79570/78.00\%, 
		fr\_loss: 0.14519/85.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.67462
	Training time elapsed: 38.47 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.35279/59.00\%, bp\_loss: 2.52432/47.00\%, hp\_loss: 3.09261/21.00\%, j\_loss: 1.49729/59.00\%, 
		fr\_loss: 0.16590/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 4.02467
	Part 2 - fp\_loss: 1.88315/52.00\%, bp\_loss: 1.45390/64.00\%, hp\_loss: 2.76828/31.00\%, j\_loss: 1.34441/64.00\%, 
		fr\_loss: 0.30048/70.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.85312
	Part 3 - fp\_loss: 1.15929/69.00\%, bp\_loss: 0.99666/75.00\%, hp\_loss: 2.46472/36.00\%, j\_loss: 0.78115/78.00\%, 
		fr\_loss: 0.14167/86.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.54088
	Part 4 - fp\_loss: 1.09297/70.00\%, bp\_loss: 1.14172/71.00\%, hp\_loss: 2.69084/30.00\%, j\_loss: 0.71601/81.00\%, 
		fr\_loss: 0.13316/87.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.54542
	Training time elapsed: 75.85 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.26854/61.00\%, bp\_loss: 1.74579/57.00\%, hp\_loss: 3.09309/21.00\%, j\_loss: 1.20076/68.00\%, 
		fr\_loss: 0.20255/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.48924
	Part 2 - fp\_loss: 1.84193/51.00\%, bp\_loss: 1.44011/64.00\%, hp\_loss: 2.71153/31.00\%, j\_loss: 1.33760/64.00\%, 
		fr\_loss: 0.26353/73.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.76759
	Part 3 - fp\_loss: 1.13977/68.00\%, bp\_loss: 0.91154/77.00\%, hp\_loss: 2.43114/36.00\%, j\_loss: 0.71376/79.00\%, 
		fr\_loss: 0.13201/87.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.41845
	Part 4 - fp\_loss: 1.11155/68.00\%, bp\_loss: 1.26494/68.00\%, hp\_loss: 2.61466/31.00\%, j\_loss: 0.76753/78.00\%, 
		fr\_loss: 0.13259/86.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 2.61978
	Training time elapsed: 113.22 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.39704/59.00\%, bp\_loss: 1.85975/56.00\%, hp\_loss: 3.09611/21.00\%, j\_loss: 1.34181/65.00\%, 
		fr\_loss: 0.17209/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.69918
	Part 2 - fp\_loss: 1.81984/52.00\%, bp\_loss: 1.33873/65.00\%, hp\_loss: 2.68473/32.00\%, j\_loss: 1.33056/65.00\%, 
		fr\_loss: 0.28060/72.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.72812
	Part 3 - fp\_loss: 1.14588/69.00\%, bp\_loss: 1.05986/73.00\%, hp\_loss: 2.44833/35.00\%, j\_loss: 0.75678/76.00\%, 
		fr\_loss: 0.16335/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.54552
	Part 4 - fp\_loss: 1.16989/67.00\%, bp\_loss: 1.20773/69.00\%, hp\_loss: 2.67309/31.00\%, j\_loss: 0.75462/78.00\%, 
		fr\_loss: 0.16780/83.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 2.67161
	Training time elapsed: 150.58 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.38249/60.00\%, bp\_loss: 1.75339/57.00\%, hp\_loss: 3.07121/22.00\%, j\_loss: 1.28966/68.00\%, 
		fr\_loss: 0.17573/82.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.60402
	Part 2 - fp\_loss: 1.97001/50.00\%, bp\_loss: 1.44482/62.00\%, hp\_loss: 2.75734/30.00\%, j\_loss: 1.52265/62.00\%, 
		fr\_loss: 0.28274/71.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 4.05105
	Part 3 - fp\_loss: 1.23494/66.00\%, bp\_loss: 0.98282/75.00\%, hp\_loss: 2.42590/37.00\%, j\_loss: 0.74311/78.00\%, 
		fr\_loss: 0.14880/85.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.53200
	Part 4 - fp\_loss: 1.14725/68.00\%, bp\_loss: 0.97591/75.00\%, hp\_loss: 2.59646/33.00\%, j\_loss: 0.68760/80.00\%, 
		fr\_loss: 0.13253/87.00\%, p\_loss: 0.00004/100.00\%, 
		total weighted loss: 2.46547
	Training time elapsed: 187.97 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.46741/56.00\%, bp\_loss: 1.69687/57.00\%, hp\_loss: 3.12110/20.00\%, j\_loss: 1.35849/66.00\%, 
		fr\_loss: 0.16652/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.70411
	Part 2 - fp\_loss: 1.94916/50.00\%, bp\_loss: 1.41328/64.00\%, hp\_loss: 2.70804/30.00\%, j\_loss: 1.38550/63.00\%, 
		fr\_loss: 0.29934/69.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.89582
	Part 3 - fp\_loss: 1.19983/65.00\%, bp\_loss: 1.02333/75.00\%, hp\_loss: 2.34572/39.00\%, j\_loss: 0.80840/75.00\%, 
		fr\_loss: 0.16134/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.58037
	Part 4 - fp\_loss: 1.19706/69.00\%, bp\_loss: 1.06899/71.00\%, hp\_loss: 2.65556/32.00\%, j\_loss: 0.69645/80.00\%, 
		fr\_loss: 0.16688/83.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.57922
	Training time elapsed: 225.37 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.28354/62.00\%, bp\_loss: 1.69570/57.00\%, hp\_loss: 3.05793/23.00\%, j\_loss: 1.25028/68.00\%, 
		fr\_loss: 0.16580/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.48394
	Part 2 - fp\_loss: 1.81128/52.00\%, bp\_loss: 1.47999/62.00\%, hp\_loss: 2.67327/32.00\%, j\_loss: 1.35514/63.00\%, 
		fr\_loss: 0.27142/72.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.77817
	Part 3 - fp\_loss: 1.12966/69.00\%, bp\_loss: 0.93274/76.00\%, hp\_loss: 2.37435/37.00\%, j\_loss: 0.74049/79.00\%, 
		fr\_loss: 0.14224/85.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.43969
	Part 4 - fp\_loss: 1.10289/68.00\%, bp\_loss: 1.05586/72.00\%, hp\_loss: 2.55085/35.00\%, j\_loss: 0.74333/79.00\%, 
		fr\_loss: 0.14413/85.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.52092
	Training time elapsed: 262.73 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.35913/60.00\%, bp\_loss: 1.67828/57.00\%, hp\_loss: 3.07780/21.00\%, j\_loss: 1.21701/70.00\%, 
		fr\_loss: 0.17072/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.49412
	Part 2 - fp\_loss: 2.01295/48.00\%, bp\_loss: 1.45805/63.00\%, hp\_loss: 2.80384/28.00\%, j\_loss: 1.41412/64.00\%, 
		fr\_loss: 0.30421/69.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 4.00337
	Part 3 - fp\_loss: 1.18764/68.00\%, bp\_loss: 0.93428/75.00\%, hp\_loss: 2.34926/38.00\%, j\_loss: 0.69983/81.00\%, 
		fr\_loss: 0.14591/85.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.42462
	Part 4 - fp\_loss: 1.22770/67.00\%, bp\_loss: 0.99701/73.00\%, hp\_loss: 2.63324/32.00\%, j\_loss: 0.71523/80.00\%, 
		fr\_loss: 0.14699/85.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.56514
	Training time elapsed: 300.11 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.87143/47.00\%, bp\_loss: 1.96724/49.00\%, hp\_loss: 3.09714/24.00\%, j\_loss: 1.84091/55.00\%, 
		fr\_loss: 0.26309/73.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 9.03982
	Part 2 - fp\_loss: 1.82594/52.00\%, bp\_loss: 2.20396/45.00\%, hp\_loss: 2.51237/43.00\%, j\_loss: 1.95736/45.00\%, 
		fr\_loss: 0.33090/67.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 8.83053
	Part 3 - fp\_loss: 1.08793/69.00\%, bp\_loss: 1.36108/67.00\%, hp\_loss: 1.97380/52.00\%, j\_loss: 1.01490/73.00\%, 
		fr\_loss: 0.18468/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 5.62241
	Part 4 - fp\_loss: 1.22950/66.00\%, bp\_loss: 1.61550/62.00\%, hp\_loss: 2.19956/47.00\%, j\_loss: 1.19965/64.00\%, 
		fr\_loss: 0.23214/77.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 6.47637
	`Validation time elapsed: 0.72 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.74372/51.00\%, bp\_loss: 1.97906/49.00\%, hp\_loss: 3.09641/24.00\%, j\_loss: 1.74320/56.00\%, 
		fr\_loss: 0.23923/76.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 8.80162
	Part 2 - fp\_loss: 1.76074/53.00\%, bp\_loss: 1.54227/62.00\%, hp\_loss: 2.65034/40.00\%, j\_loss: 1.44169/63.00\%, 
		fr\_loss: 0.33225/67.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 7.72731
	Part 3 - fp\_loss: 1.22656/66.00\%, bp\_loss: 1.29968/68.00\%, hp\_loss: 2.08964/46.00\%, j\_loss: 0.92138/75.00\%, 
		fr\_loss: 0.21595/78.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 5.75321
	Part 4 - fp\_loss: 1.18933/68.00\%, bp\_loss: 1.45442/64.00\%, hp\_loss: 2.32085/42.00\%, j\_loss: 0.87643/75.00\%, 
		fr\_loss: 0.20199/80.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 6.04303
	`Validation time elapsed: 9.49 seconds
`
-----------
Completed epoch 7.

EPOCH 8
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.32890/61.00\%, bp\_loss: 1.67517/57.00\%, hp\_loss: 3.01113/23.00\%, j\_loss: 1.25378/68.00\%, 
		fr\_loss: 0.18366/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.50778
	Part 2 - fp\_loss: 1.95308/50.00\%, bp\_loss: 1.72633/56.00\%, hp\_loss: 2.74612/32.00\%, j\_loss: 1.57337/56.00\%, 
		fr\_loss: 0.30773/69.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 4.19937
	Part 3 - fp\_loss: 1.17670/67.00\%, bp\_loss: 1.30301/68.00\%, hp\_loss: 2.41111/35.00\%, j\_loss: 0.96455/75.00\%, 
		fr\_loss: 0.14144/85.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.80858
	Part 4 - fp\_loss: 1.05485/70.00\%, bp\_loss: 1.30935/69.00\%, hp\_loss: 2.60048/33.00\%, j\_loss: 0.87893/75.00\%, 
		fr\_loss: 0.14025/86.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.71955
	Training time elapsed: 1.00 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.37058/59.00\%, bp\_loss: 1.68522/58.00\%, hp\_loss: 3.11772/21.00\%, j\_loss: 1.22166/69.00\%, 
		fr\_loss: 0.16922/83.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.51705
	Part 2 - fp\_loss: 1.93231/50.00\%, bp\_loss: 1.45084/65.00\%, hp\_loss: 2.79056/29.00\%, j\_loss: 1.36632/66.00\%, 
		fr\_loss: 0.28769/71.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.89258
	Part 3 - fp\_loss: 1.08510/69.00\%, bp\_loss: 1.00056/74.00\%, hp\_loss: 2.35442/38.00\%, j\_loss: 0.72814/79.00\%, 
		fr\_loss: 0.13974/86.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.41692
	Part 4 - fp\_loss: 1.02184/71.00\%, bp\_loss: 1.10944/70.00\%, hp\_loss: 2.55950/33.00\%, j\_loss: 0.69577/81.00\%, 
		fr\_loss: 0.13092/87.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.43829
	Training time elapsed: 38.42 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.36853/59.00\%, bp\_loss: 1.69184/55.00\%, hp\_loss: 3.07739/21.00\%, j\_loss: 1.31846/66.00\%, 
		fr\_loss: 0.15945/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.59294
	Part 2 - fp\_loss: 1.91781/51.00\%, bp\_loss: 1.43500/63.00\%, hp\_loss: 2.78009/28.00\%, j\_loss: 1.38663/65.00\%, 
		fr\_loss: 0.28841/71.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.89848
	Part 3 - fp\_loss: 1.18523/67.00\%, bp\_loss: 1.01846/74.00\%, hp\_loss: 2.44205/35.00\%, j\_loss: 0.75896/77.00\%, 
		fr\_loss: 0.15998/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.54971
	Part 4 - fp\_loss: 1.24873/64.00\%, bp\_loss: 1.07346/72.00\%, hp\_loss: 2.69468/29.00\%, j\_loss: 0.77105/79.00\%, 
		fr\_loss: 0.14829/85.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.67415
	Training time elapsed: 75.86 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.40333/61.00\%, bp\_loss: 1.51204/60.00\%, hp\_loss: 3.07838/21.00\%, j\_loss: 1.15744/69.00\%, 
		fr\_loss: 0.16222/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.39845
	Part 2 - fp\_loss: 1.74517/55.00\%, bp\_loss: 1.36279/65.00\%, hp\_loss: 2.77154/29.00\%, j\_loss: 1.22176/67.00\%, 
		fr\_loss: 0.26034/74.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.59498
	Part 3 - fp\_loss: 1.16356/67.00\%, bp\_loss: 0.99663/75.00\%, hp\_loss: 2.40407/36.00\%, j\_loss: 0.76258/77.00\%, 
		fr\_loss: 0.13500/86.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.49958
	Part 4 - fp\_loss: 1.23963/64.00\%, bp\_loss: 0.98432/73.00\%, hp\_loss: 2.56910/32.00\%, j\_loss: 0.73138/78.00\%, 
		fr\_loss: 0.14488/85.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 2.56210
	Training time elapsed: 113.27 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.32098/61.00\%, bp\_loss: 1.69754/56.00\%, hp\_loss: 3.05291/23.00\%, j\_loss: 1.25156/67.00\%, 
		fr\_loss: 0.18345/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.52063
	Part 2 - fp\_loss: 1.75572/53.00\%, bp\_loss: 1.33318/66.00\%, hp\_loss: 2.73988/31.00\%, j\_loss: 1.25864/67.00\%, 
		fr\_loss: 0.26464/74.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.62306
	Part 3 - fp\_loss: 1.14619/68.00\%, bp\_loss: 0.95134/76.00\%, hp\_loss: 2.44323/36.00\%, j\_loss: 0.71686/80.00\%, 
		fr\_loss: 0.16636/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.47469
	Part 4 - fp\_loss: 1.20286/67.00\%, bp\_loss: 1.23383/68.00\%, hp\_loss: 2.59620/33.00\%, j\_loss: 0.90087/77.00\%, 
		fr\_loss: 0.14722/85.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.79854
	Training time elapsed: 150.67 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.37634/61.00\%, bp\_loss: 1.68404/57.00\%, hp\_loss: 3.09687/23.00\%, j\_loss: 1.22148/67.00\%, 
		fr\_loss: 0.17232/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.51624
	Part 2 - fp\_loss: 1.83075/53.00\%, bp\_loss: 1.45285/63.00\%, hp\_loss: 2.76218/29.00\%, j\_loss: 1.34376/64.00\%, 
		fr\_loss: 0.26469/74.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.78833
	Part 3 - fp\_loss: 1.15292/68.00\%, bp\_loss: 1.13305/70.00\%, hp\_loss: 2.36474/37.00\%, j\_loss: 0.76337/77.00\%, 
		fr\_loss: 0.13847/86.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.52764
	Part 4 - fp\_loss: 1.29440/64.00\%, bp\_loss: 0.99856/74.00\%, hp\_loss: 2.54037/34.00\%, j\_loss: 0.79906/78.00\%, 
		fr\_loss: 0.15619/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.66413
	Training time elapsed: 188.09 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.41621/58.00\%, bp\_loss: 1.64198/55.00\%, hp\_loss: 3.06475/23.00\%, j\_loss: 1.28445/68.00\%, 
		fr\_loss: 0.15694/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.56152
	Part 2 - fp\_loss: 1.73694/55.00\%, bp\_loss: 1.40272/65.00\%, hp\_loss: 2.79216/30.00\%, j\_loss: 1.23033/68.00\%, 
		fr\_loss: 0.22820/77.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.58546
	Part 3 - fp\_loss: 1.09511/69.00\%, bp\_loss: 1.09101/73.00\%, hp\_loss: 2.38536/38.00\%, j\_loss: 0.70524/80.00\%, 
		fr\_loss: 0.14927/85.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.44497
	Part 4 - fp\_loss: 1.30272/63.00\%, bp\_loss: 1.11827/70.00\%, hp\_loss: 2.63279/31.00\%, j\_loss: 0.80763/77.00\%, 
		fr\_loss: 0.15844/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.74275
	Training time elapsed: 225.46 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.37225/59.00\%, bp\_loss: 1.81902/54.00\%, hp\_loss: 3.09714/21.00\%, j\_loss: 1.37274/63.00\%, 
		fr\_loss: 0.17589/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.70960
	Part 2 - fp\_loss: 1.80452/52.00\%, bp\_loss: 1.39707/65.00\%, hp\_loss: 2.74314/29.00\%, j\_loss: 1.26074/67.00\%, 
		fr\_loss: 0.24101/76.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.64607
	Part 3 - fp\_loss: 1.26559/66.00\%, bp\_loss: 0.90313/77.00\%, hp\_loss: 2.43370/34.00\%, j\_loss: 0.66720/81.00\%, 
		fr\_loss: 0.15571/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.45675
	Part 4 - fp\_loss: 1.34170/63.00\%, bp\_loss: 1.03135/72.00\%, hp\_loss: 2.54913/33.00\%, j\_loss: 0.79812/79.00\%, 
		fr\_loss: 0.17901/82.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 2.72212
	Training time elapsed: 262.87 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.39281/60.00\%, bp\_loss: 1.87577/56.00\%, hp\_loss: 3.15277/21.00\%, j\_loss: 1.29330/67.00\%, 
		fr\_loss: 0.17855/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.67682
	Part 2 - fp\_loss: 1.73254/55.00\%, bp\_loss: 1.35742/66.00\%, hp\_loss: 2.75856/30.00\%, j\_loss: 1.21410/68.00\%, 
		fr\_loss: 0.25796/74.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.57313
	Part 3 - fp\_loss: 1.19737/68.00\%, bp\_loss: 1.35516/68.00\%, hp\_loss: 2.55143/34.00\%, j\_loss: 0.87874/74.00\%, 
		fr\_loss: 0.15454/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.80394
	Part 4 - fp\_loss: 1.23941/67.00\%, bp\_loss: 1.07718/72.00\%, hp\_loss: 2.63831/31.00\%, j\_loss: 0.74553/81.00\%, 
		fr\_loss: 0.14266/86.00\%, p\_loss: 0.00003/100.00\%, 
		total weighted loss: 2.62255
	Training time elapsed: 300.27 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.69742/52.00\%, bp\_loss: 1.86295/51.00\%, hp\_loss: 3.09693/24.00\%, j\_loss: 1.71572/54.00\%, 
		fr\_loss: 0.24262/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.61564
	Part 2 - fp\_loss: 1.57991/58.00\%, bp\_loss: 1.71831/56.00\%, hp\_loss: 2.57165/42.00\%, j\_loss: 1.50314/60.00\%, 
		fr\_loss: 0.29016/71.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 7.66317
	Part 3 - fp\_loss: 1.13928/70.00\%, bp\_loss: 1.40975/63.00\%, hp\_loss: 1.99235/50.00\%, j\_loss: 1.07898/68.00\%, 
		fr\_loss: 0.19430/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.81467
	Part 4 - fp\_loss: 1.25740/67.00\%, bp\_loss: 2.06877/55.00\%, hp\_loss: 2.19839/44.00\%, j\_loss: 1.38245/60.00\%, 
		fr\_loss: 0.22632/77.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 7.13334
	`Validation time elapsed: 0.74 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.69339/50.00\%, bp\_loss: 2.18469/48.00\%, hp\_loss: 3.07186/26.00\%, j\_loss: 1.77904/54.00\%, 
		fr\_loss: 0.23520/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.96418
	Part 2 - fp\_loss: 1.55922/57.00\%, bp\_loss: 1.49492/62.00\%, hp\_loss: 2.52250/43.00\%, j\_loss: 1.33667/67.00\%, 
		fr\_loss: 0.26600/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 7.17931
	Part 3 - fp\_loss: 1.06743/71.00\%, bp\_loss: 1.53080/63.00\%, hp\_loss: 1.95297/50.00\%, j\_loss: 0.99106/69.00\%, 
		fr\_loss: 0.18724/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.72949
	Part 4 - fp\_loss: 1.18475/68.00\%, bp\_loss: 1.33386/70.00\%, hp\_loss: 2.20914/44.00\%, j\_loss: 0.73428/83.00\%, 
		fr\_loss: 0.20248/79.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 5.66453
	`Validation time elapsed: 9.52 seconds
`
-----------
Completed epoch 8.

EPOCH 9
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.32688/61.00\%, bp\_loss: 1.65033/59.00\%, hp\_loss: 3.06823/21.00\%, j\_loss: 1.27337/66.00\%, 
		fr\_loss: 0.15067/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.50305
	Part 2 - fp\_loss: 1.69856/56.00\%, bp\_loss: 1.57980/59.00\%, hp\_loss: 2.78220/29.00\%, j\_loss: 1.42053/62.00\%, 
		fr\_loss: 0.24990/75.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.82832
	Part 3 - fp\_loss: 1.02231/72.00\%, bp\_loss: 1.13529/72.00\%, hp\_loss: 2.44066/36.00\%, j\_loss: 0.81237/77.00\%, 
		fr\_loss: 0.14916/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.54547
	Part 4 - fp\_loss: 1.24934/65.00\%, bp\_loss: 1.56910/65.00\%, hp\_loss: 2.66388/32.00\%, j\_loss: 1.03537/70.00\%, 
		fr\_loss: 0.15588/84.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.08581
	Training time elapsed: 1.07 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.31746/59.00\%, bp\_loss: 1.73086/56.00\%, hp\_loss: 3.06047/24.00\%, j\_loss: 1.36543/66.00\%, 
		fr\_loss: 0.17863/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.64018
	Part 2 - fp\_loss: 1.71312/55.00\%, bp\_loss: 1.26032/68.00\%, hp\_loss: 2.71275/31.00\%, j\_loss: 1.21194/68.00\%, 
		fr\_loss: 0.25940/74.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.51982
	Part 3 - fp\_loss: 1.13394/68.00\%, bp\_loss: 1.09912/71.00\%, hp\_loss: 2.39963/38.00\%, j\_loss: 0.81851/75.00\%, 
		fr\_loss: 0.13868/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.57379
	Part 4 - fp\_loss: 1.40847/62.00\%, bp\_loss: 1.11512/71.00\%, hp\_loss: 2.64392/32.00\%, j\_loss: 0.90846/77.00\%, 
		fr\_loss: 0.16151/84.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.90192
	Training time elapsed: 38.31 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.40274/59.00\%, bp\_loss: 1.66751/56.00\%, hp\_loss: 3.07828/21.00\%, j\_loss: 1.26174/68.00\%, 
		fr\_loss: 0.16091/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.54776
	Part 2 - fp\_loss: 1.80415/52.00\%, bp\_loss: 1.38459/66.00\%, hp\_loss: 2.72444/30.00\%, j\_loss: 1.27279/66.00\%, 
		fr\_loss: 0.23494/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.64251
	Part 3 - fp\_loss: 1.11905/70.00\%, bp\_loss: 1.20931/72.00\%, hp\_loss: 2.36964/37.00\%, j\_loss: 0.76129/80.00\%, 
		fr\_loss: 0.13697/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.53148
	Part 4 - fp\_loss: 2.19330/50.00\%, bp\_loss: 1.10665/72.00\%, hp\_loss: 2.62059/32.00\%, j\_loss: 1.09766/73.00\%, 
		fr\_loss: 0.35565/64.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.66812
	Training time elapsed: 75.52 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.41930/59.00\%, bp\_loss: 1.54189/60.00\%, hp\_loss: 3.10863/21.00\%, j\_loss: 1.20967/70.00\%, 
		fr\_loss: 0.17168/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.48615
	Part 2 - fp\_loss: 1.73351/54.00\%, bp\_loss: 1.38859/65.00\%, hp\_loss: 2.70037/31.00\%, j\_loss: 1.23909/69.00\%, 
		fr\_loss: 0.24402/75.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.57655
	Part 3 - fp\_loss: 1.13783/68.00\%, bp\_loss: 1.11500/73.00\%, hp\_loss: 2.42218/38.00\%, j\_loss: 0.73307/79.00\%, 
		fr\_loss: 0.13034/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.49347
	Part 4 - fp\_loss: 2.14920/51.00\%, bp\_loss: 0.94021/74.00\%, hp\_loss: 2.63724/32.00\%, j\_loss: 0.96496/76.00\%, 
		fr\_loss: 0.30871/69.00\%, p\_loss: 0.00002/100.00\%, 
		total weighted loss: 3.42151
	Training time elapsed: 112.75 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.29772/60.00\%, bp\_loss: 1.57289/57.00\%, hp\_loss: 3.06359/23.00\%, j\_loss: 1.27124/67.00\%, 
		fr\_loss: 0.16470/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.47574
	Part 2 - fp\_loss: 1.68945/56.00\%, bp\_loss: 1.27875/67.00\%, hp\_loss: 2.73833/31.00\%, j\_loss: 1.18194/70.00\%, 
		fr\_loss: 0.25115/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.48294
	Part 3 - fp\_loss: 1.12321/69.00\%, bp\_loss: 1.00669/75.00\%, hp\_loss: 2.30622/39.00\%, j\_loss: 0.73352/79.00\%, 
		fr\_loss: 0.16099/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.44999
	Part 4 - fp\_loss: 2.03649/55.00\%, bp\_loss: 1.02625/73.00\%, hp\_loss: 2.54821/34.00\%, j\_loss: 0.95351/77.00\%, 
		fr\_loss: 0.32015/68.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.36424
	Training time elapsed: 149.94 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.35311/61.00\%, bp\_loss: 1.55971/58.00\%, hp\_loss: 3.04231/21.00\%, j\_loss: 1.20249/71.00\%, 
		fr\_loss: 0.14691/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.40656
	Part 2 - fp\_loss: 1.60565/59.00\%, bp\_loss: 1.20074/69.00\%, hp\_loss: 2.79802/29.00\%, j\_loss: 1.05469/71.00\%, 
		fr\_loss: 0.24122/75.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.29837
	Part 3 - fp\_loss: 1.21035/68.00\%, bp\_loss: 1.07493/73.00\%, hp\_loss: 2.46765/35.00\%, j\_loss: 0.78116/82.00\%, 
		fr\_loss: 0.14777/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.59688
	Part 4 - fp\_loss: 1.92789/56.00\%, bp\_loss: 0.99344/74.00\%, hp\_loss: 2.59833/33.00\%, j\_loss: 0.89213/76.00\%, 
		fr\_loss: 0.28730/71.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.22091
	Training time elapsed: 187.10 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.36451/58.00\%, bp\_loss: 1.53347/58.00\%, hp\_loss: 3.04894/21.00\%, j\_loss: 1.23947/68.00\%, 
		fr\_loss: 0.15589/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.45234
	Part 2 - fp\_loss: 1.73641/55.00\%, bp\_loss: 1.20209/70.00\%, hp\_loss: 2.70260/32.00\%, j\_loss: 1.13044/71.00\%, 
		fr\_loss: 0.25270/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.42276
	Part 3 - fp\_loss: 1.03496/71.00\%, bp\_loss: 1.02403/76.00\%, hp\_loss: 2.36390/40.00\%, j\_loss: 0.64063/81.00\%, 
		fr\_loss: 0.10982/89.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.28431
	Part 4 - fp\_loss: 2.33412/47.00\%, bp\_loss: 1.13802/72.00\%, hp\_loss: 2.59455/34.00\%, j\_loss: 1.08877/75.00\%, 
		fr\_loss: 0.39482/62.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.77041
	Training time elapsed: 224.34 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.40543/58.00\%, bp\_loss: 1.41135/62.00\%, hp\_loss: 3.11750/22.00\%, j\_loss: 1.09534/75.00\%, 
		fr\_loss: 0.17599/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.33269
	Part 2 - fp\_loss: 1.64831/57.00\%, bp\_loss: 1.29856/68.00\%, hp\_loss: 2.73953/31.00\%, j\_loss: 1.13638/69.00\%, 
		fr\_loss: 0.24563/75.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.41759
	Part 3 - fp\_loss: 1.18382/68.00\%, bp\_loss: 1.08498/75.00\%, hp\_loss: 2.50516/34.00\%, j\_loss: 0.68308/80.00\%, 
		fr\_loss: 0.15563/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.50767
	Part 4 - fp\_loss: 1.96908/56.00\%, bp\_loss: 1.21877/69.00\%, hp\_loss: 2.60675/32.00\%, j\_loss: 1.06907/73.00\%, 
		fr\_loss: 0.32103/68.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.52230
	Training time elapsed: 261.57 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.43660/58.00\%, bp\_loss: 1.53805/59.00\%, hp\_loss: 3.11154/20.00\%, j\_loss: 1.20051/70.00\%, 
		fr\_loss: 0.17398/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.48767
	Part 2 - fp\_loss: 1.71421/54.00\%, bp\_loss: 1.62990/59.00\%, hp\_loss: 2.76896/30.00\%, j\_loss: 1.39682/62.00\%, 
		fr\_loss: 0.24745/75.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.82104
	Part 3 - fp\_loss: 1.12159/70.00\%, bp\_loss: 0.95776/76.00\%, hp\_loss: 2.40346/39.00\%, j\_loss: 0.72832/80.00\%, 
		fr\_loss: 0.13877/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.43625
	Part 4 - fp\_loss: 1.85792/59.00\%, bp\_loss: 1.02535/72.00\%, hp\_loss: 2.58126/34.00\%, j\_loss: 0.86790/79.00\%, 
		fr\_loss: 0.28936/71.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.16820
	Training time elapsed: 298.74 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.77068/52.00\%, bp\_loss: 1.70595/54.00\%, hp\_loss: 3.08336/27.00\%, j\_loss: 1.60020/61.00\%, 
		fr\_loss: 0.26257/73.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.42275
	Part 2 - fp\_loss: 1.51073/60.00\%, bp\_loss: 2.23211/42.00\%, hp\_loss: 2.61263/39.00\%, j\_loss: 1.52724/59.00\%, 
		fr\_loss: 0.25368/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.13640
	Part 3 - fp\_loss: 1.05907/71.00\%, bp\_loss: 1.12965/71.00\%, hp\_loss: 2.01392/50.00\%, j\_loss: 0.88215/75.00\%, 
		fr\_loss: 0.18296/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.26776
	Part 4 - fp\_loss: 1.72438/59.00\%, bp\_loss: 1.60022/60.00\%, hp\_loss: 2.21591/42.00\%, j\_loss: 1.37045/63.00\%, 
		fr\_loss: 0.31809/68.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 7.22906
	`Validation time elapsed: 0.75 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.74024/51.00\%, bp\_loss: 1.90168/53.00\%, hp\_loss: 3.07127/26.00\%, j\_loss: 1.69712/58.00\%, 
		fr\_loss: 0.24667/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.65699
	Part 2 - fp\_loss: 1.59703/58.00\%, bp\_loss: 2.25765/48.00\%, hp\_loss: 2.65515/39.00\%, j\_loss: 1.58059/59.00\%, 
		fr\_loss: 0.27127/72.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.36168
	Part 3 - fp\_loss: 1.12210/69.00\%, bp\_loss: 1.40119/68.00\%, hp\_loss: 1.99356/50.00\%, j\_loss: 0.85090/76.00\%, 
		fr\_loss: 0.20582/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.57359
	Part 4 - fp\_loss: 1.75100/58.00\%, bp\_loss: 1.38571/64.00\%, hp\_loss: 2.24700/42.00\%, j\_loss: 1.08746/71.00\%, 
		fr\_loss: 0.34970/65.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 6.82089
	`Validation time elapsed: 9.54 seconds
`
-----------
Completed epoch 9.

EPOCH 10
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.37836/58.00\%, bp\_loss: 1.39153/61.00\%, hp\_loss: 3.03955/23.00\%, j\_loss: 1.15005/73.00\%, 
		fr\_loss: 0.15878/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.32733
	Part 2 - fp\_loss: 1.57327/59.00\%, bp\_loss: 1.85755/52.00\%, hp\_loss: 2.64827/34.00\%, j\_loss: 1.36632/63.00\%, 
		fr\_loss: 0.24852/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.75322
	Part 3 - fp\_loss: 1.04448/72.00\%, bp\_loss: 0.96882/73.00\%, hp\_loss: 2.40473/38.00\%, j\_loss: 0.76801/79.00\%, 
		fr\_loss: 0.13049/87.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.43280
	Part 4 - fp\_loss: 1.92677/56.00\%, bp\_loss: 1.33697/65.00\%, hp\_loss: 2.56626/34.00\%, j\_loss: 1.20448/71.00\%, 
		fr\_loss: 0.29102/71.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.62986
	Training time elapsed: 1.04 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.31914/62.00\%, bp\_loss: 1.45245/60.00\%, hp\_loss: 3.02954/23.00\%, j\_loss: 1.12537/74.00\%, 
		fr\_loss: 0.15929/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.28883
	Part 2 - fp\_loss: 1.60085/57.00\%, bp\_loss: 1.17908/71.00\%, hp\_loss: 2.67537/33.00\%, j\_loss: 1.10122/71.00\%, 
		fr\_loss: 0.27800/72.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.33599
	Part 3 - fp\_loss: 1.10826/70.00\%, bp\_loss: 0.96614/75.00\%, hp\_loss: 2.46613/36.00\%, j\_loss: 0.69436/78.00\%, 
		fr\_loss: 0.15786/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.43604
	Part 4 - fp\_loss: 1.88804/59.00\%, bp\_loss: 1.28007/68.00\%, hp\_loss: 2.62220/33.00\%, j\_loss: 0.99520/76.00\%, 
		fr\_loss: 0.28282/71.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.39273
	Training time elapsed: 37.51 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.32073/60.00\%, bp\_loss: 1.46565/59.00\%, hp\_loss: 3.12169/20.00\%, j\_loss: 1.20658/70.00\%, 
		fr\_loss: 0.14950/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.39264
	Part 2 - fp\_loss: 1.67189/55.00\%, bp\_loss: 1.33743/67.00\%, hp\_loss: 2.75715/28.00\%, j\_loss: 1.21069/70.00\%, 
		fr\_loss: 0.25008/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.52509
	Part 3 - fp\_loss: 1.24956/66.00\%, bp\_loss: 0.99220/75.00\%, hp\_loss: 2.45821/36.00\%, j\_loss: 0.73050/80.00\%, 
		fr\_loss: 0.16592/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.55632
	Part 4 - fp\_loss: 1.81398/58.00\%, bp\_loss: 0.99970/74.00\%, hp\_loss: 2.60296/32.00\%, j\_loss: 0.88290/77.00\%, 
		fr\_loss: 0.24854/75.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.11923
	Training time elapsed: 73.96 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.36241/61.00\%, bp\_loss: 1.36826/60.00\%, hp\_loss: 3.08935/21.00\%, j\_loss: 1.14091/72.00\%, 
		fr\_loss: 0.17184/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.33124
	Part 2 - fp\_loss: 1.65638/56.00\%, bp\_loss: 1.27246/67.00\%, hp\_loss: 2.79032/28.00\%, j\_loss: 1.16608/71.00\%, 
		fr\_loss: 0.21902/78.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.43212
	Part 3 - fp\_loss: 1.17360/67.00\%, bp\_loss: 1.21800/74.00\%, hp\_loss: 2.55220/33.00\%, j\_loss: 0.72942/78.00\%, 
		fr\_loss: 0.13862/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.58591
	Part 4 - fp\_loss: 1.82668/57.00\%, bp\_loss: 1.06134/74.00\%, hp\_loss: 2.67218/32.00\%, j\_loss: 0.79310/79.00\%, 
		fr\_loss: 0.23920/76.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.06570
	Training time elapsed: 110.39 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.37332/61.00\%, bp\_loss: 2.27001/46.00\%, hp\_loss: 3.06510/22.00\%, j\_loss: 1.48453/63.00\%, 
		fr\_loss: 0.14898/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.92071
	Part 2 - fp\_loss: 1.59916/57.00\%, bp\_loss: 1.27750/68.00\%, hp\_loss: 2.71373/29.00\%, j\_loss: 1.10334/72.00\%, 
		fr\_loss: 0.22110/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.32139
	Part 3 - fp\_loss: 1.18236/68.00\%, bp\_loss: 0.98614/76.00\%, hp\_loss: 2.40991/37.00\%, j\_loss: 0.66362/80.00\%, 
		fr\_loss: 0.15691/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.43052
	Part 4 - fp\_loss: 1.75771/58.00\%, bp\_loss: 1.03259/71.00\%, hp\_loss: 2.65061/31.00\%, j\_loss: 0.84960/77.00\%, 
		fr\_loss: 0.22620/78.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.05961
	Training time elapsed: 146.85 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.46408/59.00\%, bp\_loss: 2.08521/50.00\%, hp\_loss: 3.07947/21.00\%, j\_loss: 1.39715/64.00\%, 
		fr\_loss: 0.16914/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.84774
	Part 2 - fp\_loss: 1.68916/56.00\%, bp\_loss: 1.28349/66.00\%, hp\_loss: 2.80163/29.00\%, j\_loss: 1.16365/69.00\%, 
		fr\_loss: 0.23679/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.47056
	Part 3 - fp\_loss: 1.08848/71.00\%, bp\_loss: 0.98001/75.00\%, hp\_loss: 2.43740/37.00\%, j\_loss: 0.66960/81.00\%, 
		fr\_loss: 0.13676/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.37582
	Part 4 - fp\_loss: 1.64268/61.00\%, bp\_loss: 1.08330/72.00\%, hp\_loss: 2.66684/31.00\%, j\_loss: 0.86447/77.00\%, 
		fr\_loss: 0.23009/76.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.04094
	Training time elapsed: 183.29 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.43505/57.00\%, bp\_loss: 1.79107/57.00\%, hp\_loss: 3.01068/22.00\%, j\_loss: 1.30377/65.00\%, 
		fr\_loss: 0.18519/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.64701
	Part 2 - fp\_loss: 1.59287/57.00\%, bp\_loss: 1.35810/66.00\%, hp\_loss: 2.79604/29.00\%, j\_loss: 1.16052/69.00\%, 
		fr\_loss: 0.22615/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.42935
	Part 3 - fp\_loss: 1.15066/69.00\%, bp\_loss: 1.01780/75.00\%, hp\_loss: 2.36824/38.00\%, j\_loss: 0.69558/80.00\%, 
		fr\_loss: 0.14177/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.42849
	Part 4 - fp\_loss: 1.68625/59.00\%, bp\_loss: 1.12198/71.00\%, hp\_loss: 2.58439/34.00\%, j\_loss: 0.85209/79.00\%, 
		fr\_loss: 0.22229/77.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.02942
	Training time elapsed: 219.74 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.51122/56.00\%, bp\_loss: 1.92665/55.00\%, hp\_loss: 3.05121/22.00\%, j\_loss: 1.33409/65.00\%, 
		fr\_loss: 0.18897/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.77202
	Part 2 - fp\_loss: 1.57133/58.00\%, bp\_loss: 1.25601/68.00\%, hp\_loss: 2.76360/29.00\%, j\_loss: 1.09247/72.00\%, 
		fr\_loss: 0.24915/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.33316
	Part 3 - fp\_loss: 1.05588/72.00\%, bp\_loss: 1.03467/74.00\%, hp\_loss: 2.38453/38.00\%, j\_loss: 0.67810/81.00\%, 
		fr\_loss: 0.14472/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.37652
	Part 4 - fp\_loss: 1.66820/60.00\%, bp\_loss: 1.16846/71.00\%, hp\_loss: 2.58504/33.00\%, j\_loss: 0.83282/77.00\%, 
		fr\_loss: 0.25353/75.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.04651
	Training time elapsed: 256.17 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.45340/58.00\%, bp\_loss: 1.91126/54.00\%, hp\_loss: 3.08883/20.00\%, j\_loss: 1.36023/65.00\%, 
		fr\_loss: 0.17195/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.75890
	Part 2 - fp\_loss: 1.66099/56.00\%, bp\_loss: 1.53652/62.00\%, hp\_loss: 2.78155/29.00\%, j\_loss: 1.33183/66.00\%, 
		fr\_loss: 0.22378/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.68152
	Part 3 - fp\_loss: 1.16526/67.00\%, bp\_loss: 1.09699/74.00\%, hp\_loss: 2.46921/34.00\%, j\_loss: 0.70839/80.00\%, 
		fr\_loss: 0.15736/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.51824
	Part 4 - fp\_loss: 1.68824/58.00\%, bp\_loss: 1.12488/70.00\%, hp\_loss: 2.59769/32.00\%, j\_loss: 0.86555/80.00\%, 
		fr\_loss: 0.22117/77.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.04761
	Training time elapsed: 292.59 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.66463/54.00\%, bp\_loss: 1.96400/47.00\%, hp\_loss: 3.09201/26.00\%, j\_loss: 1.72341/58.00\%, 
		fr\_loss: 0.24602/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.69007
	Part 2 - fp\_loss: 1.44297/60.00\%, bp\_loss: 1.96556/47.00\%, hp\_loss: 2.55293/41.00\%, j\_loss: 1.56001/55.00\%, 
		fr\_loss: 0.26507/73.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 7.78655
	Part 3 - fp\_loss: 1.07099/71.00\%, bp\_loss: 1.48532/64.00\%, hp\_loss: 2.03236/50.00\%, j\_loss: 1.05040/72.00\%, 
		fr\_loss: 0.18358/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.82265
	Part 4 - fp\_loss: 1.67108/59.00\%, bp\_loss: 1.92283/56.00\%, hp\_loss: 2.20159/45.00\%, j\_loss: 1.52500/61.00\%, 
		fr\_loss: 0.31910/68.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 7.63960
	`Validation time elapsed: 0.74 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.75707/51.00\%, bp\_loss: 2.05503/49.00\%, hp\_loss: 3.07199/26.00\%, j\_loss: 1.73646/55.00\%, 
		fr\_loss: 0.24878/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.86933
	Part 2 - fp\_loss: 1.44501/59.00\%, bp\_loss: 1.76456/57.00\%, hp\_loss: 2.54419/42.00\%, j\_loss: 1.43705/65.00\%, 
		fr\_loss: 0.29613/70.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 7.48693
	Part 3 - fp\_loss: 1.06395/71.00\%, bp\_loss: 1.28414/70.00\%, hp\_loss: 2.01689/49.00\%, j\_loss: 0.77069/77.00\%, 
		fr\_loss: 0.20019/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.33586
	Part 4 - fp\_loss: 1.55389/58.00\%, bp\_loss: 1.42796/64.00\%, hp\_loss: 2.19728/44.00\%, j\_loss: 1.20100/68.00\%, 
		fr\_loss: 0.27121/73.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 6.65134
	`Validation time elapsed: 9.52 seconds
`
-----------
Completed epoch 10.

EPOCH 11
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.46852/58.00\%, bp\_loss: 1.72131/54.00\%, hp\_loss: 3.09270/23.00\%, j\_loss: 1.45284/61.00\%, 
		fr\_loss: 0.17234/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.80364
	Part 2 - fp\_loss: 1.62954/57.00\%, bp\_loss: 1.88702/48.00\%, hp\_loss: 2.75191/30.00\%, j\_loss: 1.56734/56.00\%, 
		fr\_loss: 0.24508/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.01887
	Part 3 - fp\_loss: 1.12260/69.00\%, bp\_loss: 1.30667/67.00\%, hp\_loss: 2.49248/34.00\%, j\_loss: 0.90402/74.00\%, 
		fr\_loss: 0.13432/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.73938
	Part 4 - fp\_loss: 1.86280/57.00\%, bp\_loss: 1.40177/68.00\%, hp\_loss: 2.64344/32.00\%, j\_loss: 1.10845/71.00\%, 
		fr\_loss: 0.24824/75.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.50166
	Training time elapsed: 1.07 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.41929/58.00\%, bp\_loss: 1.56143/59.00\%, hp\_loss: 3.09422/21.00\%, j\_loss: 1.23342/68.00\%, 
		fr\_loss: 0.17065/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.51041
	Part 2 - fp\_loss: 1.66827/56.00\%, bp\_loss: 1.37923/67.00\%, hp\_loss: 2.77607/29.00\%, j\_loss: 1.15783/70.00\%, 
		fr\_loss: 0.24611/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.48466
	Part 3 - fp\_loss: 1.15023/67.00\%, bp\_loss: 0.95191/76.00\%, hp\_loss: 2.45616/37.00\%, j\_loss: 0.67826/79.00\%, 
		fr\_loss: 0.13809/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.41388
	Part 4 - fp\_loss: 1.73039/58.00\%, bp\_loss: 1.04489/71.00\%, hp\_loss: 2.61548/33.00\%, j\_loss: 0.86979/78.00\%, 
		fr\_loss: 0.20650/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.03960
	Training time elapsed: 38.50 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.45382/58.00\%, bp\_loss: 1.73560/55.00\%, hp\_loss: 3.15776/18.00\%, j\_loss: 1.34345/66.00\%, 
		fr\_loss: 0.16835/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.70672
	Part 2 - fp\_loss: 1.60616/56.00\%, bp\_loss: 1.31857/67.00\%, hp\_loss: 2.70478/32.00\%, j\_loss: 1.14405/70.00\%, 
		fr\_loss: 0.22380/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.37793
	Part 3 - fp\_loss: 1.13729/67.00\%, bp\_loss: 1.11841/72.00\%, hp\_loss: 2.41883/37.00\%, j\_loss: 0.80356/78.00\%, 
		fr\_loss: 0.14698/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.58036
	Part 4 - fp\_loss: 1.67896/57.00\%, bp\_loss: 1.77307/54.00\%, hp\_loss: 2.67034/29.00\%, j\_loss: 1.38830/66.00\%, 
		fr\_loss: 0.20952/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.77032
	Training time elapsed: 75.86 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.43006/58.00\%, bp\_loss: 1.78500/56.00\%, hp\_loss: 3.05477/22.00\%, j\_loss: 1.32626/66.00\%, 
		fr\_loss: 0.15113/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.64436
	Part 2 - fp\_loss: 1.54337/58.00\%, bp\_loss: 1.23059/67.00\%, hp\_loss: 2.70422/31.00\%, j\_loss: 1.19179/70.00\%, 
		fr\_loss: 0.24615/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.39007
	Part 3 - fp\_loss: 1.12378/69.00\%, bp\_loss: 1.15126/71.00\%, hp\_loss: 2.47789/36.00\%, j\_loss: 0.81639/80.00\%, 
		fr\_loss: 0.14640/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.61343
	Part 4 - fp\_loss: 1.65672/60.00\%, bp\_loss: 1.24172/69.00\%, hp\_loss: 2.65730/32.00\%, j\_loss: 0.97126/76.00\%, 
		fr\_loss: 0.24324/75.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.21257
	Training time elapsed: 113.27 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.36316/59.00\%, bp\_loss: 1.60936/58.00\%, hp\_loss: 2.97162/24.00\%, j\_loss: 1.19678/70.00\%, 
		fr\_loss: 0.16379/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.41644
	Part 2 - fp\_loss: 1.56686/56.00\%, bp\_loss: 1.34933/64.00\%, hp\_loss: 2.74774/30.00\%, j\_loss: 1.24719/68.00\%, 
		fr\_loss: 0.22692/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.48667
	Part 3 - fp\_loss: 1.17253/67.00\%, bp\_loss: 1.15029/71.00\%, hp\_loss: 2.43746/36.00\%, j\_loss: 0.81217/75.00\%, 
		fr\_loss: 0.15378/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.62854
	Part 4 - fp\_loss: 1.66268/58.00\%, bp\_loss: 1.15026/70.00\%, hp\_loss: 2.62146/31.00\%, j\_loss: 0.91110/76.00\%, 
		fr\_loss: 0.20399/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.07795
	Training time elapsed: 150.68 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.40542/56.00\%, bp\_loss: 1.53758/60.00\%, hp\_loss: 3.15473/20.00\%, j\_loss: 1.24921/69.00\%, 
		fr\_loss: 0.17057/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.53018
	Part 2 - fp\_loss: 1.49720/59.00\%, bp\_loss: 1.28010/68.00\%, hp\_loss: 2.65324/33.00\%, j\_loss: 1.09313/71.00\%, 
		fr\_loss: 0.21824/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.23997
	Part 3 - fp\_loss: 1.06477/71.00\%, bp\_loss: 1.26394/70.00\%, hp\_loss: 2.40075/38.00\%, j\_loss: 0.75794/79.00\%, 
		fr\_loss: 0.14213/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.53186
	Part 4 - fp\_loss: 1.56661/62.00\%, bp\_loss: 1.23428/71.00\%, hp\_loss: 2.63390/31.00\%, j\_loss: 0.84128/80.00\%, 
		fr\_loss: 0.20099/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.98603
	Training time elapsed: 188.07 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.34624/63.00\%, bp\_loss: 1.62558/57.00\%, hp\_loss: 3.07952/21.00\%, j\_loss: 1.12907/72.00\%, 
		fr\_loss: 0.17112/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.38484
	Part 2 - fp\_loss: 1.46719/60.00\%, bp\_loss: 1.29583/67.00\%, hp\_loss: 2.70185/31.00\%, j\_loss: 1.08512/73.00\%, 
		fr\_loss: 0.23646/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.25448
	Part 3 - fp\_loss: 1.06147/70.00\%, bp\_loss: 0.87658/78.00\%, hp\_loss: 2.32378/39.00\%, j\_loss: 0.60540/83.00\%, 
		fr\_loss: 0.14491/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.24115
	Part 4 - fp\_loss: 1.68128/58.00\%, bp\_loss: 1.10453/72.00\%, hp\_loss: 2.65372/30.00\%, j\_loss: 0.86343/77.00\%, 
		fr\_loss: 0.24241/76.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.07395
	Training time elapsed: 225.47 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.45136/59.00\%, bp\_loss: 1.45355/61.00\%, hp\_loss: 3.07791/21.00\%, j\_loss: 1.11007/73.00\%, 
		fr\_loss: 0.16330/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.35848
	Part 2 - fp\_loss: 1.48627/61.00\%, bp\_loss: 1.26106/69.00\%, hp\_loss: 2.75487/29.00\%, j\_loss: 1.06947/74.00\%, 
		fr\_loss: 0.21938/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.23676
	Part 3 - fp\_loss: 1.20126/67.00\%, bp\_loss: 1.05578/74.00\%, hp\_loss: 2.45428/36.00\%, j\_loss: 0.74980/80.00\%, 
		fr\_loss: 0.15213/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.55558
	Part 4 - fp\_loss: 1.61870/60.00\%, bp\_loss: 1.01474/73.00\%, hp\_loss: 2.60262/32.00\%, j\_loss: 0.82154/79.00\%, 
		fr\_loss: 0.21807/78.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.93417
	Training time elapsed: 262.87 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.44213/56.00\%, bp\_loss: 1.51325/60.00\%, hp\_loss: 3.02599/22.00\%, j\_loss: 1.24203/70.00\%, 
		fr\_loss: 0.18196/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.50683
	Part 2 - fp\_loss: 1.50988/58.00\%, bp\_loss: 1.25416/68.00\%, hp\_loss: 2.75202/29.00\%, j\_loss: 1.07621/71.00\%, 
		fr\_loss: 0.23307/76.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.26608
	Part 3 - fp\_loss: 1.19677/66.00\%, bp\_loss: 1.01636/75.00\%, hp\_loss: 2.45134/37.00\%, j\_loss: 0.78110/77.00\%, 
		fr\_loss: 0.16537/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.58516
	Part 4 - fp\_loss: 1.59967/60.00\%, bp\_loss: 1.06182/71.00\%, hp\_loss: 2.62567/31.00\%, j\_loss: 0.83696/77.00\%, 
		fr\_loss: 0.22249/77.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.96553
	Training time elapsed: 300.26 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.77881/50.00\%, bp\_loss: 1.93616/47.00\%, hp\_loss: 3.12478/23.00\%, j\_loss: 1.79721/55.00\%, 
		fr\_loss: 0.25261/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.88957
	Part 2 - fp\_loss: 1.34265/63.00\%, bp\_loss: 1.93658/48.00\%, hp\_loss: 2.52737/43.00\%, j\_loss: 1.56153/55.00\%, 
		fr\_loss: 0.24219/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 7.61032
	Part 3 - fp\_loss: 1.09326/70.00\%, bp\_loss: 1.57174/60.00\%, hp\_loss: 1.97013/50.00\%, j\_loss: 1.07330/67.00\%, 
		fr\_loss: 0.19495/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.90338
	Part 4 - fp\_loss: 1.63165/60.00\%, bp\_loss: 2.29004/52.00\%, hp\_loss: 2.24865/42.00\%, j\_loss: 1.61245/55.00\%, 
		fr\_loss: 0.27290/72.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.05570
	`Validation time elapsed: 0.75 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.72763/53.00\%, bp\_loss: 1.79488/53.00\%, hp\_loss: 3.05503/27.00\%, j\_loss: 1.62518/59.00\%, 
		fr\_loss: 0.22539/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.42811
	Part 2 - fp\_loss: 1.41204/59.00\%, bp\_loss: 1.39021/67.00\%, hp\_loss: 2.53816/42.00\%, j\_loss: 1.15052/71.00\%, 
		fr\_loss: 0.26037/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.75130
	Part 3 - fp\_loss: 1.20540/68.00\%, bp\_loss: 1.25804/71.00\%, hp\_loss: 1.94432/50.00\%, j\_loss: 0.86299/76.00\%, 
		fr\_loss: 0.20624/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.47699
	Part 4 - fp\_loss: 1.45604/62.00\%, bp\_loss: 1.54071/63.00\%, hp\_loss: 2.10999/47.00\%, j\_loss: 1.12691/68.00\%, 
		fr\_loss: 0.23300/76.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 6.46666
	`Validation time elapsed: 9.56 seconds
`
-----------
Completed epoch 11.

EPOCH 12
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.42642/57.00\%, bp\_loss: 1.62992/56.00\%, hp\_loss: 3.10881/20.00\%, j\_loss: 1.37472/63.00\%, 
		fr\_loss: 0.16462/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.67417
	Part 2 - fp\_loss: 1.47915/60.00\%, bp\_loss: 1.76213/53.00\%, hp\_loss: 2.69082/32.00\%, j\_loss: 1.48159/58.00\%, 
		fr\_loss: 0.22396/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.78101
	Part 3 - fp\_loss: 1.02769/71.00\%, bp\_loss: 1.38765/67.00\%, hp\_loss: 2.41759/38.00\%, j\_loss: 0.93936/73.00\%, 
		fr\_loss: 0.12795/87.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.72273
	Part 4 - fp\_loss: 1.76654/54.00\%, bp\_loss: 1.66833/63.00\%, hp\_loss: 2.63602/33.00\%, j\_loss: 1.33357/66.00\%, 
		fr\_loss: 0.25518/75.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.76332
	Training time elapsed: 1.08 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.49691/57.00\%, bp\_loss: 1.72227/57.00\%, hp\_loss: 3.10620/22.00\%, j\_loss: 1.35940/66.00\%, 
		fr\_loss: 0.17615/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.73254
	Part 2 - fp\_loss: 1.44647/60.00\%, bp\_loss: 1.17764/70.00\%, hp\_loss: 2.71318/29.00\%, j\_loss: 1.03470/74.00\%, 
		fr\_loss: 0.20892/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.13410
	Part 3 - fp\_loss: 1.09725/71.00\%, bp\_loss: 1.26217/71.00\%, hp\_loss: 2.41035/37.00\%, j\_loss: 0.73669/80.00\%, 
		fr\_loss: 0.11074/88.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.49781
	Part 4 - fp\_loss: 1.42616/62.00\%, bp\_loss: 1.10376/71.00\%, hp\_loss: 2.54425/34.00\%, j\_loss: 0.78719/79.00\%, 
		fr\_loss: 0.20722/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.80189
	Training time elapsed: 38.43 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.46818/56.00\%, bp\_loss: 1.47850/60.00\%, hp\_loss: 3.12313/20.00\%, j\_loss: 1.20351/69.00\%, 
		fr\_loss: 0.16062/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.47871
	Part 2 - fp\_loss: 1.54093/57.00\%, bp\_loss: 1.24763/68.00\%, hp\_loss: 2.70716/30.00\%, j\_loss: 1.11364/71.00\%, 
		fr\_loss: 0.21020/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.28075
	Part 3 - fp\_loss: 1.18494/66.00\%, bp\_loss: 1.32131/67.00\%, hp\_loss: 2.48254/34.00\%, j\_loss: 1.04158/75.00\%, 
		fr\_loss: 0.14023/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.91543
	Part 4 - fp\_loss: 1.52171/61.00\%, bp\_loss: 1.09051/73.00\%, hp\_loss: 2.64086/32.00\%, j\_loss: 0.81463/80.00\%, 
		fr\_loss: 0.18951/81.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.88440
	Training time elapsed: 75.80 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.34033/60.00\%, bp\_loss: 1.44483/61.00\%, hp\_loss: 3.06501/23.00\%, j\_loss: 1.16913/68.00\%, 
		fr\_loss: 0.15453/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.34677
	Part 2 - fp\_loss: 1.51452/59.00\%, bp\_loss: 1.16951/70.00\%, hp\_loss: 2.70262/30.00\%, j\_loss: 1.02363/72.00\%, 
		fr\_loss: 0.22525/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.16778
	Part 3 - fp\_loss: 1.15683/69.00\%, bp\_loss: 0.95613/77.00\%, hp\_loss: 2.35971/39.00\%, j\_loss: 0.70304/82.00\%, 
		fr\_loss: 0.15247/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.42868
	Part 4 - fp\_loss: 1.57827/61.00\%, bp\_loss: 1.13963/73.00\%, hp\_loss: 2.60161/33.00\%, j\_loss: 0.81872/77.00\%, 
		fr\_loss: 0.23615/76.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.96638
	Training time elapsed: 113.20 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.55893/56.00\%, bp\_loss: 1.65035/58.00\%, hp\_loss: 3.15765/20.00\%, j\_loss: 1.28563/67.00\%, 
		fr\_loss: 0.18766/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.69515
	Part 2 - fp\_loss: 1.49818/59.00\%, bp\_loss: 1.31772/68.00\%, hp\_loss: 2.78099/29.00\%, j\_loss: 1.04914/72.00\%, 
		fr\_loss: 0.23259/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.26044
	Part 3 - fp\_loss: 1.17452/68.00\%, bp\_loss: 0.95684/75.00\%, hp\_loss: 2.47186/35.00\%, j\_loss: 0.67980/83.00\%, 
		fr\_loss: 0.15090/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.44657
	Part 4 - fp\_loss: 1.56202/60.00\%, bp\_loss: 1.08822/72.00\%, hp\_loss: 2.57310/33.00\%, j\_loss: 0.83158/78.00\%, 
		fr\_loss: 0.21962/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.93060
	Training time elapsed: 150.58 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.42853/58.00\%, bp\_loss: 1.54216/60.00\%, hp\_loss: 3.14694/20.00\%, j\_loss: 1.15659/72.00\%, 
		fr\_loss: 0.15916/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.43675
	Part 2 - fp\_loss: 1.51178/59.00\%, bp\_loss: 1.26087/69.00\%, hp\_loss: 2.79062/29.00\%, j\_loss: 1.06503/74.00\%, 
		fr\_loss: 0.22353/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.25991
	Part 3 - fp\_loss: 1.16734/69.00\%, bp\_loss: 1.22661/70.00\%, hp\_loss: 2.49947/34.00\%, j\_loss: 0.83083/79.00\%, 
		fr\_loss: 0.14163/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.67395
	Part 4 - fp\_loss: 1.56138/60.00\%, bp\_loss: 1.03166/72.00\%, hp\_loss: 2.62791/33.00\%, j\_loss: 0.83813/78.00\%, 
		fr\_loss: 0.19353/80.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.91023
	Training time elapsed: 187.97 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.31835/61.00\%, bp\_loss: 1.44491/60.00\%, hp\_loss: 3.07567/22.00\%, j\_loss: 1.10193/74.00\%, 
		fr\_loss: 0.16323/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.28051
	Part 2 - fp\_loss: 1.46794/59.00\%, bp\_loss: 1.24822/70.00\%, hp\_loss: 2.76015/30.00\%, j\_loss: 1.07454/71.00\%, 
		fr\_loss: 0.19977/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.21079
	Part 3 - fp\_loss: 1.16314/68.00\%, bp\_loss: 0.94650/75.00\%, hp\_loss: 2.44347/36.00\%, j\_loss: 0.68107/82.00\%, 
		fr\_loss: 0.14012/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.41975
	Part 4 - fp\_loss: 1.50615/62.00\%, bp\_loss: 0.99928/75.00\%, hp\_loss: 2.61906/32.00\%, j\_loss: 0.69626/81.00\%, 
		fr\_loss: 0.18332/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.71815
	Training time elapsed: 225.36 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.41471/57.00\%, bp\_loss: 1.50258/59.00\%, hp\_loss: 3.08478/22.00\%, j\_loss: 1.27961/68.00\%, 
		fr\_loss: 0.16500/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.52817
	Part 2 - fp\_loss: 1.48853/60.00\%, bp\_loss: 1.27089/67.00\%, hp\_loss: 2.77111/29.00\%, j\_loss: 1.07013/72.00\%, 
		fr\_loss: 0.22625/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.25324
	Part 3 - fp\_loss: 1.10465/70.00\%, bp\_loss: 0.90318/78.00\%, hp\_loss: 2.43731/36.00\%, j\_loss: 0.60894/83.00\%, 
		fr\_loss: 0.13887/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.30227
	Part 4 - fp\_loss: 1.48151/63.00\%, bp\_loss: 0.98901/73.00\%, hp\_loss: 2.64357/32.00\%, j\_loss: 0.67969/81.00\%, 
		fr\_loss: 0.22916/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.73937
	Training time elapsed: 262.71 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.40389/58.00\%, bp\_loss: 1.73611/55.00\%, hp\_loss: 3.05646/23.00\%, j\_loss: 1.33235/66.00\%, 
		fr\_loss: 0.15268/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.62474
	Part 2 - fp\_loss: 1.43769/62.00\%, bp\_loss: 1.28784/67.00\%, hp\_loss: 2.69666/31.00\%, j\_loss: 1.08463/73.00\%, 
		fr\_loss: 0.22155/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.22038
	Part 3 - fp\_loss: 1.23141/67.00\%, bp\_loss: 0.88081/77.00\%, hp\_loss: 2.41335/36.00\%, j\_loss: 0.69851/81.00\%, 
		fr\_loss: 0.13297/87.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.43543
	Part 4 - fp\_loss: 1.45191/63.00\%, bp\_loss: 1.03274/72.00\%, hp\_loss: 2.58447/34.00\%, j\_loss: 0.80203/79.00\%, 
		fr\_loss: 0.20195/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.81510
	Training time elapsed: 300.07 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.74289/51.00\%, bp\_loss: 1.90518/49.00\%, hp\_loss: 3.10246/25.00\%, j\_loss: 1.77997/55.00\%, 
		fr\_loss: 0.22938/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.75989
	Part 2 - fp\_loss: 1.32809/62.00\%, bp\_loss: 1.68992/56.00\%, hp\_loss: 2.47926/44.00\%, j\_loss: 1.38181/61.00\%, 
		fr\_loss: 0.24052/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 7.11960
	Part 3 - fp\_loss: 1.10486/72.00\%, bp\_loss: 1.47634/65.00\%, hp\_loss: 1.96321/51.00\%, j\_loss: 0.97301/73.00\%, 
		fr\_loss: 0.18798/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.70540
	Part 4 - fp\_loss: 1.38138/64.00\%, bp\_loss: 2.35397/54.00\%, hp\_loss: 2.18632/46.00\%, j\_loss: 1.40215/60.00\%, 
		fr\_loss: 0.27864/72.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 7.60247
	`Validation time elapsed: 0.75 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.77401/52.00\%, bp\_loss: 1.92365/50.00\%, hp\_loss: 3.11170/25.00\%, j\_loss: 1.74803/58.00\%, 
		fr\_loss: 0.24262/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.80000
	Part 2 - fp\_loss: 1.44315/60.00\%, bp\_loss: 1.41437/67.00\%, hp\_loss: 2.53047/42.00\%, j\_loss: 1.16444/72.00\%, 
		fr\_loss: 0.25023/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.80266
	Part 3 - fp\_loss: 1.14353/70.00\%, bp\_loss: 1.17248/74.00\%, hp\_loss: 1.96139/52.00\%, j\_loss: 0.74972/80.00\%, 
		fr\_loss: 0.19836/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.22548
	Part 4 - fp\_loss: 1.33384/64.00\%, bp\_loss: 1.27254/70.00\%, hp\_loss: 2.21945/45.00\%, j\_loss: 0.75217/77.00\%, 
		fr\_loss: 0.24060/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.81861
	`Validation time elapsed: 9.57 seconds
`
-----------
Completed epoch 12.

EPOCH 13
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.33479/60.00\%, bp\_loss: 1.75517/53.00\%, hp\_loss: 3.15947/20.00\%, j\_loss: 1.33486/65.00\%, 
		fr\_loss: 0.17568/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.65234
	Part 2 - fp\_loss: 1.53023/58.00\%, bp\_loss: 1.46380/63.00\%, hp\_loss: 2.71787/31.00\%, j\_loss: 1.24380/66.00\%, 
		fr\_loss: 0.21070/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.47412
	Part 3 - fp\_loss: 1.25901/65.00\%, bp\_loss: 1.33113/66.00\%, hp\_loss: 2.38751/39.00\%, j\_loss: 0.96906/71.00\%, 
		fr\_loss: 0.15938/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.87354
	Part 4 - fp\_loss: 1.69894/56.00\%, bp\_loss: 1.93521/60.00\%, hp\_loss: 2.63869/32.00\%, j\_loss: 1.29553/65.00\%, 
		fr\_loss: 0.24259/76.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.75976
	Training time elapsed: 1.00 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.27951/61.00\%, bp\_loss: 1.35789/64.00\%, hp\_loss: 3.02132/22.00\%, j\_loss: 1.03932/76.00\%, 
		fr\_loss: 0.14269/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.13553
	Part 2 - fp\_loss: 1.41859/60.00\%, bp\_loss: 1.54444/64.00\%, hp\_loss: 2.68985/30.00\%, j\_loss: 1.09080/69.00\%, 
		fr\_loss: 0.21744/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.28783
	Part 3 - fp\_loss: 1.18852/67.00\%, bp\_loss: 0.91273/78.00\%, hp\_loss: 2.36550/38.00\%, j\_loss: 0.63343/83.00\%, 
		fr\_loss: 0.14772/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.35888
	Part 4 - fp\_loss: 1.53081/60.00\%, bp\_loss: 1.19581/68.00\%, hp\_loss: 2.54227/34.00\%, j\_loss: 0.88942/75.00\%, 
		fr\_loss: 0.21412/78.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.99037
	Training time elapsed: 37.71 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.32853/62.00\%, bp\_loss: 1.92043/50.00\%, hp\_loss: 3.10439/22.00\%, j\_loss: 1.34787/68.00\%, 
		fr\_loss: 0.16574/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.68532
	Part 2 - fp\_loss: 1.49198/59.00\%, bp\_loss: 1.23355/66.00\%, hp\_loss: 2.76262/28.00\%, j\_loss: 1.13806/71.00\%, 
		fr\_loss: 0.23690/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.31980
	Part 3 - fp\_loss: 1.31964/65.00\%, bp\_loss: 0.97843/75.00\%, hp\_loss: 2.44177/36.00\%, j\_loss: 0.77478/80.00\%, 
		fr\_loss: 0.17930/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.63996
	Part 4 - fp\_loss: 1.51670/62.00\%, bp\_loss: 1.45856/65.00\%, hp\_loss: 2.65274/31.00\%, j\_loss: 0.94129/76.00\%, 
		fr\_loss: 0.21774/78.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.15077
	Training time elapsed: 74.47 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.39225/60.00\%, bp\_loss: 1.50341/59.00\%, hp\_loss: 3.15773/20.00\%, j\_loss: 1.18812/73.00\%, 
		fr\_loss: 0.14145/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.42404
	Part 2 - fp\_loss: 1.49973/59.00\%, bp\_loss: 1.25004/69.00\%, hp\_loss: 2.69755/31.00\%, j\_loss: 1.05410/72.00\%, 
		fr\_loss: 0.25424/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.24247
	Part 3 - fp\_loss: 1.40618/63.00\%, bp\_loss: 0.90655/76.00\%, hp\_loss: 2.35954/39.00\%, j\_loss: 0.77533/79.00\%, 
		fr\_loss: 0.21274/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.67098
	Part 4 - fp\_loss: 1.50375/61.00\%, bp\_loss: 1.08271/71.00\%, hp\_loss: 2.66989/32.00\%, j\_loss: 0.79956/79.00\%, 
		fr\_loss: 0.19401/80.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.87122
	Training time elapsed: 111.15 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.39572/60.00\%, bp\_loss: 1.90914/54.00\%, hp\_loss: 3.07823/22.00\%, j\_loss: 1.29017/67.00\%, 
		fr\_loss: 0.15076/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.63500
	Part 2 - fp\_loss: 1.52098/59.00\%, bp\_loss: 1.28491/67.00\%, hp\_loss: 2.75484/30.00\%, j\_loss: 1.09632/68.00\%, 
		fr\_loss: 0.22691/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.29565
	Part 3 - fp\_loss: 2.45323/45.00\%, bp\_loss: 0.84623/79.00\%, hp\_loss: 2.42032/38.00\%, j\_loss: 1.04212/79.00\%, 
		fr\_loss: 0.31086/68.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.55956
	Part 4 - fp\_loss: 1.53478/59.00\%, bp\_loss: 1.12927/70.00\%, hp\_loss: 2.57368/34.00\%, j\_loss: 0.91848/76.00\%, 
		fr\_loss: 0.21904/78.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 3.01580
	Training time elapsed: 147.81 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.35037/61.00\%, bp\_loss: 1.59669/61.00\%, hp\_loss: 3.04946/23.00\%, j\_loss: 1.14232/73.00\%, 
		fr\_loss: 0.14889/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.36023
	Part 2 - fp\_loss: 1.32885/63.00\%, bp\_loss: 1.17965/70.00\%, hp\_loss: 2.69324/33.00\%, j\_loss: 0.95376/73.00\%, 
		fr\_loss: 0.22214/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.00220
	Part 3 - fp\_loss: 2.05678/53.00\%, bp\_loss: 0.96160/76.00\%, hp\_loss: 2.35435/37.00\%, j\_loss: 1.00636/76.00\%, 
		fr\_loss: 0.39176/60.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.42130
	Part 4 - fp\_loss: 1.40962/63.00\%, bp\_loss: 1.36268/68.00\%, hp\_loss: 2.61478/31.00\%, j\_loss: 0.86023/77.00\%, 
		fr\_loss: 0.19940/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.95768
	Training time elapsed: 184.49 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.49846/58.00\%, bp\_loss: 1.74417/55.00\%, hp\_loss: 3.08406/20.00\%, j\_loss: 1.35421/67.00\%, 
		fr\_loss: 0.15256/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.70447
	Part 2 - fp\_loss: 1.51082/60.00\%, bp\_loss: 1.43872/65.00\%, hp\_loss: 2.76233/29.00\%, j\_loss: 1.16300/70.00\%, 
		fr\_loss: 0.19735/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.37608
	Part 3 - fp\_loss: 1.92832/56.00\%, bp\_loss: 0.87962/77.00\%, hp\_loss: 2.40997/36.00\%, j\_loss: 0.88914/78.00\%, 
		fr\_loss: 0.34290/65.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.18307
	Part 4 - fp\_loss: 1.49831/61.00\%, bp\_loss: 0.94902/75.00\%, hp\_loss: 2.61581/32.00\%, j\_loss: 0.75031/80.00\%, 
		fr\_loss: 0.21879/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.78771
	Training time elapsed: 221.13 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.46109/57.00\%, bp\_loss: 1.65307/58.00\%, hp\_loss: 3.07577/22.00\%, j\_loss: 1.28047/68.00\%, 
		fr\_loss: 0.16506/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.59472
	Part 2 - fp\_loss: 1.43561/61.00\%, bp\_loss: 1.25517/68.00\%, hp\_loss: 2.73518/32.00\%, j\_loss: 1.02701/72.00\%, 
		fr\_loss: 0.21527/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.15719
	Part 3 - fp\_loss: 1.85989/55.00\%, bp\_loss: 1.01159/74.00\%, hp\_loss: 2.39183/38.00\%, j\_loss: 0.93236/75.00\%, 
		fr\_loss: 0.37572/63.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.25905
	Part 4 - fp\_loss: 1.53298/60.00\%, bp\_loss: 1.10935/70.00\%, hp\_loss: 2.58932/33.00\%, j\_loss: 0.84514/79.00\%, 
		fr\_loss: 0.20104/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.92227
	Training time elapsed: 257.81 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.28857/62.00\%, bp\_loss: 1.55914/58.00\%, hp\_loss: 3.05763/24.00\%, j\_loss: 1.18401/71.00\%, 
		fr\_loss: 0.14113/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.35445
	Part 2 - fp\_loss: 1.34929/62.00\%, bp\_loss: 1.42584/67.00\%, hp\_loss: 2.68738/32.00\%, j\_loss: 1.06762/71.00\%, 
		fr\_loss: 0.22029/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.19652
	Part 3 - fp\_loss: 1.98043/54.00\%, bp\_loss: 1.00339/74.00\%, hp\_loss: 2.48359/36.00\%, j\_loss: 0.99400/76.00\%, 
		fr\_loss: 0.32097/67.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.35128
	Part 4 - fp\_loss: 1.42200/64.00\%, bp\_loss: 1.09645/72.00\%, hp\_loss: 2.67072/33.00\%, j\_loss: 0.80856/78.00\%, 
		fr\_loss: 0.22125/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.87096
	Training time elapsed: 294.46 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.62990/55.00\%, bp\_loss: 1.53707/58.00\%, hp\_loss: 3.04058/26.00\%, j\_loss: 1.44003/65.00\%, 
		fr\_loss: 0.26661/73.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 7.91419
	Part 2 - fp\_loss: 1.36703/62.00\%, bp\_loss: 1.73636/56.00\%, hp\_loss: 2.56637/41.00\%, j\_loss: 1.28712/62.00\%, 
		fr\_loss: 0.24788/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 7.20477
	Part 3 - fp\_loss: 1.62846/60.00\%, bp\_loss: 1.16323/69.00\%, hp\_loss: 1.91243/53.00\%, j\_loss: 1.21983/69.00\%, 
		fr\_loss: 0.41704/58.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.34099
	Part 4 - fp\_loss: 1.34933/66.00\%, bp\_loss: 2.29124/58.00\%, hp\_loss: 2.18297/46.00\%, j\_loss: 1.26697/63.00\%, 
		fr\_loss: 0.26024/73.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 7.35076
	`Validation time elapsed: 0.74 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.84198/50.00\%, bp\_loss: 1.84048/53.00\%, hp\_loss: 3.08146/26.00\%, j\_loss: 1.68830/60.00\%, 
		fr\_loss: 0.27436/72.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.72658
	Part 2 - fp\_loss: 1.42244/61.00\%, bp\_loss: 1.50390/65.00\%, hp\_loss: 2.46597/45.00\%, j\_loss: 1.15561/68.00\%, 
		fr\_loss: 0.25577/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.80371
	Part 3 - fp\_loss: 1.59999/59.00\%, bp\_loss: 1.17952/72.00\%, hp\_loss: 1.94573/52.00\%, j\_loss: 1.08665/73.00\%, 
		fr\_loss: 0.42084/57.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.23273
	Part 4 - fp\_loss: 1.26569/67.00\%, bp\_loss: 1.22155/69.00\%, hp\_loss: 2.13626/46.00\%, j\_loss: 0.83981/79.00\%, 
		fr\_loss: 0.25803/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.72133
	`Validation time elapsed: 9.54 seconds
`
-----------
Completed epoch 13.

EPOCH 14
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.40551/58.00\%, bp\_loss: 1.41891/60.00\%, hp\_loss: 3.13831/19.00\%, j\_loss: 1.18361/73.00\%, 
		fr\_loss: 0.15502/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.40855
	Part 2 - fp\_loss: 1.36199/62.00\%, bp\_loss: 1.53237/62.00\%, hp\_loss: 2.75380/30.00\%, j\_loss: 1.14595/68.00\%, 
		fr\_loss: 0.21280/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.32559
	Part 3 - fp\_loss: 1.94617/54.00\%, bp\_loss: 1.04662/72.00\%, hp\_loss: 2.43961/36.00\%, j\_loss: 1.07822/74.00\%, 
		fr\_loss: 0.32766/67.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.42483
	Part 4 - fp\_loss: 1.49148/63.00\%, bp\_loss: 1.71248/66.00\%, hp\_loss: 2.64358/32.00\%, j\_loss: 1.01908/75.00\%, 
		fr\_loss: 0.20920/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.28083
	Training time elapsed: 1.10 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.43917/58.00\%, bp\_loss: 1.52138/58.00\%, hp\_loss: 3.07092/21.00\%, j\_loss: 1.24192/70.00\%, 
		fr\_loss: 0.14533/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.48453
	Part 2 - fp\_loss: 1.36083/62.00\%, bp\_loss: 1.20541/69.00\%, hp\_loss: 2.67058/32.00\%, j\_loss: 0.99397/73.00\%, 
		fr\_loss: 0.21545/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.05263
	Part 3 - fp\_loss: 1.76279/59.00\%, bp\_loss: 0.93862/75.00\%, hp\_loss: 2.29632/39.00\%, j\_loss: 0.90120/75.00\%, 
		fr\_loss: 0.34629/65.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.09937
	Part 4 - fp\_loss: 1.41772/63.00\%, bp\_loss: 0.96660/75.00\%, hp\_loss: 2.54433/36.00\%, j\_loss: 0.68791/82.00\%, 
		fr\_loss: 0.21577/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.66582
	Training time elapsed: 38.55 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.29461/63.00\%, bp\_loss: 1.56324/60.00\%, hp\_loss: 3.08909/23.00\%, j\_loss: 1.16434/74.00\%, 
		fr\_loss: 0.13984/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.34719
	Part 2 - fp\_loss: 1.40815/62.00\%, bp\_loss: 1.29202/68.00\%, hp\_loss: 2.71381/31.00\%, j\_loss: 1.06410/71.00\%, 
		fr\_loss: 0.21831/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.18823
	Part 3 - fp\_loss: 1.86497/56.00\%, bp\_loss: 0.89815/76.00\%, hp\_loss: 2.38093/37.00\%, j\_loss: 0.97333/76.00\%, 
		fr\_loss: 0.32007/68.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.20961
	Part 4 - fp\_loss: 1.33242/66.00\%, bp\_loss: 1.08794/72.00\%, hp\_loss: 2.58993/34.00\%, j\_loss: 0.81694/78.00\%, 
		fr\_loss: 0.20291/79.00\%, p\_loss: 0.00001/100.00\%, 
		total weighted loss: 2.78942
	Training time elapsed: 75.94 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.30457/61.00\%, bp\_loss: 1.44591/60.00\%, hp\_loss: 3.01896/24.00\%, j\_loss: 1.07410/75.00\%, 
		fr\_loss: 0.16453/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.23037
	Part 2 - fp\_loss: 1.34559/64.00\%, bp\_loss: 2.12621/44.00\%, hp\_loss: 2.66950/32.00\%, j\_loss: 1.41108/63.00\%, 
		fr\_loss: 0.21246/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.73504
	Part 3 - fp\_loss: 1.84681/56.00\%, bp\_loss: 0.86437/78.00\%, hp\_loss: 2.40343/36.00\%, j\_loss: 0.84247/79.00\%, 
		fr\_loss: 0.31716/67.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.06337
	Part 4 - fp\_loss: 1.40102/63.00\%, bp\_loss: 1.08346/73.00\%, hp\_loss: 2.59673/33.00\%, j\_loss: 0.74619/81.00\%, 
		fr\_loss: 0.20064/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.75140
	Training time elapsed: 113.36 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.40872/58.00\%, bp\_loss: 1.56359/59.00\%, hp\_loss: 3.09886/23.00\%, j\_loss: 1.16595/73.00\%, 
		fr\_loss: 0.15630/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.42535
	Part 2 - fp\_loss: 1.45114/59.00\%, bp\_loss: 1.54499/60.00\%, hp\_loss: 2.68419/33.00\%, j\_loss: 1.22740/64.00\%, 
		fr\_loss: 0.21397/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.43569
	Part 3 - fp\_loss: 1.92357/55.00\%, bp\_loss: 0.85718/78.00\%, hp\_loss: 2.41736/36.00\%, j\_loss: 0.87957/78.00\%, 
		fr\_loss: 0.36689/64.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.19060
	Part 4 - fp\_loss: 1.40794/64.00\%, bp\_loss: 1.18291/68.00\%, hp\_loss: 2.54155/35.00\%, j\_loss: 0.90701/78.00\%, 
		fr\_loss: 0.22089/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.94921
	Training time elapsed: 150.75 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.43843/57.00\%, bp\_loss: 1.45414/60.00\%, hp\_loss: 3.07906/23.00\%, j\_loss: 1.26402/70.00\%, 
		fr\_loss: 0.14803/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.49123
	Part 2 - fp\_loss: 1.41986/61.00\%, bp\_loss: 1.39069/67.00\%, hp\_loss: 2.67030/33.00\%, j\_loss: 1.06579/70.00\%, 
		fr\_loss: 0.20945/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.20346
	Part 3 - fp\_loss: 1.93028/55.00\%, bp\_loss: 0.89859/77.00\%, hp\_loss: 2.43246/37.00\%, j\_loss: 0.88204/78.00\%, 
		fr\_loss: 0.37110/63.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.21759
	Part 4 - fp\_loss: 1.40407/64.00\%, bp\_loss: 1.00950/74.00\%, hp\_loss: 2.60194/35.00\%, j\_loss: 0.74504/80.00\%, 
		fr\_loss: 0.21494/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.74544
	Training time elapsed: 188.15 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.38069/62.00\%, bp\_loss: 1.42055/61.00\%, hp\_loss: 3.06616/24.00\%, j\_loss: 1.16095/71.00\%, 
		fr\_loss: 0.16402/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.36133
	Part 2 - fp\_loss: 1.36999/62.00\%, bp\_loss: 1.45461/65.00\%, hp\_loss: 2.68330/31.00\%, j\_loss: 1.08562/70.00\%, 
		fr\_loss: 0.21871/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.23071
	Part 3 - fp\_loss: 1.90650/55.00\%, bp\_loss: 0.87756/78.00\%, hp\_loss: 2.44312/37.00\%, j\_loss: 0.89870/78.00\%, 
		fr\_loss: 0.31931/68.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.16747
	Part 4 - fp\_loss: 1.44311/61.00\%, bp\_loss: 0.97964/73.00\%, hp\_loss: 2.62522/32.00\%, j\_loss: 0.78175/80.00\%, 
		fr\_loss: 0.18533/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.77009
	Training time elapsed: 225.53 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.42754/57.00\%, bp\_loss: 1.51908/58.00\%, hp\_loss: 3.08540/22.00\%, j\_loss: 1.32551/67.00\%, 
		fr\_loss: 0.15472/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.57535
	Part 2 - fp\_loss: 1.42044/59.00\%, bp\_loss: 1.36093/65.00\%, hp\_loss: 2.69400/30.00\%, j\_loss: 1.09923/70.00\%, 
		fr\_loss: 0.23428/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.26021
	Part 3 - fp\_loss: 1.91033/55.00\%, bp\_loss: 0.88639/78.00\%, hp\_loss: 2.42534/35.00\%, j\_loss: 0.84881/78.00\%, 
		fr\_loss: 0.32405/67.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.12155
	Part 4 - fp\_loss: 1.46909/61.00\%, bp\_loss: 1.31782/70.00\%, hp\_loss: 2.61292/34.00\%, j\_loss: 0.86496/78.00\%, 
		fr\_loss: 0.21053/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.98926
	Training time elapsed: 262.91 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 1.34604/60.00\%, bp\_loss: 1.51046/58.00\%, hp\_loss: 3.10201/21.00\%, j\_loss: 1.22055/71.00\%, 
		fr\_loss: 0.14105/86.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.41835
	Part 2 - fp\_loss: 1.49271/61.00\%, bp\_loss: 1.44057/65.00\%, hp\_loss: 2.76254/30.00\%, j\_loss: 1.08136/71.00\%, 
		fr\_loss: 0.22841/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.31706
	Part 3 - fp\_loss: 1.82438/57.00\%, bp\_loss: 0.87701/76.00\%, hp\_loss: 2.44273/36.00\%, j\_loss: 0.85535/77.00\%, 
		fr\_loss: 0.33735/66.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.10081
	Part 4 - fp\_loss: 1.40084/64.00\%, bp\_loss: 0.98050/73.00\%, hp\_loss: 2.58146/33.00\%, j\_loss: 0.77707/80.00\%, 
		fr\_loss: 0.20578/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.75186
	Training time elapsed: 300.27 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 1.73643/51.00\%, bp\_loss: 1.72616/52.00\%, hp\_loss: 3.04032/26.00\%, j\_loss: 1.71112/58.00\%, 
		fr\_loss: 0.26113/73.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.47517
	Part 2 - fp\_loss: 1.36580/61.00\%, bp\_loss: 1.87547/55.00\%, hp\_loss: 2.59577/41.00\%, j\_loss: 1.36365/62.00\%, 
		fr\_loss: 0.23513/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 7.43582
	Part 3 - fp\_loss: 1.68507/58.00\%, bp\_loss: 1.08937/70.00\%, hp\_loss: 2.01632/50.00\%, j\_loss: 1.17011/71.00\%, 
		fr\_loss: 0.38951/60.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.35038
	Part 4 - fp\_loss: 1.31981/67.00\%, bp\_loss: 1.91657/59.00\%, hp\_loss: 2.18562/44.00\%, j\_loss: 1.23355/64.00\%, 
		fr\_loss: 0.23545/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.89101
	`Validation time elapsed: 0.74 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 1.69790/53.00\%, bp\_loss: 1.74554/53.00\%, hp\_loss: 3.11020/23.00\%, j\_loss: 1.62805/62.00\%, 
		fr\_loss: 0.25468/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 8.43636
	Part 2 - fp\_loss: 1.35598/61.00\%, bp\_loss: 1.69790/62.00\%, hp\_loss: 2.52674/42.00\%, j\_loss: 1.20854/67.00\%, 
		fr\_loss: 0.25298/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 7.04214
	Part 3 - fp\_loss: 1.61055/61.00\%, bp\_loss: 1.08451/75.00\%, hp\_loss: 1.98720/51.00\%, j\_loss: 0.94932/75.00\%, 
		fr\_loss: 0.39872/59.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.03030
	Part 4 - fp\_loss: 1.31660/65.00\%, bp\_loss: 1.19301/70.00\%, hp\_loss: 2.23752/43.00\%, j\_loss: 0.81436/78.00\%, 
		fr\_loss: 0.21716/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.77866
	`Validation time elapsed: 9.51 seconds
`
-----------
Completed epoch 14.

EPOCH 15
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 1.35045/62.00\%, bp\_loss: 1.42832/61.00\%, hp\_loss: 3.09501/21.00\%, j\_loss: 1.14278/72.00\%, 
		fr\_loss: 0.14872/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.32373
	Part 2 - fp\_loss: 1.33357/62.00\%, bp\_loss: 1.51748/60.00\%, hp\_loss: 2.70455/32.00\%, j\_loss: 1.14272/69.00\%, 
		fr\_loss: 0.22545/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.30156
	Part 3 - fp\_loss: 1.87471/54.00\%, bp\_loss: 0.87579/77.00\%, hp\_loss: 2.42310/37.00\%, j\_loss: 0.92826/78.00\%, 
		fr\_loss: 0.32760/67.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.18288
	Part 4 - fp\_loss: 1.51570/61.00\%, bp\_loss: 1.48056/68.00\%, hp\_loss: 2.63820/34.00\%, j\_loss: 1.09639/71.00\%, 
		fr\_loss: 0.19601/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.28588
	Training time elapsed: 1.05 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 1.44160/59.00\%, bp\_loss: 1.56276/57.00\%, hp\_loss: 3.13265/20.00\%, j\_loss: 1.31519/69.00\%, 
		fr\_loss: 0.14673/85.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.59135
	Part 2 - fp\_loss: 1.37010/62.00\%, bp\_loss: 1.37998/67.00\%, hp\_loss: 2.73796/30.00\%, j\_loss: 1.06687/71.00\%, 
		fr\_loss: 0.21044/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.19775
	Part 3 - fp\_loss: 1.78332/60.00\%, bp\_loss: 0.77785/80.00\%, hp\_loss: 2.39508/37.00\%, j\_loss: 0.71129/81.00\%, 
		fr\_loss: 0.34158/66.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.89641
	Part 4 - fp\_loss: 1.39230/64.00\%, bp\_loss: 0.97546/74.00\%, hp\_loss: 2.63111/33.00\%, j\_loss: 0.73899/81.00\%, 
		fr\_loss: 0.20343/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.72054
	Training time elapsed: 38.48 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 1.37855/58.00\%, bp\_loss: 1.43410/61.00\%, hp\_loss: 3.10567/20.00\%, j\_loss: 1.14663/73.00\%, 
		fr\_loss: 0.16719/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.36503
	Part 2 - fp\_loss: 1.34451/64.00\%, bp\_loss: 1.34156/66.00\%, hp\_loss: 2.72576/30.00\%, j\_loss: 1.03617/73.00\%, 
		fr\_loss: 0.21843/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.14705
	Part 3 - fp\_loss: 1.80669/58.00\%, bp\_loss: 1.12712/74.00\%, hp\_loss: 2.42567/36.00\%, j\_loss: 0.98237/74.00\%, 
		fr\_loss: 0.32730/67.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.27885
	Part 4 - fp\_loss: 1.42943/60.00\%, bp\_loss: 1.40249/71.00\%, hp\_loss: 2.66221/30.00\%, j\_loss: 0.79359/78.00\%, 
		fr\_loss: 0.18803/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.91575
	Training time elapsed: 75.89 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 1.51751/55.00\%, bp\_loss: 1.34855/61.00\%, hp\_loss: 3.04453/22.00\%, j\_loss: 1.32264/69.00\%, 
		fr\_loss: 0.14921/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.54854
	Part 2 - fp\_loss: 1.44330/61.00\%, bp\_loss: 1.31015/69.00\%, hp\_loss: 2.76933/29.00\%, j\_loss: 1.00512/74.00\%, 
		fr\_loss: 0.20029/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.15091
	Part 3 - fp\_loss: 1.75214/59.00\%, bp\_loss: 1.07288/72.00\%, hp\_loss: 2.38201/38.00\%, j\_loss: 0.94244/73.00\%, 
		fr\_loss: 0.32469/67.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.17966
	Part 4 - fp\_loss: 1.36562/64.00\%, bp\_loss: 0.94427/76.00\%, hp\_loss: 2.53860/34.00\%, j\_loss: 0.65544/82.00\%, 
		fr\_loss: 0.17659/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.55970
	Training time elapsed: 113.27 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 1.57437/55.00\%, bp\_loss: 1.31084/63.00\%, hp\_loss: 3.10854/21.00\%, j\_loss: 1.19380/72.00\%, 
		fr\_loss: 0.19845/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.50524
	Part 2 - fp\_loss: 1.38071/62.00\%, bp\_loss: 1.23809/68.00\%, hp\_loss: 2.74088/29.00\%, j\_loss: 1.03277/74.00\%, 
		fr\_loss: 0.20933/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.12615
	Part 3 - fp\_loss: 1.95774/54.00\%, bp\_loss: 1.00234/76.00\%, hp\_loss: 2.39872/38.00\%, j\_loss: 0.92821/77.00\%, 
		fr\_loss: 0.35652/64.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.28392
	Part 4 - fp\_loss: 1.40194/62.00\%, bp\_loss: 0.93607/76.00\%, hp\_loss: 2.64628/31.00\%, j\_loss: 0.73006/79.00\%, 
		fr\_loss: 0.20171/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.70744
	Training time elapsed: 150.67 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 1.54477/55.00\%, bp\_loss: 1.39028/61.00\%, hp\_loss: 3.12857/21.00\%, j\_loss: 1.21021/71.00\%, 
		fr\_loss: 0.17622/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.51446
	Part 2 - fp\_loss: 1.29902/64.00\%, bp\_loss: 1.33442/68.00\%, hp\_loss: 2.69916/31.00\%, j\_loss: 0.96143/75.00\%, 
		fr\_loss: 0.21179/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.03280
	Part 3 - fp\_loss: 1.77850/59.00\%, bp\_loss: 1.15882/73.00\%, hp\_loss: 2.38316/37.00\%, j\_loss: 0.95102/74.00\%, 
		fr\_loss: 0.35364/64.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.25651
	Part 4 - fp\_loss: 1.37665/64.00\%, bp\_loss: 1.30834/71.00\%, hp\_loss: 2.66750/32.00\%, j\_loss: 0.74862/79.00\%, 
		fr\_loss: 0.20128/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.83098
	Training time elapsed: 188.08 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 1.51527/55.00\%, bp\_loss: 1.48279/60.00\%, hp\_loss: 3.05674/22.00\%, j\_loss: 1.26094/71.00\%, 
		fr\_loss: 0.15762/84.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.53806
	Part 2 - fp\_loss: 1.36039/64.00\%, bp\_loss: 1.45800/64.00\%, hp\_loss: 2.74475/30.00\%, j\_loss: 1.12965/74.00\%, 
		fr\_loss: 0.20837/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.27904
	Part 3 - fp\_loss: 1.74953/60.00\%, bp\_loss: 1.15190/71.00\%, hp\_loss: 2.37247/38.00\%, j\_loss: 0.93133/73.00\%, 
		fr\_loss: 0.32654/67.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.18995
	Part 4 - fp\_loss: 1.32943/65.00\%, bp\_loss: 0.99244/74.00\%, hp\_loss: 2.58840/34.00\%, j\_loss: 0.69713/80.00\%, 
		fr\_loss: 0.18933/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.62543
	Training time elapsed: 225.49 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 1.78117/50.00\%, bp\_loss: 2.54321/45.00\%, hp\_loss: 3.07686/21.00\%, j\_loss: 1.81721/52.00\%, 
		fr\_loss: 0.21852/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.61234
	Part 2 - fp\_loss: 1.45666/61.00\%, bp\_loss: 1.41709/64.00\%, hp\_loss: 2.72568/30.00\%, j\_loss: 1.15848/72.00\%, 
		fr\_loss: 0.22696/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.35660
	Part 3 - fp\_loss: 1.80446/56.00\%, bp\_loss: 1.00826/75.00\%, hp\_loss: 2.47464/35.00\%, j\_loss: 0.93042/76.00\%, 
		fr\_loss: 0.31321/69.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.19073
	Part 4 - fp\_loss: 1.34987/64.00\%, bp\_loss: 1.01472/74.00\%, hp\_loss: 2.63556/31.00\%, j\_loss: 0.76442/80.00\%, 
		fr\_loss: 0.18697/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.72142
	Training time elapsed: 262.89 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 2.79493/30.00\%, bp\_loss: 1.95790/54.00\%, hp\_loss: 3.04860/22.00\%, j\_loss: 2.04149/53.00\%, 
		fr\_loss: 0.42523/57.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.36613
	Part 2 - fp\_loss: 1.36350/63.00\%, bp\_loss: 1.32081/67.00\%, hp\_loss: 2.74274/29.00\%, j\_loss: 0.98131/76.00\%, 
		fr\_loss: 0.20382/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.08594
	Part 3 - fp\_loss: 1.78527/58.00\%, bp\_loss: 0.95331/78.00\%, hp\_loss: 2.40657/37.00\%, j\_loss: 0.77683/79.00\%, 
		fr\_loss: 0.30053/70.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.97796
	Part 4 - fp\_loss: 1.39298/63.00\%, bp\_loss: 1.16581/70.00\%, hp\_loss: 2.67168/29.00\%, j\_loss: 0.84688/78.00\%, 
		fr\_loss: 0.19542/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.89004
	Training time elapsed: 300.29 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 2.77575/34.00\%, bp\_loss: 1.96659/50.00\%, hp\_loss: 3.02702/27.00\%, j\_loss: 2.12728/50.00\%, 
		fr\_loss: 0.37536/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 10.27200
	Part 2 - fp\_loss: 1.39266/61.00\%, bp\_loss: 1.54106/59.00\%, hp\_loss: 2.53500/40.00\%, j\_loss: 1.23370/67.00\%, 
		fr\_loss: 0.25551/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.95792
	Part 3 - fp\_loss: 1.57682/63.00\%, bp\_loss: 1.20244/68.00\%, hp\_loss: 1.94401/52.00\%, j\_loss: 1.08211/71.00\%, 
		fr\_loss: 0.39323/60.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.19861
	Part 4 - fp\_loss: 1.18444/69.00\%, bp\_loss: 1.66710/59.00\%, hp\_loss: 2.13253/47.00\%, j\_loss: 1.09979/64.00\%, 
		fr\_loss: 0.21653/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.30040
	`Validation time elapsed: 0.75 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 2.80119/33.00\%, bp\_loss: 2.36049/47.00\%, hp\_loss: 3.05930/26.00\%, j\_loss: 2.28914/46.00\%, 
		fr\_loss: 0.41060/59.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 10.92071
	Part 2 - fp\_loss: 1.37259/61.00\%, bp\_loss: 1.51527/66.00\%, hp\_loss: 2.59539/42.00\%, j\_loss: 1.11806/69.00\%, 
		fr\_loss: 0.25058/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.85189
	Part 3 - fp\_loss: 1.61912/63.00\%, bp\_loss: 1.25864/71.00\%, hp\_loss: 2.00392/50.00\%, j\_loss: 0.94089/71.00\%, 
		fr\_loss: 0.39970/60.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.22227
	Part 4 - fp\_loss: 1.27936/66.00\%, bp\_loss: 1.32851/67.00\%, hp\_loss: 2.21600/42.00\%, j\_loss: 0.87951/75.00\%, 
		fr\_loss: 0.24045/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.94383
	`Validation time elapsed: 9.54 seconds
`
-----------
Completed epoch 15.

EPOCH 16
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 2.76502/32.00\%, bp\_loss: 1.70490/58.00\%, hp\_loss: 3.02112/25.00\%, j\_loss: 1.95075/57.00\%, 
		fr\_loss: 0.41662/58.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.16769
	Part 2 - fp\_loss: 1.44857/58.00\%, bp\_loss: 1.41371/62.00\%, hp\_loss: 2.77074/29.00\%, j\_loss: 1.20157/67.00\%, 
		fr\_loss: 0.20500/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.38619
	Part 3 - fp\_loss: 1.84892/57.00\%, bp\_loss: 0.97692/76.00\%, hp\_loss: 2.38164/37.00\%, j\_loss: 0.92536/77.00\%, 
		fr\_loss: 0.31555/68.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.17294
	Part 4 - fp\_loss: 1.44557/62.00\%, bp\_loss: 1.30420/66.00\%, hp\_loss: 2.63348/32.00\%, j\_loss: 1.05639/72.00\%, 
		fr\_loss: 0.20450/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.16499
	Training time elapsed: 1.01 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 2.64091/34.00\%, bp\_loss: 2.05988/51.00\%, hp\_loss: 3.10384/20.00\%, j\_loss: 1.99098/53.00\%, 
		fr\_loss: 0.43489/56.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.29543
	Part 2 - fp\_loss: 1.33790/63.00\%, bp\_loss: 1.34412/66.00\%, hp\_loss: 2.69156/31.00\%, j\_loss: 1.04225/69.00\%, 
		fr\_loss: 0.19468/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.11658
	Part 3 - fp\_loss: 1.80345/58.00\%, bp\_loss: 1.00256/74.00\%, hp\_loss: 2.42491/36.00\%, j\_loss: 0.93049/76.00\%, 
		fr\_loss: 0.30767/69.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.16812
	Part 4 - fp\_loss: 1.32211/65.00\%, bp\_loss: 1.03869/73.00\%, hp\_loss: 2.64580/32.00\%, j\_loss: 0.70318/82.00\%, 
		fr\_loss: 0.18677/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.65635
	Training time elapsed: 38.27 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 2.41860/42.00\%, bp\_loss: 1.83013/55.00\%, hp\_loss: 3.07864/22.00\%, j\_loss: 1.75217/57.00\%, 
		fr\_loss: 0.46191/53.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.89601
	Part 2 - fp\_loss: 1.36733/63.00\%, bp\_loss: 1.25949/69.00\%, hp\_loss: 2.72425/30.00\%, j\_loss: 0.97499/75.00\%, 
		fr\_loss: 0.18511/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.03889
	Part 3 - fp\_loss: 1.73965/59.00\%, bp\_loss: 1.03425/74.00\%, hp\_loss: 2.38524/38.00\%, j\_loss: 0.89229/74.00\%, 
		fr\_loss: 0.30283/69.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.09080
	Part 4 - fp\_loss: 1.40517/64.00\%, bp\_loss: 0.98173/74.00\%, hp\_loss: 2.69706/30.00\%, j\_loss: 0.72852/79.00\%, 
		fr\_loss: 0.20830/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.74304
	Training time elapsed: 75.51 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 2.31599/44.00\%, bp\_loss: 1.77028/58.00\%, hp\_loss: 3.04325/24.00\%, j\_loss: 1.61661/59.00\%, 
		fr\_loss: 0.46195/53.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.68061
	Part 2 - fp\_loss: 1.44423/62.00\%, bp\_loss: 1.49717/65.00\%, hp\_loss: 2.77057/29.00\%, j\_loss: 1.05248/71.00\%, 
		fr\_loss: 0.21231/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.26722
	Part 3 - fp\_loss: 1.73252/61.00\%, bp\_loss: 1.03358/74.00\%, hp\_loss: 2.35062/39.00\%, j\_loss: 0.85678/74.00\%, 
		fr\_loss: 0.31583/68.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.05413
	Part 4 - fp\_loss: 1.34094/64.00\%, bp\_loss: 0.94106/76.00\%, hp\_loss: 2.57734/33.00\%, j\_loss: 0.67100/81.00\%, 
		fr\_loss: 0.20751/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.60451
	Training time elapsed: 112.73 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 2.38238/41.00\%, bp\_loss: 1.75624/56.00\%, hp\_loss: 3.07778/22.00\%, j\_loss: 1.68534/59.00\%, 
		fr\_loss: 0.42892/57.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.75565
	Part 2 - fp\_loss: 1.37011/60.00\%, bp\_loss: 1.22004/70.00\%, hp\_loss: 2.64403/33.00\%, j\_loss: 0.94661/75.00\%, 
		fr\_loss: 0.20278/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.99366
	Part 3 - fp\_loss: 1.73425/59.00\%, bp\_loss: 1.14158/71.00\%, hp\_loss: 2.39895/37.00\%, j\_loss: 0.94318/73.00\%, 
		fr\_loss: 0.27817/72.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.15063
	Part 4 - fp\_loss: 1.35862/64.00\%, bp\_loss: 1.20123/69.00\%, hp\_loss: 2.57988/34.00\%, j\_loss: 0.79798/74.00\%, 
		fr\_loss: 0.18503/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.79666
	Training time elapsed: 149.94 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 2.31852/43.00\%, bp\_loss: 1.70093/56.00\%, hp\_loss: 3.04727/23.00\%, j\_loss: 1.67087/57.00\%, 
		fr\_loss: 0.45117/54.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.70576
	Part 2 - fp\_loss: 1.43710/61.00\%, bp\_loss: 1.26386/69.00\%, hp\_loss: 2.77584/29.00\%, j\_loss: 1.01606/76.00\%, 
		fr\_loss: 0.20853/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.15505
	Part 3 - fp\_loss: 1.72594/59.00\%, bp\_loss: 1.07386/74.00\%, hp\_loss: 2.43965/36.00\%, j\_loss: 0.95040/76.00\%, 
		fr\_loss: 0.28308/72.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.15049
	Part 4 - fp\_loss: 1.35948/63.00\%, bp\_loss: 1.30451/69.00\%, hp\_loss: 2.69219/29.00\%, j\_loss: 0.84231/77.00\%, 
		fr\_loss: 0.17483/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.89589
	Training time elapsed: 187.14 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 2.44900/39.00\%, bp\_loss: 1.60023/59.00\%, hp\_loss: 3.02311/23.00\%, j\_loss: 1.65788/61.00\%, 
		fr\_loss: 0.46509/53.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.73447
	Part 2 - fp\_loss: 1.41112/60.00\%, bp\_loss: 1.29340/67.00\%, hp\_loss: 2.74797/30.00\%, j\_loss: 1.09213/74.00\%, 
		fr\_loss: 0.21162/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.22172
	Part 3 - fp\_loss: 1.66654/61.00\%, bp\_loss: 1.04415/74.00\%, hp\_loss: 2.40354/36.00\%, j\_loss: 0.83618/77.00\%, 
		fr\_loss: 0.26349/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.96724
	Part 4 - fp\_loss: 1.31650/65.00\%, bp\_loss: 1.25383/70.00\%, hp\_loss: 2.52666/34.00\%, j\_loss: 0.78777/78.00\%, 
		fr\_loss: 0.20261/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.78277
	Training time elapsed: 224.39 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 2.56664/37.00\%, bp\_loss: 1.71545/59.00\%, hp\_loss: 3.07205/23.00\%, j\_loss: 1.67648/60.00\%, 
		fr\_loss: 0.41826/58.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.81431
	Part 2 - fp\_loss: 1.34856/63.00\%, bp\_loss: 1.39946/66.00\%, hp\_loss: 2.78313/29.00\%, j\_loss: 0.99597/72.00\%, 
		fr\_loss: 0.20667/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.13170
	Part 3 - fp\_loss: 1.83293/56.00\%, bp\_loss: 1.00564/75.00\%, hp\_loss: 2.46890/35.00\%, j\_loss: 0.92883/76.00\%, 
		fr\_loss: 0.30302/69.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.19067
	Part 4 - fp\_loss: 1.42078/61.00\%, bp\_loss: 1.10406/71.00\%, hp\_loss: 2.64277/29.00\%, j\_loss: 0.82272/79.00\%, 
		fr\_loss: 0.16714/83.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.82430
	Training time elapsed: 261.61 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 2.58391/37.00\%, bp\_loss: 1.77081/59.00\%, hp\_loss: 3.14886/21.00\%, j\_loss: 1.70541/58.00\%, 
		fr\_loss: 0.43447/56.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.90774
	Part 2 - fp\_loss: 1.39157/61.00\%, bp\_loss: 1.33123/67.00\%, hp\_loss: 2.80886/28.00\%, j\_loss: 1.05371/72.00\%, 
		fr\_loss: 0.21990/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.21142
	Part 3 - fp\_loss: 1.77424/58.00\%, bp\_loss: 1.00285/76.00\%, hp\_loss: 2.39986/38.00\%, j\_loss: 0.88414/77.00\%, 
		fr\_loss: 0.27870/72.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.07078
	Part 4 - fp\_loss: 1.41287/61.00\%, bp\_loss: 1.15410/71.00\%, hp\_loss: 2.55107/34.00\%, j\_loss: 0.87169/75.00\%, 
		fr\_loss: 0.22754/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.91721
	Training time elapsed: 298.86 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 2.51546/39.00\%, bp\_loss: 1.85284/50.00\%, hp\_loss: 3.06633/25.00\%, j\_loss: 2.06146/50.00\%, 
		fr\_loss: 0.41821/58.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 9.91430
	Part 2 - fp\_loss: 1.30517/64.00\%, bp\_loss: 1.76933/57.00\%, hp\_loss: 2.61553/40.00\%, j\_loss: 1.25446/67.00\%, 
		fr\_loss: 0.21462/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 7.15912
	Part 3 - fp\_loss: 1.66583/58.00\%, bp\_loss: 1.14152/70.00\%, hp\_loss: 1.93590/51.00\%, j\_loss: 1.09898/72.00\%, 
		fr\_loss: 0.39421/60.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.23645
	Part 4 - fp\_loss: 1.25792/67.00\%, bp\_loss: 1.75462/54.00\%, hp\_loss: 2.17107/44.00\%, j\_loss: 1.33578/61.00\%, 
		fr\_loss: 0.23203/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.75142
	`Validation time elapsed: 0.77 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 2.58880/38.00\%, bp\_loss: 1.89374/53.00\%, hp\_loss: 3.12427/24.00\%, j\_loss: 1.93867/55.00\%, 
		fr\_loss: 0.43236/56.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 9.97784
	Part 2 - fp\_loss: 1.32682/63.00\%, bp\_loss: 1.53119/65.00\%, hp\_loss: 2.53598/43.00\%, j\_loss: 1.05929/72.00\%, 
		fr\_loss: 0.22732/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.68060
	Part 3 - fp\_loss: 1.41875/66.00\%, bp\_loss: 1.22319/72.00\%, hp\_loss: 1.95774/52.00\%, j\_loss: 0.88709/76.00\%, 
		fr\_loss: 0.30101/70.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.78777
	Part 4 - fp\_loss: 1.19430/68.00\%, bp\_loss: 1.45939/65.00\%, hp\_loss: 2.19728/43.00\%, j\_loss: 0.85120/75.00\%, 
		fr\_loss: 0.22387/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.92604
	`Validation time elapsed: 9.56 seconds
`
-----------
Completed epoch 16.

EPOCH 17
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 2.62141/35.00\%, bp\_loss: 1.64681/55.00\%, hp\_loss: 3.07287/20.00\%, j\_loss: 1.84837/55.00\%, 
		fr\_loss: 0.40814/59.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.98312
	Part 2 - fp\_loss: 1.48823/60.00\%, bp\_loss: 1.50443/62.00\%, hp\_loss: 2.76385/28.00\%, j\_loss: 1.13908/69.00\%, 
		fr\_loss: 0.22105/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.38473
	Part 3 - fp\_loss: 2.04792/49.00\%, bp\_loss: 0.88865/77.00\%, hp\_loss: 2.49547/33.00\%, j\_loss: 0.98631/74.00\%, 
		fr\_loss: 0.34170/66.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.36720
	Part 4 - fp\_loss: 1.43750/62.00\%, bp\_loss: 1.42576/64.00\%, hp\_loss: 2.60663/32.00\%, j\_loss: 1.16303/68.00\%, 
		fr\_loss: 0.19820/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.28970
	Training time elapsed: 1.11 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 2.59170/37.00\%, bp\_loss: 1.76411/56.00\%, hp\_loss: 3.11269/22.00\%, j\_loss: 1.82879/58.00\%, 
		fr\_loss: 0.39391/61.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.98159
	Part 2 - fp\_loss: 1.33205/63.00\%, bp\_loss: 1.38285/65.00\%, hp\_loss: 2.70692/33.00\%, j\_loss: 1.07690/72.00\%, 
		fr\_loss: 0.22420/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.19406
	Part 3 - fp\_loss: 1.69223/60.00\%, bp\_loss: 0.97784/76.00\%, hp\_loss: 2.33444/40.00\%, j\_loss: 0.90596/76.00\%, 
		fr\_loss: 0.29319/70.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.03895
	Part 4 - fp\_loss: 1.42233/63.00\%, bp\_loss: 1.32480/69.00\%, hp\_loss: 2.62869/34.00\%, j\_loss: 0.86955/75.00\%, 
		fr\_loss: 0.18355/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.95032
	Training time elapsed: 37.59 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 2.41436/43.00\%, bp\_loss: 1.63387/57.00\%, hp\_loss: 3.10893/20.00\%, j\_loss: 1.57341/64.00\%, 
		fr\_loss: 0.36994/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.57336
	Part 2 - fp\_loss: 1.35146/63.00\%, bp\_loss: 1.42270/67.00\%, hp\_loss: 2.65832/32.00\%, j\_loss: 1.03433/73.00\%, 
		fr\_loss: 0.18864/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.12300
	Part 3 - fp\_loss: 1.79100/58.00\%, bp\_loss: 0.93823/76.00\%, hp\_loss: 2.45633/37.00\%, j\_loss: 0.88013/75.00\%, 
		fr\_loss: 0.30724/69.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.10124
	Part 4 - fp\_loss: 1.40348/62.00\%, bp\_loss: 1.18318/71.00\%, hp\_loss: 2.68203/31.00\%, j\_loss: 0.85555/77.00\%, 
		fr\_loss: 0.19744/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.91429
	Training time elapsed: 74.01 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 2.37828/41.00\%, bp\_loss: 1.58900/61.00\%, hp\_loss: 3.06287/22.00\%, j\_loss: 1.53933/63.00\%, 
		fr\_loss: 0.35658/64.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.48060
	Part 2 - fp\_loss: 1.31241/63.00\%, bp\_loss: 1.24443/69.00\%, hp\_loss: 2.70137/31.00\%, j\_loss: 0.97799/72.00\%, 
		fr\_loss: 0.21486/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.03280
	Part 3 - fp\_loss: 1.73032/59.00\%, bp\_loss: 1.09160/75.00\%, hp\_loss: 2.46164/37.00\%, j\_loss: 0.88420/76.00\%, 
		fr\_loss: 0.27443/72.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.08976
	Part 4 - fp\_loss: 1.39842/62.00\%, bp\_loss: 1.22846/71.00\%, hp\_loss: 2.65627/32.00\%, j\_loss: 0.85030/79.00\%, 
		fr\_loss: 0.18300/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.89793
	Training time elapsed: 110.44 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 2.64667/36.00\%, bp\_loss: 1.51400/60.00\%, hp\_loss: 3.07645/23.00\%, j\_loss: 1.73335/59.00\%, 
		fr\_loss: 0.43035/56.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.86418
	Part 2 - fp\_loss: 1.35606/63.00\%, bp\_loss: 1.32070/68.00\%, hp\_loss: 2.76559/28.00\%, j\_loss: 0.97635/76.00\%, 
		fr\_loss: 0.20140/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.08167
	Part 3 - fp\_loss: 1.77335/58.00\%, bp\_loss: 0.93024/76.00\%, hp\_loss: 2.52181/34.00\%, j\_loss: 0.86548/77.00\%, 
		fr\_loss: 0.26050/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.04827
	Part 4 - fp\_loss: 1.34834/63.00\%, bp\_loss: 1.14868/72.00\%, hp\_loss: 2.59751/31.00\%, j\_loss: 0.75451/81.00\%, 
		fr\_loss: 0.19722/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.74976
	Training time elapsed: 146.85 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 2.45701/37.00\%, bp\_loss: 1.79859/57.00\%, hp\_loss: 3.14965/19.00\%, j\_loss: 1.74087/57.00\%, 
		fr\_loss: 0.37500/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.82884
	Part 2 - fp\_loss: 1.39708/62.00\%, bp\_loss: 1.27788/66.00\%, hp\_loss: 2.73859/29.00\%, j\_loss: 1.04836/75.00\%, 
		fr\_loss: 0.20645/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.15830
	Part 3 - fp\_loss: 1.60971/62.00\%, bp\_loss: 0.97324/77.00\%, hp\_loss: 2.40700/37.00\%, j\_loss: 0.77777/80.00\%, 
		fr\_loss: 0.25540/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.85209
	Part 4 - fp\_loss: 1.37279/64.00\%, bp\_loss: 1.15900/72.00\%, hp\_loss: 2.56065/33.00\%, j\_loss: 0.74972/78.00\%, 
		fr\_loss: 0.19446/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.74647
	Training time elapsed: 183.33 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 2.42769/39.00\%, bp\_loss: 1.55305/60.00\%, hp\_loss: 3.03062/22.00\%, j\_loss: 1.57507/62.00\%, 
		fr\_loss: 0.38806/61.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.55207
	Part 2 - fp\_loss: 1.32854/63.00\%, bp\_loss: 1.16671/70.00\%, hp\_loss: 2.70687/31.00\%, j\_loss: 0.90656/76.00\%, 
		fr\_loss: 0.21952/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.95242
	Part 3 - fp\_loss: 1.62974/62.00\%, bp\_loss: 0.87882/79.00\%, hp\_loss: 2.36102/39.00\%, j\_loss: 0.74456/79.00\%, 
		fr\_loss: 0.26237/73.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.79375
	Part 4 - fp\_loss: 1.32265/65.00\%, bp\_loss: 1.08465/73.00\%, hp\_loss: 2.57644/33.00\%, j\_loss: 0.72507/80.00\%, 
		fr\_loss: 0.19948/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.68420
	Training time elapsed: 219.82 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 2.38748/41.00\%, bp\_loss: 1.62443/58.00\%, hp\_loss: 3.07331/22.00\%, j\_loss: 1.61295/60.00\%, 
		fr\_loss: 0.39272/60.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.60874
	Part 2 - fp\_loss: 1.32087/65.00\%, bp\_loss: 1.20855/70.00\%, hp\_loss: 2.70475/31.00\%, j\_loss: 0.95233/74.00\%, 
		fr\_loss: 0.21804/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.00479
	Part 3 - fp\_loss: 1.65198/63.00\%, bp\_loss: 0.96169/75.00\%, hp\_loss: 2.48267/36.00\%, j\_loss: 0.82656/78.00\%, 
		fr\_loss: 0.26761/73.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.95347
	Part 4 - fp\_loss: 1.40005/62.00\%, bp\_loss: 1.30477/70.00\%, hp\_loss: 2.63755/32.00\%, j\_loss: 0.82778/76.00\%, 
		fr\_loss: 0.20683/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.91733
	Training time elapsed: 256.26 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 2.37094/42.00\%, bp\_loss: 1.69395/60.00\%, hp\_loss: 3.13544/20.00\%, j\_loss: 1.55559/62.00\%, 
		fr\_loss: 0.37971/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.56958
	Part 2 - fp\_loss: 1.39556/63.00\%, bp\_loss: 1.30176/69.00\%, hp\_loss: 2.72759/31.00\%, j\_loss: 0.98105/75.00\%, 
		fr\_loss: 0.19503/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.08266
	Part 3 - fp\_loss: 1.64255/61.00\%, bp\_loss: 1.34016/67.00\%, hp\_loss: 2.40776/37.00\%, j\_loss: 1.08712/71.00\%, 
		fr\_loss: 0.23441/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.26717
	Part 4 - fp\_loss: 1.39966/61.00\%, bp\_loss: 1.25201/70.00\%, hp\_loss: 2.62834/33.00\%, j\_loss: 0.83122/79.00\%, 
		fr\_loss: 0.17688/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.87204
	Training time elapsed: 292.64 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 2.33744/42.00\%, bp\_loss: 1.81532/51.00\%, hp\_loss: 2.98457/29.00\%, j\_loss: 1.92386/52.00\%, 
		fr\_loss: 0.33973/66.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 9.40092
	Part 2 - fp\_loss: 1.44501/58.00\%, bp\_loss: 1.44087/61.00\%, hp\_loss: 2.65689/39.00\%, j\_loss: 1.25600/67.00\%, 
		fr\_loss: 0.25830/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 7.05708
	Part 3 - fp\_loss: 1.45327/63.00\%, bp\_loss: 1.24039/67.00\%, hp\_loss: 1.95254/51.00\%, j\_loss: 1.15848/69.00\%, 
		fr\_loss: 0.32340/67.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.12809
	Part 4 - fp\_loss: 1.19385/68.00\%, bp\_loss: 1.56369/60.00\%, hp\_loss: 2.19982/44.00\%, j\_loss: 1.09056/69.00\%, 
		fr\_loss: 0.23889/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.28682
	`Validation time elapsed: 0.75 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 2.18427/46.00\%, bp\_loss: 1.98642/49.00\%, hp\_loss: 3.09665/25.00\%, j\_loss: 1.87792/54.00\%, 
		fr\_loss: 0.37477/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 9.52003
	Part 2 - fp\_loss: 1.38022/60.00\%, bp\_loss: 1.59016/64.00\%, hp\_loss: 2.57756/41.00\%, j\_loss: 1.15686/70.00\%, 
		fr\_loss: 0.23815/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.94295
	Part 3 - fp\_loss: 1.45909/65.00\%, bp\_loss: 1.29641/68.00\%, hp\_loss: 1.97744/52.00\%, j\_loss: 1.05300/73.00\%, 
		fr\_loss: 0.29391/70.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.07986
	Part 4 - fp\_loss: 1.20931/67.00\%, bp\_loss: 1.36815/67.00\%, hp\_loss: 2.19832/45.00\%, j\_loss: 0.80678/79.00\%, 
		fr\_loss: 0.22498/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.80754
	`Validation time elapsed: 9.55 seconds
`
-----------
Completed epoch 17.

EPOCH 18
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 2.29179/45.00\%, bp\_loss: 1.53356/59.00\%, hp\_loss: 3.03044/23.00\%, j\_loss: 1.60373/63.00\%, 
		fr\_loss: 0.40113/59.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.51995
	Part 2 - fp\_loss: 1.33347/64.00\%, bp\_loss: 1.29284/64.00\%, hp\_loss: 2.75768/29.00\%, j\_loss: 1.11805/74.00\%, 
		fr\_loss: 0.20035/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.20029
	Part 3 - fp\_loss: 1.69585/60.00\%, bp\_loss: 0.96529/75.00\%, hp\_loss: 2.33628/38.00\%, j\_loss: 0.96383/74.00\%, 
		fr\_loss: 0.28105/72.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.08327
	Part 4 - fp\_loss: 1.35674/63.00\%, bp\_loss: 1.22289/68.00\%, hp\_loss: 2.49310/37.00\%, j\_loss: 0.96704/73.00\%, 
		fr\_loss: 0.19359/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.95381
	Training time elapsed: 1.10 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 2.28122/42.00\%, bp\_loss: 1.61613/60.00\%, hp\_loss: 3.12658/21.00\%, j\_loss: 1.59041/62.00\%, 
		fr\_loss: 0.37730/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.53113
	Part 2 - fp\_loss: 1.29801/64.00\%, bp\_loss: 1.27017/69.00\%, hp\_loss: 2.71967/32.00\%, j\_loss: 0.96843/75.00\%, 
		fr\_loss: 0.20857/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.02296
	Part 3 - fp\_loss: 1.61385/61.00\%, bp\_loss: 1.26738/73.00\%, hp\_loss: 2.40307/37.00\%, j\_loss: 0.90290/73.00\%, 
		fr\_loss: 0.25088/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.06183
	Part 4 - fp\_loss: 1.23261/67.00\%, bp\_loss: 1.26617/69.00\%, hp\_loss: 2.57597/33.00\%, j\_loss: 0.80807/76.00\%, 
		fr\_loss: 0.18707/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.76409
	Training time elapsed: 38.58 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 2.16185/45.00\%, bp\_loss: 1.47883/61.00\%, hp\_loss: 3.06108/22.00\%, j\_loss: 1.44207/64.00\%, 
		fr\_loss: 0.37696/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.26194
	Part 2 - fp\_loss: 1.29387/64.00\%, bp\_loss: 1.32697/67.00\%, hp\_loss: 2.75266/29.00\%, j\_loss: 0.94604/77.00\%, 
		fr\_loss: 0.20558/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.02244
	Part 3 - fp\_loss: 1.66869/59.00\%, bp\_loss: 1.04948/75.00\%, hp\_loss: 2.41386/37.00\%, j\_loss: 0.89912/77.00\%, 
		fr\_loss: 0.26451/73.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.03698
	Part 4 - fp\_loss: 1.26029/66.00\%, bp\_loss: 1.06206/74.00\%, hp\_loss: 2.60486/33.00\%, j\_loss: 0.72018/82.00\%, 
		fr\_loss: 0.19401/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.64441
	Training time elapsed: 75.98 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 2.36053/43.00\%, bp\_loss: 1.60297/58.00\%, hp\_loss: 3.10789/21.00\%, j\_loss: 1.61502/61.00\%, 
		fr\_loss: 0.38030/61.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.58884
	Part 2 - fp\_loss: 1.29545/65.00\%, bp\_loss: 1.17367/71.00\%, hp\_loss: 2.73499/31.00\%, j\_loss: 0.81762/80.00\%, 
		fr\_loss: 0.20591/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.84385
	Part 3 - fp\_loss: 1.61254/60.00\%, bp\_loss: 0.99821/76.00\%, hp\_loss: 2.36468/37.00\%, j\_loss: 0.86366/77.00\%, 
		fr\_loss: 0.25316/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.93196
	Part 4 - fp\_loss: 1.32728/64.00\%, bp\_loss: 1.15563/71.00\%, hp\_loss: 2.57110/34.00\%, j\_loss: 0.76540/81.00\%, 
		fr\_loss: 0.19661/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.74367
	Training time elapsed: 113.39 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 2.32058/42.00\%, bp\_loss: 1.50726/61.00\%, hp\_loss: 3.07958/22.00\%, j\_loss: 1.52709/62.00\%, 
		fr\_loss: 0.37795/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.44137
	Part 2 - fp\_loss: 1.28334/63.00\%, bp\_loss: 1.29672/67.00\%, hp\_loss: 2.74781/29.00\%, j\_loss: 0.95339/76.00\%, 
		fr\_loss: 0.19237/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.00079
	Part 3 - fp\_loss: 1.57841/61.00\%, bp\_loss: 0.87866/78.00\%, hp\_loss: 2.37318/38.00\%, j\_loss: 0.78142/78.00\%, 
		fr\_loss: 0.26645/73.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.81263
	Part 4 - fp\_loss: 1.26248/66.00\%, bp\_loss: 1.36848/69.00\%, hp\_loss: 2.63068/32.00\%, j\_loss: 0.79670/78.00\%, 
		fr\_loss: 0.18550/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.81319
	Training time elapsed: 150.79 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 2.24975/44.00\%, bp\_loss: 1.63620/57.00\%, hp\_loss: 3.06987/21.00\%, j\_loss: 1.61187/62.00\%, 
		fr\_loss: 0.38721/61.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.53578
	Part 2 - fp\_loss: 1.27792/64.00\%, bp\_loss: 1.27559/68.00\%, hp\_loss: 2.77009/30.00\%, j\_loss: 0.95371/77.00\%, 
		fr\_loss: 0.21300/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.01938
	Part 3 - fp\_loss: 1.65484/60.00\%, bp\_loss: 0.93620/76.00\%, hp\_loss: 2.41946/39.00\%, j\_loss: 0.80579/79.00\%, 
		fr\_loss: 0.25299/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.89290
	Part 4 - fp\_loss: 1.32247/65.00\%, bp\_loss: 1.15542/72.00\%, hp\_loss: 2.57392/35.00\%, j\_loss: 0.78409/80.00\%, 
		fr\_loss: 0.19825/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.76238
	Training time elapsed: 188.19 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 2.31650/42.00\%, bp\_loss: 1.62833/58.00\%, hp\_loss: 3.10203/21.00\%, j\_loss: 1.54177/62.00\%, 
		fr\_loss: 0.39730/60.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.51643
	Part 2 - fp\_loss: 1.31192/65.00\%, bp\_loss: 1.25888/68.00\%, hp\_loss: 2.72463/29.00\%, j\_loss: 0.99106/75.00\%, 
		fr\_loss: 0.23271/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.07478
	Part 3 - fp\_loss: 1.58764/62.00\%, bp\_loss: 0.87087/78.00\%, hp\_loss: 2.44993/35.00\%, j\_loss: 0.78719/78.00\%, 
		fr\_loss: 0.25353/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.83077
	Part 4 - fp\_loss: 1.35695/63.00\%, bp\_loss: 1.16603/70.00\%, hp\_loss: 2.70343/30.00\%, j\_loss: 0.80722/80.00\%, 
		fr\_loss: 0.18840/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.83492
	Training time elapsed: 225.59 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 2.19506/45.00\%, bp\_loss: 1.75442/54.00\%, hp\_loss: 3.09883/23.00\%, j\_loss: 1.65243/57.00\%, 
		fr\_loss: 0.38745/61.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.59338
	Part 2 - fp\_loss: 1.26793/64.00\%, bp\_loss: 1.24303/68.00\%, hp\_loss: 2.72418/30.00\%, j\_loss: 0.93466/77.00\%, 
		fr\_loss: 0.20759/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.96638
	Part 3 - fp\_loss: 1.67645/60.00\%, bp\_loss: 0.82864/79.00\%, hp\_loss: 2.40579/34.00\%, j\_loss: 0.83502/79.00\%, 
		fr\_loss: 0.27240/73.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.91597
	Part 4 - fp\_loss: 1.45882/61.00\%, bp\_loss: 1.07962/74.00\%, hp\_loss: 2.61020/33.00\%, j\_loss: 0.79229/79.00\%, 
		fr\_loss: 0.22125/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.84990
	Training time elapsed: 262.96 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 2.09877/46.00\%, bp\_loss: 1.58544/59.00\%, hp\_loss: 3.07546/21.00\%, j\_loss: 1.37498/66.00\%, 
		fr\_loss: 0.39040/61.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.21303
	Part 2 - fp\_loss: 1.30331/63.00\%, bp\_loss: 1.34107/66.00\%, hp\_loss: 2.72252/29.00\%, j\_loss: 1.01207/73.00\%, 
		fr\_loss: 0.20719/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.08999
	Part 3 - fp\_loss: 1.66251/60.00\%, bp\_loss: 0.83310/79.00\%, hp\_loss: 2.37684/37.00\%, j\_loss: 0.76668/78.00\%, 
		fr\_loss: 0.25189/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.81280
	Part 4 - fp\_loss: 1.29231/65.00\%, bp\_loss: 1.16884/71.00\%, hp\_loss: 2.57824/33.00\%, j\_loss: 0.78848/79.00\%, 
		fr\_loss: 0.18940/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.74816
	Training time elapsed: 300.30 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 2.29240/41.00\%, bp\_loss: 1.67411/56.00\%, hp\_loss: 3.02723/28.00\%, j\_loss: 1.82776/55.00\%, 
		fr\_loss: 0.36116/63.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 9.18267
	Part 2 - fp\_loss: 1.37547/61.00\%, bp\_loss: 1.45838/59.00\%, hp\_loss: 2.56812/42.00\%, j\_loss: 1.28338/63.00\%, 
		fr\_loss: 0.26185/73.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.94720
	Part 3 - fp\_loss: 1.47732/63.00\%, bp\_loss: 1.09517/72.00\%, hp\_loss: 1.92915/52.00\%, j\_loss: 0.99533/73.00\%, 
		fr\_loss: 0.29946/70.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.79643
	Part 4 - fp\_loss: 1.33724/65.00\%, bp\_loss: 1.66505/58.00\%, hp\_loss: 2.23189/46.00\%, j\_loss: 1.18135/66.00\%, 
		fr\_loss: 0.23805/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.65358
	`Validation time elapsed: 0.76 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 2.24403/44.00\%, bp\_loss: 2.09952/51.00\%, hp\_loss: 3.08403/26.00\%, j\_loss: 1.87522/55.00\%, 
		fr\_loss: 0.36634/63.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 9.66914
	Part 2 - fp\_loss: 1.32572/62.00\%, bp\_loss: 1.65777/61.00\%, hp\_loss: 2.53422/42.00\%, j\_loss: 1.11778/69.00\%, 
		fr\_loss: 0.23951/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.87499
	Part 3 - fp\_loss: 1.38801/68.00\%, bp\_loss: 1.12724/74.00\%, hp\_loss: 1.97594/51.00\%, j\_loss: 0.78563/79.00\%, 
		fr\_loss: 0.27353/73.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.55036
	Part 4 - fp\_loss: 1.21588/68.00\%, bp\_loss: 1.53713/66.00\%, hp\_loss: 2.23401/43.00\%, j\_loss: 0.86080/74.00\%, 
		fr\_loss: 0.22857/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.07639
	`Validation time elapsed: 9.55 seconds
`
-----------
Completed epoch 18.

EPOCH 19
-----------
Train iter 0/802:
	Part 1 - fp\_loss: 2.48404/38.00\%, bp\_loss: 1.53000/60.00\%, hp\_loss: 3.09943/20.00\%, j\_loss: 1.71097/59.00\%, 
		fr\_loss: 0.36991/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.71173
	Part 2 - fp\_loss: 1.41625/62.00\%, bp\_loss: 1.32924/61.00\%, hp\_loss: 2.76896/27.00\%, j\_loss: 1.24222/66.00\%, 
		fr\_loss: 0.19705/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.37686
	Part 3 - fp\_loss: 1.74781/58.00\%, bp\_loss: 0.77707/79.00\%, hp\_loss: 2.51160/34.00\%, j\_loss: 0.77097/79.00\%, 
		fr\_loss: 0.26003/74.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.89151
	Part 4 - fp\_loss: 1.44591/61.00\%, bp\_loss: 1.35216/65.00\%, hp\_loss: 2.61688/33.00\%, j\_loss: 1.00846/74.00\%, 
		fr\_loss: 0.19257/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.11469
	Training time elapsed: 1.08 seconds

Train iter 100/802:
	Part 1 - fp\_loss: 2.14945/45.00\%, bp\_loss: 1.51504/61.00\%, hp\_loss: 3.12206/21.00\%, j\_loss: 1.44651/66.00\%, 
		fr\_loss: 0.37111/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.28348
	Part 2 - fp\_loss: 1.33560/63.00\%, bp\_loss: 1.31888/67.00\%, hp\_loss: 2.71055/31.00\%, j\_loss: 0.99994/75.00\%, 
		fr\_loss: 0.21546/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.09204
	Part 3 - fp\_loss: 1.60965/60.00\%, bp\_loss: 0.92174/75.00\%, hp\_loss: 2.34958/38.00\%, j\_loss: 0.83922/75.00\%, 
		fr\_loss: 0.23707/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.86251
	Part 4 - fp\_loss: 1.27714/66.00\%, bp\_loss: 1.11593/72.00\%, hp\_loss: 2.56031/35.00\%, j\_loss: 0.74341/79.00\%, 
		fr\_loss: 0.18482/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.66968
	Training time elapsed: 38.51 seconds

Train iter 200/802:
	Part 1 - fp\_loss: 2.19909/45.00\%, bp\_loss: 1.56288/59.00\%, hp\_loss: 3.06700/21.00\%, j\_loss: 1.48055/64.00\%, 
		fr\_loss: 0.33797/66.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.30703
	Part 2 - fp\_loss: 1.24949/65.00\%, bp\_loss: 1.23050/70.00\%, hp\_loss: 2.71316/31.00\%, j\_loss: 0.88146/77.00\%, 
		fr\_loss: 0.19713/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.88643
	Part 3 - fp\_loss: 1.50194/63.00\%, bp\_loss: 0.86656/78.00\%, hp\_loss: 2.42404/38.00\%, j\_loss: 0.82571/79.00\%, 
		fr\_loss: 0.23271/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.79657
	Part 4 - fp\_loss: 1.27320/66.00\%, bp\_loss: 1.11367/72.00\%, hp\_loss: 2.64551/33.00\%, j\_loss: 0.78703/80.00\%, 
		fr\_loss: 0.20272/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.75410
	Training time elapsed: 75.90 seconds

Train iter 300/802:
	Part 1 - fp\_loss: 2.23095/44.00\%, bp\_loss: 1.59545/58.00\%, hp\_loss: 3.10083/22.00\%, j\_loss: 1.56945/61.00\%, 
		fr\_loss: 0.36310/63.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.45691
	Part 2 - fp\_loss: 1.27179/63.00\%, bp\_loss: 1.28187/68.00\%, hp\_loss: 2.65656/31.00\%, j\_loss: 0.93678/76.00\%, 
		fr\_loss: 0.20646/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.96066
	Part 3 - fp\_loss: 1.61006/59.00\%, bp\_loss: 0.78399/79.00\%, hp\_loss: 2.36985/39.00\%, j\_loss: 0.79153/78.00\%, 
		fr\_loss: 0.24961/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.79233
	Part 4 - fp\_loss: 1.26892/67.00\%, bp\_loss: 1.20001/69.00\%, hp\_loss: 2.58309/33.00\%, j\_loss: 0.86838/79.00\%, 
		fr\_loss: 0.18516/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.82293
	Training time elapsed: 113.27 seconds

Train iter 400/802:
	Part 1 - fp\_loss: 2.47641/36.00\%, bp\_loss: 1.57760/61.00\%, hp\_loss: 3.07747/21.00\%, j\_loss: 1.63965/61.00\%, 
		fr\_loss: 0.38040/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.65478
	Part 2 - fp\_loss: 1.27358/66.00\%, bp\_loss: 1.23582/69.00\%, hp\_loss: 2.66664/32.00\%, j\_loss: 0.86996/76.00\%, 
		fr\_loss: 0.19141/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.86889
	Part 3 - fp\_loss: 1.64918/60.00\%, bp\_loss: 1.07497/72.00\%, hp\_loss: 2.42362/37.00\%, j\_loss: 0.97782/73.00\%, 
		fr\_loss: 0.25104/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.10303
	Part 4 - fp\_loss: 1.27634/66.00\%, bp\_loss: 1.09467/72.00\%, hp\_loss: 2.61212/33.00\%, j\_loss: 0.72036/81.00\%, 
		fr\_loss: 0.17732/82.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.64789
	Training time elapsed: 150.66 seconds

Train iter 500/802:
	Part 1 - fp\_loss: 2.25911/42.00\%, bp\_loss: 1.50322/60.00\%, hp\_loss: 3.03760/24.00\%, j\_loss: 1.54600/61.00\%, 
		fr\_loss: 0.37483/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.41263
	Part 2 - fp\_loss: 1.26251/65.00\%, bp\_loss: 1.31876/66.00\%, hp\_loss: 2.70773/30.00\%, j\_loss: 0.94266/76.00\%, 
		fr\_loss: 0.18573/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.96760
	Part 3 - fp\_loss: 1.53425/62.00\%, bp\_loss: 1.00221/75.00\%, hp\_loss: 2.36202/39.00\%, j\_loss: 0.85584/76.00\%, 
		fr\_loss: 0.23534/76.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.86757
	Part 4 - fp\_loss: 1.34909/63.00\%, bp\_loss: 1.14081/71.00\%, hp\_loss: 2.58051/34.00\%, j\_loss: 0.86443/76.00\%, 
		fr\_loss: 0.19537/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.85074
	Training time elapsed: 188.03 seconds

Train iter 600/802:
	Part 1 - fp\_loss: 2.28683/44.00\%, bp\_loss: 1.50883/60.00\%, hp\_loss: 3.11158/19.00\%, j\_loss: 1.44424/65.00\%, 
		fr\_loss: 0.39033/61.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.36411
	Part 2 - fp\_loss: 1.26939/64.00\%, bp\_loss: 2.04705/59.00\%, hp\_loss: 2.65434/33.00\%, j\_loss: 1.18920/66.00\%, 
		fr\_loss: 0.21000/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.44431
	Part 3 - fp\_loss: 1.62098/61.00\%, bp\_loss: 0.88456/77.00\%, hp\_loss: 2.36422/38.00\%, j\_loss: 0.78234/79.00\%, 
		fr\_loss: 0.25097/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.81843
	Part 4 - fp\_loss: 1.35901/63.00\%, bp\_loss: 1.19032/73.00\%, hp\_loss: 2.59440/33.00\%, j\_loss: 0.73129/80.00\%, 
		fr\_loss: 0.18146/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.72768
	Training time elapsed: 225.42 seconds

Train iter 700/802:
	Part 1 - fp\_loss: 2.16068/46.00\%, bp\_loss: 2.00310/54.00\%, hp\_loss: 3.00899/24.00\%, j\_loss: 1.63615/59.00\%, 
		fr\_loss: 0.37624/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.59636
	Part 2 - fp\_loss: 1.37864/61.00\%, bp\_loss: 1.28788/68.00\%, hp\_loss: 2.74365/31.00\%, j\_loss: 1.00605/73.00\%, 
		fr\_loss: 0.19397/80.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.09879
	Part 3 - fp\_loss: 1.63715/61.00\%, bp\_loss: 0.98357/73.00\%, hp\_loss: 2.50347/35.00\%, j\_loss: 0.90779/76.00\%, 
		fr\_loss: 0.24072/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.01320
	Part 4 - fp\_loss: 1.29206/64.00\%, bp\_loss: 1.09513/73.00\%, hp\_loss: 2.64718/30.00\%, j\_loss: 0.72957/79.00\%, 
		fr\_loss: 0.18767/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.68597
	Training time elapsed: 262.83 seconds

Train iter 800/802:
	Part 1 - fp\_loss: 2.38204/41.00\%, bp\_loss: 1.66791/57.00\%, hp\_loss: 3.10858/21.00\%, j\_loss: 1.65285/59.00\%, 
		fr\_loss: 0.36362/63.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 4.64043
	Part 2 - fp\_loss: 1.33654/63.00\%, bp\_loss: 1.24873/69.00\%, hp\_loss: 2.73049/30.00\%, j\_loss: 0.97623/75.00\%, 
		fr\_loss: 0.20572/79.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 3.04398
	Part 3 - fp\_loss: 1.49185/62.00\%, bp\_loss: 0.90226/77.00\%, hp\_loss: 2.42143/37.00\%, j\_loss: 0.75883/79.00\%, 
		fr\_loss: 0.24984/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.75169
	Part 4 - fp\_loss: 1.33483/63.00\%, bp\_loss: 1.03015/74.00\%, hp\_loss: 2.63680/32.00\%, j\_loss: 0.74148/81.00\%, 
		fr\_loss: 0.18727/81.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 2.69624
	Training time elapsed: 300.20 seconds

Valid iter 0/160:
	Part 1 - fp\_loss: 2.44609/37.00\%, bp\_loss: 1.75767/52.00\%, hp\_loss: 3.05166/27.00\%, j\_loss: 1.96276/51.00\%, 
		fr\_loss: 0.36116/64.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 9.57934
	Part 2 - fp\_loss: 1.30756/61.00\%, bp\_loss: 1.44198/62.00\%, hp\_loss: 2.54366/42.00\%, j\_loss: 1.22799/67.00\%, 
		fr\_loss: 0.24043/75.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.76161
	Part 3 - fp\_loss: 1.33556/68.00\%, bp\_loss: 1.07539/70.00\%, hp\_loss: 1.91383/52.00\%, j\_loss: 0.96420/73.00\%, 
		fr\_loss: 0.27103/73.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.56002
	Part 4 - fp\_loss: 1.17342/68.00\%, bp\_loss: 1.49325/60.00\%, hp\_loss: 2.20160/44.00\%, j\_loss: 1.09823/71.00\%, 
		fr\_loss: 0.22159/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.18810
	`Validation time elapsed: 0.77 seconds
`
Valid iter 100/160:
	Part 1 - fp\_loss: 2.25751/44.00\%, bp\_loss: 2.05885/52.00\%, hp\_loss: 3.02399/28.00\%, j\_loss: 1.80986/51.00\%, 
		fr\_loss: 0.36989/62.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 9.52010
	Part 2 - fp\_loss: 1.40102/61.00\%, bp\_loss: 1.56217/62.00\%, hp\_loss: 2.56231/41.00\%, j\_loss: 1.17272/70.00\%, 
		fr\_loss: 0.23341/77.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 6.93164
	Part 3 - fp\_loss: 1.37041/67.00\%, bp\_loss: 1.19071/71.00\%, hp\_loss: 1.96478/50.00\%, j\_loss: 0.83044/75.00\%, 
		fr\_loss: 0.26706/72.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.62341
	Part 4 - fp\_loss: 1.08928/70.00\%, bp\_loss: 1.36720/69.00\%, hp\_loss: 2.26374/43.00\%, j\_loss: 0.70654/80.00\%, 
		fr\_loss: 0.21932/78.00\%, p\_loss: 0.00000/100.00\%, 
		total weighted loss: 5.64607
	`Validation time elapsed: 9.54 seconds
`
-----------
Completed epoch 19.

Training completed! Saving files.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{} LOAD SAVED STATS}
         
         \PY{n}{stats\PYZus{}file\PYZus{}path} \PY{o}{=} \PY{n}{OUTPUT\PYZus{}PATH} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/12\PYZhy{}09\PYZus{}21\PYZhy{}52\PYZhy{}24\PYZus{}C1F0E8.stat}\PY{l+s+s2}{\PYZdq{}}
         
         \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{stats\PYZus{}file\PYZus{}path}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rb}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{k}{as} \PY{n}{file}\PY{p}{:}
             \PY{n}{saved\PYZus{}stats} \PY{o}{=} \PY{n}{pickle}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{file}\PY{p}{)}
             
         \PY{n}{train\PYZus{}stats}\PY{p}{,} \PY{n}{val\PYZus{}stats} \PY{o}{=} \PY{n}{saved\PYZus{}stats}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{c+c1}{\PYZsh{} GRAPH WEIGHTED LOSSES}
         
         \PY{n}{val\PYZus{}weighted\PYZus{}losses} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[} \PY{n}{S}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{losses}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{S} \PY{o+ow}{in} \PY{n}{val\PYZus{}stats}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{111}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{val\PYZus{}weighted\PYZus{}losses}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{val\PYZus{}weighted\PYZus{}losses}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Weighted Validation Loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{weighted valid loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{upper left}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_8_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n}{val\PYZus{}judge\PYZus{}losses} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[} \PY{n}{S}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{j\PYZus{}losses}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{S} \PY{o+ow}{in} \PY{n}{val\PYZus{}stats}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{val\PYZus{}judge\PYZus{}accs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[} \PY{n}{S}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{j\PYZus{}accs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{S} \PY{o+ow}{in} \PY{n}{val\PYZus{}stats}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{val\PYZus{}judge\PYZus{}losses}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{val\PYZus{}judge\PYZus{}losses}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Judge Model Losses}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Losses}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{val\PYZus{}judge\PYZus{}accs}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{yticks}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{55}\PY{p}{,} \PY{l+m+mi}{75}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{int8}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Judge Model Accuracies}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{ Accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_9_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_9_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} SAMPLING}
        
        \PY{k}{if} \PY{o+ow}{not} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{models}\PY{l+s+s2}{\PYZdq{}} \PY{o+ow}{in} \PY{n+nb}{vars}\PY{p}{(}\PY{p}{)} \PY{o+ow}{or} \PY{n}{models} \PY{o+ow}{is} \PY{k+kc}{None} \PY{o+ow}{or} \PY{n+nb}{len}\PY{p}{(}\PY{n}{models}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{l+m+mi}{2}\PY{p}{:}
            \PY{n}{mkdir}\PY{p}{(}\PY{n}{OUTPUT\PYZus{}PATH}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} file location, update to point to the correct file}
            \PY{n}{MODELS\PYZus{}FILE\PYZus{}PATH} \PY{o}{=} \PY{n}{OUTPUT\PYZus{}PATH} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/12\PYZhy{}09\PYZus{}21\PYZhy{}52\PYZhy{}24\PYZus{}7D4325.models}\PY{l+s+s2}{\PYZdq{}}
            \PY{c+c1}{\PYZsh{} which epoch\PYZsq{}s models to use}
            \PY{n}{NTH\PYZus{}EPOCH} \PY{o}{=} \PY{l+m+mi}{12}
            
            \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{MODELS\PYZus{}FILE\PYZus{}PATH}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rb}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{k}{as} \PY{n}{file}\PY{p}{:}
                \PY{n}{saved\PYZus{}models} \PY{o}{=} \PY{n}{pickle}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{file}\PY{p}{)}
                \PY{n}{models} \PY{o}{=} \PY{n}{saved\PYZus{}models}\PY{p}{[}\PY{n}{NTH\PYZus{}EPOCH}\PY{p}{]}
                
        \PY{c+c1}{\PYZsh{} how many ticks to sample, 16 ticks \PYZti{} 1 measure of music}
        \PY{n}{NUM\PYZus{}TICKS\PYZus{}TO\PYZus{}SAMPLE} \PY{o}{=} \PY{l+m+mi}{512}
        \PY{c+c1}{\PYZsh{} number of iterations to repeat the sampling process, one iteration}
        \PY{c+c1}{\PYZsh{} will run for NUM\PYZus{}PARTS * NUM\PYZus{}TICKS\PYZus{}TO\PYZus{}SAMPLE times.}
        \PY{n}{NUM\PYZus{}REPEATS} \PY{o}{=} \PY{l+m+mi}{1000}
                
        \PY{n}{output} \PY{o}{=} \PY{n}{sample}\PY{p}{(}\PY{n}{models}\PY{p}{,} 
                        \PY{n}{num\PYZus{}parts}\PY{o}{=}\PY{n}{NUM\PYZus{}PARTS}\PY{p}{,}
                        \PY{n}{num\PYZus{}ticks}\PY{o}{=}\PY{n}{NUM\PYZus{}TICKS\PYZus{}TO\PYZus{}SAMPLE}\PY{p}{,}
                        \PY{n}{num\PYZus{}dims}\PY{o}{=}\PY{n}{PITCH\PYZus{}VOCAB\PYZus{}SIZE}\PY{p}{,}
                        \PY{n}{seq\PYZus{}len}\PY{o}{=}\PY{n}{SEQ\PYZus{}LEN}\PY{p}{,}
                        \PY{n}{num\PYZus{}repeats}\PY{o}{=}\PY{n}{NUM\PYZus{}REPEATS}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/ubuntu/cs682-project/util/models.py:103: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten\_parameters().
  lstm, self.hidden = self.lstm(X, self.hidden)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
current iter: 0/2048000
current iter: 1000/2048000
current iter: 2000/2048000
current iter: 3000/2048000
current iter: 4000/2048000
current iter: 5000/2048000
current iter: 6000/2048000
current iter: 7000/2048000
current iter: 8000/2048000
current iter: 9000/2048000
current iter: 10000/2048000
current iter: 11000/2048000
current iter: 12000/2048000
current iter: 13000/2048000
current iter: 14000/2048000
current iter: 15000/2048000
current iter: 16000/2048000
current iter: 17000/2048000
current iter: 18000/2048000
current iter: 19000/2048000
current iter: 20000/2048000
current iter: 21000/2048000
current iter: 22000/2048000
current iter: 23000/2048000
current iter: 24000/2048000
current iter: 25000/2048000
current iter: 26000/2048000
current iter: 27000/2048000
current iter: 28000/2048000
current iter: 29000/2048000
current iter: 30000/2048000
current iter: 31000/2048000
current iter: 32000/2048000
current iter: 33000/2048000
current iter: 34000/2048000
current iter: 35000/2048000
current iter: 36000/2048000
current iter: 37000/2048000
current iter: 38000/2048000
current iter: 39000/2048000
current iter: 40000/2048000
current iter: 41000/2048000
current iter: 42000/2048000
current iter: 43000/2048000
current iter: 44000/2048000
current iter: 45000/2048000
current iter: 46000/2048000
current iter: 47000/2048000
current iter: 48000/2048000
current iter: 49000/2048000
current iter: 50000/2048000
current iter: 51000/2048000
current iter: 52000/2048000
current iter: 53000/2048000
current iter: 54000/2048000
current iter: 55000/2048000
current iter: 56000/2048000
current iter: 57000/2048000
current iter: 58000/2048000
current iter: 59000/2048000
current iter: 60000/2048000
current iter: 61000/2048000
current iter: 62000/2048000
current iter: 63000/2048000
current iter: 64000/2048000
current iter: 65000/2048000
current iter: 66000/2048000
current iter: 67000/2048000
current iter: 68000/2048000
current iter: 69000/2048000
current iter: 70000/2048000
current iter: 71000/2048000
current iter: 72000/2048000
current iter: 73000/2048000
current iter: 74000/2048000
current iter: 75000/2048000
current iter: 76000/2048000
current iter: 77000/2048000
current iter: 78000/2048000
current iter: 79000/2048000
current iter: 80000/2048000
current iter: 81000/2048000
current iter: 82000/2048000
current iter: 83000/2048000
current iter: 84000/2048000
current iter: 85000/2048000
current iter: 86000/2048000
current iter: 87000/2048000
current iter: 88000/2048000
current iter: 89000/2048000
current iter: 90000/2048000
current iter: 91000/2048000
current iter: 92000/2048000
current iter: 93000/2048000
current iter: 94000/2048000
current iter: 95000/2048000
current iter: 96000/2048000
current iter: 97000/2048000
current iter: 98000/2048000
current iter: 99000/2048000
current iter: 100000/2048000
current iter: 101000/2048000
current iter: 102000/2048000
current iter: 103000/2048000
current iter: 104000/2048000
current iter: 105000/2048000
current iter: 106000/2048000
current iter: 107000/2048000
current iter: 108000/2048000
current iter: 109000/2048000
current iter: 110000/2048000
current iter: 111000/2048000
current iter: 112000/2048000
current iter: 113000/2048000
current iter: 114000/2048000
current iter: 115000/2048000
current iter: 116000/2048000
current iter: 117000/2048000
current iter: 118000/2048000
current iter: 119000/2048000
current iter: 120000/2048000
current iter: 121000/2048000
current iter: 122000/2048000
current iter: 123000/2048000
current iter: 124000/2048000
current iter: 125000/2048000
current iter: 126000/2048000
current iter: 127000/2048000
current iter: 128000/2048000
current iter: 129000/2048000
current iter: 130000/2048000
current iter: 131000/2048000
current iter: 132000/2048000
current iter: 133000/2048000
current iter: 134000/2048000
current iter: 135000/2048000
current iter: 136000/2048000
current iter: 137000/2048000
current iter: 138000/2048000
current iter: 139000/2048000
current iter: 140000/2048000
current iter: 141000/2048000
current iter: 142000/2048000
current iter: 143000/2048000
current iter: 144000/2048000
current iter: 145000/2048000
current iter: 146000/2048000
current iter: 147000/2048000
current iter: 148000/2048000
current iter: 149000/2048000
current iter: 150000/2048000
current iter: 151000/2048000
current iter: 152000/2048000
current iter: 153000/2048000
current iter: 154000/2048000
current iter: 155000/2048000
current iter: 156000/2048000
current iter: 157000/2048000
current iter: 158000/2048000
current iter: 159000/2048000
current iter: 160000/2048000
current iter: 161000/2048000
current iter: 162000/2048000
current iter: 163000/2048000
current iter: 164000/2048000
current iter: 165000/2048000
current iter: 166000/2048000
current iter: 167000/2048000
current iter: 168000/2048000
current iter: 169000/2048000
current iter: 170000/2048000
current iter: 171000/2048000
current iter: 172000/2048000
current iter: 173000/2048000
current iter: 174000/2048000
current iter: 175000/2048000
current iter: 176000/2048000
current iter: 177000/2048000
current iter: 178000/2048000
current iter: 179000/2048000
current iter: 180000/2048000
current iter: 181000/2048000
current iter: 182000/2048000
current iter: 183000/2048000
current iter: 184000/2048000
current iter: 185000/2048000
current iter: 186000/2048000
current iter: 187000/2048000
current iter: 188000/2048000
current iter: 189000/2048000
current iter: 190000/2048000
current iter: 191000/2048000
current iter: 192000/2048000
current iter: 193000/2048000
current iter: 194000/2048000
current iter: 195000/2048000
current iter: 196000/2048000
current iter: 197000/2048000
current iter: 198000/2048000
current iter: 199000/2048000
current iter: 200000/2048000
current iter: 201000/2048000
current iter: 202000/2048000
current iter: 203000/2048000
current iter: 204000/2048000
current iter: 205000/2048000
current iter: 206000/2048000
current iter: 207000/2048000
current iter: 208000/2048000
current iter: 209000/2048000
current iter: 210000/2048000
current iter: 211000/2048000
current iter: 212000/2048000
current iter: 213000/2048000
current iter: 214000/2048000
current iter: 215000/2048000
current iter: 216000/2048000
current iter: 217000/2048000
current iter: 218000/2048000
current iter: 219000/2048000
current iter: 220000/2048000
current iter: 221000/2048000
current iter: 222000/2048000
current iter: 223000/2048000
current iter: 224000/2048000
current iter: 225000/2048000
current iter: 226000/2048000
current iter: 227000/2048000
current iter: 228000/2048000
current iter: 229000/2048000
current iter: 230000/2048000
current iter: 231000/2048000
current iter: 232000/2048000
current iter: 233000/2048000
current iter: 234000/2048000
current iter: 235000/2048000
current iter: 236000/2048000
current iter: 237000/2048000
current iter: 238000/2048000
current iter: 239000/2048000
current iter: 240000/2048000
current iter: 241000/2048000
current iter: 242000/2048000
current iter: 243000/2048000
current iter: 244000/2048000
current iter: 245000/2048000
current iter: 246000/2048000
current iter: 247000/2048000
current iter: 248000/2048000
current iter: 249000/2048000
current iter: 250000/2048000
current iter: 251000/2048000
current iter: 252000/2048000
current iter: 253000/2048000
current iter: 254000/2048000
current iter: 255000/2048000
current iter: 256000/2048000
current iter: 257000/2048000
current iter: 258000/2048000
current iter: 259000/2048000
current iter: 260000/2048000
current iter: 261000/2048000
current iter: 262000/2048000
current iter: 263000/2048000
current iter: 264000/2048000
current iter: 265000/2048000
current iter: 266000/2048000
current iter: 267000/2048000
current iter: 268000/2048000
current iter: 269000/2048000
current iter: 270000/2048000
current iter: 271000/2048000
current iter: 272000/2048000
current iter: 273000/2048000
current iter: 274000/2048000
current iter: 275000/2048000
current iter: 276000/2048000
current iter: 277000/2048000
current iter: 278000/2048000
current iter: 279000/2048000
current iter: 280000/2048000
current iter: 281000/2048000
current iter: 282000/2048000
current iter: 283000/2048000
current iter: 284000/2048000
current iter: 285000/2048000
current iter: 286000/2048000
current iter: 287000/2048000
current iter: 288000/2048000
current iter: 289000/2048000
current iter: 290000/2048000
current iter: 291000/2048000
current iter: 292000/2048000
current iter: 293000/2048000
current iter: 294000/2048000
current iter: 295000/2048000
current iter: 296000/2048000
current iter: 297000/2048000
current iter: 298000/2048000
current iter: 299000/2048000
current iter: 300000/2048000
current iter: 301000/2048000
current iter: 302000/2048000
current iter: 303000/2048000
current iter: 304000/2048000
current iter: 305000/2048000
current iter: 306000/2048000
current iter: 307000/2048000
current iter: 308000/2048000
current iter: 309000/2048000
current iter: 310000/2048000
current iter: 311000/2048000
current iter: 312000/2048000
current iter: 313000/2048000
current iter: 314000/2048000
current iter: 315000/2048000
current iter: 316000/2048000
current iter: 317000/2048000
current iter: 318000/2048000
current iter: 319000/2048000
current iter: 320000/2048000
current iter: 321000/2048000
current iter: 322000/2048000
current iter: 323000/2048000
current iter: 324000/2048000
current iter: 325000/2048000
current iter: 326000/2048000
current iter: 327000/2048000
current iter: 328000/2048000
current iter: 329000/2048000
current iter: 330000/2048000
current iter: 331000/2048000
current iter: 332000/2048000
current iter: 333000/2048000
current iter: 334000/2048000
current iter: 335000/2048000
current iter: 336000/2048000
current iter: 337000/2048000
current iter: 338000/2048000
current iter: 339000/2048000
current iter: 340000/2048000
current iter: 341000/2048000
current iter: 342000/2048000
current iter: 343000/2048000
current iter: 344000/2048000
current iter: 345000/2048000
current iter: 346000/2048000
current iter: 347000/2048000
current iter: 348000/2048000
current iter: 349000/2048000
current iter: 350000/2048000
current iter: 351000/2048000
current iter: 352000/2048000
current iter: 353000/2048000
current iter: 354000/2048000
current iter: 355000/2048000
current iter: 356000/2048000
current iter: 357000/2048000
current iter: 358000/2048000
current iter: 359000/2048000
current iter: 360000/2048000
current iter: 361000/2048000
current iter: 362000/2048000
current iter: 363000/2048000
current iter: 364000/2048000
current iter: 365000/2048000
current iter: 366000/2048000
current iter: 367000/2048000
current iter: 368000/2048000
current iter: 369000/2048000
current iter: 370000/2048000
current iter: 371000/2048000
current iter: 372000/2048000
current iter: 373000/2048000
current iter: 374000/2048000
current iter: 375000/2048000
current iter: 376000/2048000
current iter: 377000/2048000
current iter: 378000/2048000
current iter: 379000/2048000
current iter: 380000/2048000
current iter: 381000/2048000
current iter: 382000/2048000
current iter: 383000/2048000
current iter: 384000/2048000
current iter: 385000/2048000
current iter: 386000/2048000
current iter: 387000/2048000
current iter: 388000/2048000
current iter: 389000/2048000
current iter: 390000/2048000
current iter: 391000/2048000
current iter: 392000/2048000
current iter: 393000/2048000
current iter: 394000/2048000
current iter: 395000/2048000
current iter: 396000/2048000
current iter: 397000/2048000
current iter: 398000/2048000
current iter: 399000/2048000
current iter: 400000/2048000
current iter: 401000/2048000
current iter: 402000/2048000
current iter: 403000/2048000
current iter: 404000/2048000
current iter: 405000/2048000
current iter: 406000/2048000
current iter: 407000/2048000
current iter: 408000/2048000
current iter: 409000/2048000
current iter: 410000/2048000
current iter: 411000/2048000
current iter: 412000/2048000
current iter: 413000/2048000
current iter: 414000/2048000
current iter: 415000/2048000
current iter: 416000/2048000
current iter: 417000/2048000
current iter: 418000/2048000
current iter: 419000/2048000
current iter: 420000/2048000
current iter: 421000/2048000
current iter: 422000/2048000
current iter: 423000/2048000
current iter: 424000/2048000
current iter: 425000/2048000
current iter: 426000/2048000
current iter: 427000/2048000
current iter: 428000/2048000
current iter: 429000/2048000
current iter: 430000/2048000
current iter: 431000/2048000
current iter: 432000/2048000
current iter: 433000/2048000
current iter: 434000/2048000
current iter: 435000/2048000
current iter: 436000/2048000
current iter: 437000/2048000
current iter: 438000/2048000
current iter: 439000/2048000
current iter: 440000/2048000
current iter: 441000/2048000
current iter: 442000/2048000
current iter: 443000/2048000
current iter: 444000/2048000
current iter: 445000/2048000
current iter: 446000/2048000
current iter: 447000/2048000
current iter: 448000/2048000
current iter: 449000/2048000
current iter: 450000/2048000
current iter: 451000/2048000
current iter: 452000/2048000
current iter: 453000/2048000
current iter: 454000/2048000
current iter: 455000/2048000
current iter: 456000/2048000
current iter: 457000/2048000
current iter: 458000/2048000
current iter: 459000/2048000
current iter: 460000/2048000
current iter: 461000/2048000
current iter: 462000/2048000
current iter: 463000/2048000
current iter: 464000/2048000
current iter: 465000/2048000
current iter: 466000/2048000
current iter: 467000/2048000
current iter: 468000/2048000
current iter: 469000/2048000
current iter: 470000/2048000
current iter: 471000/2048000
current iter: 472000/2048000
current iter: 473000/2048000
current iter: 474000/2048000
current iter: 475000/2048000
current iter: 476000/2048000
current iter: 477000/2048000
current iter: 478000/2048000
current iter: 479000/2048000
current iter: 480000/2048000
current iter: 481000/2048000
current iter: 482000/2048000
current iter: 483000/2048000
current iter: 484000/2048000
current iter: 485000/2048000
current iter: 486000/2048000
current iter: 487000/2048000
current iter: 488000/2048000
current iter: 489000/2048000
current iter: 490000/2048000
current iter: 491000/2048000
current iter: 492000/2048000
current iter: 493000/2048000
current iter: 494000/2048000
current iter: 495000/2048000
current iter: 496000/2048000
current iter: 497000/2048000
current iter: 498000/2048000
current iter: 499000/2048000
current iter: 500000/2048000
current iter: 501000/2048000
current iter: 502000/2048000
current iter: 503000/2048000
current iter: 504000/2048000
current iter: 505000/2048000
current iter: 506000/2048000
current iter: 507000/2048000
current iter: 508000/2048000
current iter: 509000/2048000
current iter: 510000/2048000
current iter: 511000/2048000
current iter: 512000/2048000
current iter: 513000/2048000
current iter: 514000/2048000
current iter: 515000/2048000
current iter: 516000/2048000
current iter: 517000/2048000
current iter: 518000/2048000
current iter: 519000/2048000
current iter: 520000/2048000
current iter: 521000/2048000
current iter: 522000/2048000
current iter: 523000/2048000
current iter: 524000/2048000
current iter: 525000/2048000
current iter: 526000/2048000
current iter: 527000/2048000
current iter: 528000/2048000
current iter: 529000/2048000
current iter: 530000/2048000
current iter: 531000/2048000
current iter: 532000/2048000
current iter: 533000/2048000
current iter: 534000/2048000
current iter: 535000/2048000
current iter: 536000/2048000
current iter: 537000/2048000
current iter: 538000/2048000
current iter: 539000/2048000
current iter: 540000/2048000
current iter: 541000/2048000
current iter: 542000/2048000
current iter: 543000/2048000
current iter: 544000/2048000
current iter: 545000/2048000
current iter: 546000/2048000
current iter: 547000/2048000
current iter: 548000/2048000
current iter: 549000/2048000
current iter: 550000/2048000
current iter: 551000/2048000
current iter: 552000/2048000
current iter: 553000/2048000
current iter: 554000/2048000
current iter: 555000/2048000
current iter: 556000/2048000
current iter: 557000/2048000
current iter: 558000/2048000
current iter: 559000/2048000
current iter: 560000/2048000
current iter: 561000/2048000
current iter: 562000/2048000
current iter: 563000/2048000
current iter: 564000/2048000
current iter: 565000/2048000
current iter: 566000/2048000
current iter: 567000/2048000
current iter: 568000/2048000
current iter: 569000/2048000
current iter: 570000/2048000
current iter: 571000/2048000
current iter: 572000/2048000
current iter: 573000/2048000
current iter: 574000/2048000
current iter: 575000/2048000
current iter: 576000/2048000
current iter: 577000/2048000
current iter: 578000/2048000
current iter: 579000/2048000
current iter: 580000/2048000
current iter: 581000/2048000
current iter: 582000/2048000
current iter: 583000/2048000
current iter: 584000/2048000
current iter: 585000/2048000
current iter: 586000/2048000
current iter: 587000/2048000
current iter: 588000/2048000
current iter: 589000/2048000
current iter: 590000/2048000
current iter: 591000/2048000
current iter: 592000/2048000
current iter: 593000/2048000
current iter: 594000/2048000
current iter: 595000/2048000
current iter: 596000/2048000
current iter: 597000/2048000
current iter: 598000/2048000
current iter: 599000/2048000
current iter: 600000/2048000
current iter: 601000/2048000
current iter: 602000/2048000
current iter: 603000/2048000
current iter: 604000/2048000
current iter: 605000/2048000
current iter: 606000/2048000
current iter: 607000/2048000
current iter: 608000/2048000
current iter: 609000/2048000
current iter: 610000/2048000
current iter: 611000/2048000
current iter: 612000/2048000
current iter: 613000/2048000
current iter: 614000/2048000
current iter: 615000/2048000
current iter: 616000/2048000
current iter: 617000/2048000
current iter: 618000/2048000
current iter: 619000/2048000
current iter: 620000/2048000
current iter: 621000/2048000
current iter: 622000/2048000
current iter: 623000/2048000
current iter: 624000/2048000
current iter: 625000/2048000
current iter: 626000/2048000
current iter: 627000/2048000
current iter: 628000/2048000
current iter: 629000/2048000
current iter: 630000/2048000
current iter: 631000/2048000
current iter: 632000/2048000
current iter: 633000/2048000
current iter: 634000/2048000
current iter: 635000/2048000
current iter: 636000/2048000
current iter: 637000/2048000
current iter: 638000/2048000
current iter: 639000/2048000
current iter: 640000/2048000
current iter: 641000/2048000
current iter: 642000/2048000
current iter: 643000/2048000
current iter: 644000/2048000
current iter: 645000/2048000
current iter: 646000/2048000
current iter: 647000/2048000
current iter: 648000/2048000
current iter: 649000/2048000
current iter: 650000/2048000
current iter: 651000/2048000
current iter: 652000/2048000
current iter: 653000/2048000
current iter: 654000/2048000
current iter: 655000/2048000
current iter: 656000/2048000
current iter: 657000/2048000
current iter: 658000/2048000
current iter: 659000/2048000
current iter: 660000/2048000
current iter: 661000/2048000
current iter: 662000/2048000
current iter: 663000/2048000
current iter: 664000/2048000
current iter: 665000/2048000
current iter: 666000/2048000
current iter: 667000/2048000
current iter: 668000/2048000
current iter: 669000/2048000
current iter: 670000/2048000
current iter: 671000/2048000
current iter: 672000/2048000
current iter: 673000/2048000
current iter: 674000/2048000
current iter: 675000/2048000
current iter: 676000/2048000
current iter: 677000/2048000
current iter: 678000/2048000
current iter: 679000/2048000
current iter: 680000/2048000
current iter: 681000/2048000
current iter: 682000/2048000
current iter: 683000/2048000
current iter: 684000/2048000
current iter: 685000/2048000
current iter: 686000/2048000
current iter: 687000/2048000
current iter: 688000/2048000
current iter: 689000/2048000
current iter: 690000/2048000
current iter: 691000/2048000
current iter: 692000/2048000
current iter: 693000/2048000
current iter: 694000/2048000
current iter: 695000/2048000
current iter: 696000/2048000
current iter: 697000/2048000
current iter: 698000/2048000
current iter: 699000/2048000
current iter: 700000/2048000
current iter: 701000/2048000
current iter: 702000/2048000
current iter: 703000/2048000
current iter: 704000/2048000
current iter: 705000/2048000
current iter: 706000/2048000
current iter: 707000/2048000
current iter: 708000/2048000
current iter: 709000/2048000
current iter: 710000/2048000
current iter: 711000/2048000
current iter: 712000/2048000
current iter: 713000/2048000
current iter: 714000/2048000
current iter: 715000/2048000
current iter: 716000/2048000
current iter: 717000/2048000
current iter: 718000/2048000
current iter: 719000/2048000
current iter: 720000/2048000
current iter: 721000/2048000
current iter: 722000/2048000
current iter: 723000/2048000
current iter: 724000/2048000
current iter: 725000/2048000
current iter: 726000/2048000
current iter: 727000/2048000
current iter: 728000/2048000
current iter: 729000/2048000
current iter: 730000/2048000
current iter: 731000/2048000
current iter: 732000/2048000
current iter: 733000/2048000
current iter: 734000/2048000
current iter: 735000/2048000
current iter: 736000/2048000
current iter: 737000/2048000
current iter: 738000/2048000
current iter: 739000/2048000
current iter: 740000/2048000
current iter: 741000/2048000
current iter: 742000/2048000
current iter: 743000/2048000
current iter: 744000/2048000
current iter: 745000/2048000
current iter: 746000/2048000
current iter: 747000/2048000
current iter: 748000/2048000
current iter: 749000/2048000
current iter: 750000/2048000
current iter: 751000/2048000
current iter: 752000/2048000
current iter: 753000/2048000
current iter: 754000/2048000
current iter: 755000/2048000
current iter: 756000/2048000
current iter: 757000/2048000
current iter: 758000/2048000
current iter: 759000/2048000
current iter: 760000/2048000
current iter: 761000/2048000
current iter: 762000/2048000
current iter: 763000/2048000
current iter: 764000/2048000
current iter: 765000/2048000
current iter: 766000/2048000
current iter: 767000/2048000
current iter: 768000/2048000
current iter: 769000/2048000
current iter: 770000/2048000
current iter: 771000/2048000
current iter: 772000/2048000
current iter: 773000/2048000
current iter: 774000/2048000
current iter: 775000/2048000
current iter: 776000/2048000
current iter: 777000/2048000
current iter: 778000/2048000
current iter: 779000/2048000
current iter: 780000/2048000
current iter: 781000/2048000
current iter: 782000/2048000
current iter: 783000/2048000
current iter: 784000/2048000
current iter: 785000/2048000
current iter: 786000/2048000
current iter: 787000/2048000
current iter: 788000/2048000
current iter: 789000/2048000
current iter: 790000/2048000
current iter: 791000/2048000
current iter: 792000/2048000
current iter: 793000/2048000
current iter: 794000/2048000
current iter: 795000/2048000
current iter: 796000/2048000
current iter: 797000/2048000
current iter: 798000/2048000
current iter: 799000/2048000
current iter: 800000/2048000
current iter: 801000/2048000
current iter: 802000/2048000
current iter: 803000/2048000
current iter: 804000/2048000
current iter: 805000/2048000
current iter: 806000/2048000
current iter: 807000/2048000
current iter: 808000/2048000
current iter: 809000/2048000
current iter: 810000/2048000
current iter: 811000/2048000
current iter: 812000/2048000
current iter: 813000/2048000
current iter: 814000/2048000
current iter: 815000/2048000
current iter: 816000/2048000
current iter: 817000/2048000
current iter: 818000/2048000
current iter: 819000/2048000
current iter: 820000/2048000
current iter: 821000/2048000
current iter: 822000/2048000
current iter: 823000/2048000
current iter: 824000/2048000
current iter: 825000/2048000
current iter: 826000/2048000
current iter: 827000/2048000
current iter: 828000/2048000
current iter: 829000/2048000
current iter: 830000/2048000
current iter: 831000/2048000
current iter: 832000/2048000
current iter: 833000/2048000
current iter: 834000/2048000
current iter: 835000/2048000
current iter: 836000/2048000
current iter: 837000/2048000
current iter: 838000/2048000
current iter: 839000/2048000
current iter: 840000/2048000
current iter: 841000/2048000
current iter: 842000/2048000
current iter: 843000/2048000
current iter: 844000/2048000
current iter: 845000/2048000
current iter: 846000/2048000
current iter: 847000/2048000
current iter: 848000/2048000
current iter: 849000/2048000
current iter: 850000/2048000
current iter: 851000/2048000
current iter: 852000/2048000
current iter: 853000/2048000
current iter: 854000/2048000
current iter: 855000/2048000
current iter: 856000/2048000
current iter: 857000/2048000
current iter: 858000/2048000
current iter: 859000/2048000
current iter: 860000/2048000
current iter: 861000/2048000
current iter: 862000/2048000
current iter: 863000/2048000
current iter: 864000/2048000
current iter: 865000/2048000
current iter: 866000/2048000
current iter: 867000/2048000
current iter: 868000/2048000
current iter: 869000/2048000
current iter: 870000/2048000
current iter: 871000/2048000
current iter: 872000/2048000
current iter: 873000/2048000
current iter: 874000/2048000
current iter: 875000/2048000
current iter: 876000/2048000
current iter: 877000/2048000
current iter: 878000/2048000
current iter: 879000/2048000
current iter: 880000/2048000
current iter: 881000/2048000
current iter: 882000/2048000
current iter: 883000/2048000
current iter: 884000/2048000
current iter: 885000/2048000
current iter: 886000/2048000
current iter: 887000/2048000
current iter: 888000/2048000
current iter: 889000/2048000
current iter: 890000/2048000
current iter: 891000/2048000
current iter: 892000/2048000
current iter: 893000/2048000
current iter: 894000/2048000
current iter: 895000/2048000
current iter: 896000/2048000
current iter: 897000/2048000
current iter: 898000/2048000
current iter: 899000/2048000
current iter: 900000/2048000
current iter: 901000/2048000
current iter: 902000/2048000
current iter: 903000/2048000
current iter: 904000/2048000
current iter: 905000/2048000
current iter: 906000/2048000
current iter: 907000/2048000
current iter: 908000/2048000
current iter: 909000/2048000
current iter: 910000/2048000
current iter: 911000/2048000
current iter: 912000/2048000
current iter: 913000/2048000
current iter: 914000/2048000
current iter: 915000/2048000
current iter: 916000/2048000
current iter: 917000/2048000
current iter: 918000/2048000
current iter: 919000/2048000
current iter: 920000/2048000
current iter: 921000/2048000
current iter: 922000/2048000
current iter: 923000/2048000
current iter: 924000/2048000
current iter: 925000/2048000
current iter: 926000/2048000
current iter: 927000/2048000
current iter: 928000/2048000
current iter: 929000/2048000
current iter: 930000/2048000
current iter: 931000/2048000
current iter: 932000/2048000
current iter: 933000/2048000
current iter: 934000/2048000
current iter: 935000/2048000
current iter: 936000/2048000
current iter: 937000/2048000
current iter: 938000/2048000
current iter: 939000/2048000
current iter: 940000/2048000
current iter: 941000/2048000
current iter: 942000/2048000
current iter: 943000/2048000
current iter: 944000/2048000
current iter: 945000/2048000
current iter: 946000/2048000
current iter: 947000/2048000
current iter: 948000/2048000
current iter: 949000/2048000
current iter: 950000/2048000
current iter: 951000/2048000
current iter: 952000/2048000
current iter: 953000/2048000
current iter: 954000/2048000
current iter: 955000/2048000
current iter: 956000/2048000
current iter: 957000/2048000
current iter: 958000/2048000
current iter: 959000/2048000
current iter: 960000/2048000
current iter: 961000/2048000
current iter: 962000/2048000
current iter: 963000/2048000
current iter: 964000/2048000
current iter: 965000/2048000
current iter: 966000/2048000
current iter: 967000/2048000
current iter: 968000/2048000
current iter: 969000/2048000
current iter: 970000/2048000
current iter: 971000/2048000
current iter: 972000/2048000
current iter: 973000/2048000
current iter: 974000/2048000
current iter: 975000/2048000
current iter: 976000/2048000
current iter: 977000/2048000
current iter: 978000/2048000
current iter: 979000/2048000
current iter: 980000/2048000
current iter: 981000/2048000
current iter: 982000/2048000
current iter: 983000/2048000
current iter: 984000/2048000
current iter: 985000/2048000
current iter: 986000/2048000
current iter: 987000/2048000
current iter: 988000/2048000
current iter: 989000/2048000
current iter: 990000/2048000
current iter: 991000/2048000
current iter: 992000/2048000
current iter: 993000/2048000
current iter: 994000/2048000
current iter: 995000/2048000
current iter: 996000/2048000
current iter: 997000/2048000
current iter: 998000/2048000
current iter: 999000/2048000
current iter: 1000000/2048000
current iter: 1001000/2048000
current iter: 1002000/2048000
current iter: 1003000/2048000
current iter: 1004000/2048000
current iter: 1005000/2048000
current iter: 1006000/2048000
current iter: 1007000/2048000
current iter: 1008000/2048000
current iter: 1009000/2048000
current iter: 1010000/2048000
current iter: 1011000/2048000
current iter: 1012000/2048000
current iter: 1013000/2048000
current iter: 1014000/2048000
current iter: 1015000/2048000
current iter: 1016000/2048000
current iter: 1017000/2048000
current iter: 1018000/2048000
current iter: 1019000/2048000
current iter: 1020000/2048000
current iter: 1021000/2048000
current iter: 1022000/2048000
current iter: 1023000/2048000
current iter: 1024000/2048000
current iter: 1025000/2048000
current iter: 1026000/2048000
current iter: 1027000/2048000
current iter: 1028000/2048000
current iter: 1029000/2048000
current iter: 1030000/2048000
current iter: 1031000/2048000
current iter: 1032000/2048000
current iter: 1033000/2048000
current iter: 1034000/2048000
current iter: 1035000/2048000
current iter: 1036000/2048000
current iter: 1037000/2048000
current iter: 1038000/2048000
current iter: 1039000/2048000
current iter: 1040000/2048000
current iter: 1041000/2048000
current iter: 1042000/2048000
current iter: 1043000/2048000
current iter: 1044000/2048000
current iter: 1045000/2048000
current iter: 1046000/2048000
current iter: 1047000/2048000
current iter: 1048000/2048000
current iter: 1049000/2048000
current iter: 1050000/2048000
current iter: 1051000/2048000
current iter: 1052000/2048000
current iter: 1053000/2048000
current iter: 1054000/2048000
current iter: 1055000/2048000
current iter: 1056000/2048000
current iter: 1057000/2048000
current iter: 1058000/2048000
current iter: 1059000/2048000
current iter: 1060000/2048000
current iter: 1061000/2048000
current iter: 1062000/2048000
current iter: 1063000/2048000
current iter: 1064000/2048000
current iter: 1065000/2048000
current iter: 1066000/2048000
current iter: 1067000/2048000
current iter: 1068000/2048000
current iter: 1069000/2048000
current iter: 1070000/2048000
current iter: 1071000/2048000
current iter: 1072000/2048000
current iter: 1073000/2048000
current iter: 1074000/2048000
current iter: 1075000/2048000
current iter: 1076000/2048000
current iter: 1077000/2048000
current iter: 1078000/2048000
current iter: 1079000/2048000
current iter: 1080000/2048000
current iter: 1081000/2048000
current iter: 1082000/2048000
current iter: 1083000/2048000
current iter: 1084000/2048000
current iter: 1085000/2048000
current iter: 1086000/2048000
current iter: 1087000/2048000
current iter: 1088000/2048000
current iter: 1089000/2048000
current iter: 1090000/2048000
current iter: 1091000/2048000
current iter: 1092000/2048000
current iter: 1093000/2048000
current iter: 1094000/2048000
current iter: 1095000/2048000
current iter: 1096000/2048000
current iter: 1097000/2048000
current iter: 1098000/2048000
current iter: 1099000/2048000
current iter: 1100000/2048000
current iter: 1101000/2048000
current iter: 1102000/2048000
current iter: 1103000/2048000
current iter: 1104000/2048000
current iter: 1105000/2048000
current iter: 1106000/2048000
current iter: 1107000/2048000
current iter: 1108000/2048000
current iter: 1109000/2048000
current iter: 1110000/2048000
current iter: 1111000/2048000
current iter: 1112000/2048000
current iter: 1113000/2048000
current iter: 1114000/2048000
current iter: 1115000/2048000
current iter: 1116000/2048000
current iter: 1117000/2048000
current iter: 1118000/2048000
current iter: 1119000/2048000
current iter: 1120000/2048000
current iter: 1121000/2048000
current iter: 1122000/2048000
current iter: 1123000/2048000
current iter: 1124000/2048000
current iter: 1125000/2048000
current iter: 1126000/2048000
current iter: 1127000/2048000
current iter: 1128000/2048000
current iter: 1129000/2048000
current iter: 1130000/2048000
current iter: 1131000/2048000
current iter: 1132000/2048000
current iter: 1133000/2048000
current iter: 1134000/2048000
current iter: 1135000/2048000
current iter: 1136000/2048000
current iter: 1137000/2048000
current iter: 1138000/2048000
current iter: 1139000/2048000
current iter: 1140000/2048000
current iter: 1141000/2048000
current iter: 1142000/2048000
current iter: 1143000/2048000
current iter: 1144000/2048000
current iter: 1145000/2048000
current iter: 1146000/2048000
current iter: 1147000/2048000
current iter: 1148000/2048000
current iter: 1149000/2048000
current iter: 1150000/2048000
current iter: 1151000/2048000
current iter: 1152000/2048000
current iter: 1153000/2048000
current iter: 1154000/2048000
current iter: 1155000/2048000
current iter: 1156000/2048000
current iter: 1157000/2048000
current iter: 1158000/2048000
current iter: 1159000/2048000
current iter: 1160000/2048000
current iter: 1161000/2048000
current iter: 1162000/2048000
current iter: 1163000/2048000
current iter: 1164000/2048000
current iter: 1165000/2048000
current iter: 1166000/2048000
current iter: 1167000/2048000
current iter: 1168000/2048000
current iter: 1169000/2048000
current iter: 1170000/2048000
current iter: 1171000/2048000
current iter: 1172000/2048000
current iter: 1173000/2048000
current iter: 1174000/2048000
current iter: 1175000/2048000
current iter: 1176000/2048000
current iter: 1177000/2048000
current iter: 1178000/2048000
current iter: 1179000/2048000
current iter: 1180000/2048000
current iter: 1181000/2048000
current iter: 1182000/2048000
current iter: 1183000/2048000
current iter: 1184000/2048000
current iter: 1185000/2048000
current iter: 1186000/2048000
current iter: 1187000/2048000
current iter: 1188000/2048000
current iter: 1189000/2048000
current iter: 1190000/2048000
current iter: 1191000/2048000
current iter: 1192000/2048000
current iter: 1193000/2048000
current iter: 1194000/2048000
current iter: 1195000/2048000
current iter: 1196000/2048000
current iter: 1197000/2048000
current iter: 1198000/2048000
current iter: 1199000/2048000
current iter: 1200000/2048000
current iter: 1201000/2048000
current iter: 1202000/2048000
current iter: 1203000/2048000
current iter: 1204000/2048000
current iter: 1205000/2048000
current iter: 1206000/2048000
current iter: 1207000/2048000
current iter: 1208000/2048000
current iter: 1209000/2048000
current iter: 1210000/2048000
current iter: 1211000/2048000
current iter: 1212000/2048000
current iter: 1213000/2048000
current iter: 1214000/2048000
current iter: 1215000/2048000
current iter: 1216000/2048000
current iter: 1217000/2048000
current iter: 1218000/2048000
current iter: 1219000/2048000
current iter: 1220000/2048000
current iter: 1221000/2048000
current iter: 1222000/2048000
current iter: 1223000/2048000
current iter: 1224000/2048000
current iter: 1225000/2048000
current iter: 1226000/2048000
current iter: 1227000/2048000
current iter: 1228000/2048000
current iter: 1229000/2048000
current iter: 1230000/2048000
current iter: 1231000/2048000
current iter: 1232000/2048000
current iter: 1233000/2048000
current iter: 1234000/2048000
current iter: 1235000/2048000
current iter: 1236000/2048000
current iter: 1237000/2048000
current iter: 1238000/2048000
current iter: 1239000/2048000
current iter: 1240000/2048000
current iter: 1241000/2048000
current iter: 1242000/2048000
current iter: 1243000/2048000
current iter: 1244000/2048000
current iter: 1245000/2048000
current iter: 1246000/2048000
current iter: 1247000/2048000
current iter: 1248000/2048000
current iter: 1249000/2048000
current iter: 1250000/2048000
current iter: 1251000/2048000
current iter: 1252000/2048000
current iter: 1253000/2048000
current iter: 1254000/2048000
current iter: 1255000/2048000
current iter: 1256000/2048000
current iter: 1257000/2048000
current iter: 1258000/2048000
current iter: 1259000/2048000
current iter: 1260000/2048000
current iter: 1261000/2048000
current iter: 1262000/2048000
current iter: 1263000/2048000
current iter: 1264000/2048000
current iter: 1265000/2048000
current iter: 1266000/2048000
current iter: 1267000/2048000
current iter: 1268000/2048000
current iter: 1269000/2048000
current iter: 1270000/2048000
current iter: 1271000/2048000
current iter: 1272000/2048000
current iter: 1273000/2048000
current iter: 1274000/2048000
current iter: 1275000/2048000
current iter: 1276000/2048000
current iter: 1277000/2048000
current iter: 1278000/2048000
current iter: 1279000/2048000
current iter: 1280000/2048000
current iter: 1281000/2048000
current iter: 1282000/2048000
current iter: 1283000/2048000
current iter: 1284000/2048000
current iter: 1285000/2048000
current iter: 1286000/2048000
current iter: 1287000/2048000
current iter: 1288000/2048000
current iter: 1289000/2048000
current iter: 1290000/2048000
current iter: 1291000/2048000
current iter: 1292000/2048000
current iter: 1293000/2048000
current iter: 1294000/2048000
current iter: 1295000/2048000
current iter: 1296000/2048000
current iter: 1297000/2048000
current iter: 1298000/2048000
current iter: 1299000/2048000
current iter: 1300000/2048000
current iter: 1301000/2048000
current iter: 1302000/2048000
current iter: 1303000/2048000
current iter: 1304000/2048000
current iter: 1305000/2048000
current iter: 1306000/2048000
current iter: 1307000/2048000
current iter: 1308000/2048000
current iter: 1309000/2048000
current iter: 1310000/2048000
current iter: 1311000/2048000
current iter: 1312000/2048000
current iter: 1313000/2048000
current iter: 1314000/2048000
current iter: 1315000/2048000
current iter: 1316000/2048000
current iter: 1317000/2048000
current iter: 1318000/2048000
current iter: 1319000/2048000
current iter: 1320000/2048000
current iter: 1321000/2048000
current iter: 1322000/2048000
current iter: 1323000/2048000
current iter: 1324000/2048000
current iter: 1325000/2048000
current iter: 1326000/2048000
current iter: 1327000/2048000
current iter: 1328000/2048000
current iter: 1329000/2048000
current iter: 1330000/2048000
current iter: 1331000/2048000
current iter: 1332000/2048000
current iter: 1333000/2048000
current iter: 1334000/2048000
current iter: 1335000/2048000
current iter: 1336000/2048000
current iter: 1337000/2048000
current iter: 1338000/2048000
current iter: 1339000/2048000
current iter: 1340000/2048000
current iter: 1341000/2048000
current iter: 1342000/2048000
current iter: 1343000/2048000
current iter: 1344000/2048000
current iter: 1345000/2048000
current iter: 1346000/2048000
current iter: 1347000/2048000
current iter: 1348000/2048000
current iter: 1349000/2048000
current iter: 1350000/2048000
current iter: 1351000/2048000
current iter: 1352000/2048000
current iter: 1353000/2048000
current iter: 1354000/2048000
current iter: 1355000/2048000
current iter: 1356000/2048000
current iter: 1357000/2048000
current iter: 1358000/2048000
current iter: 1359000/2048000
current iter: 1360000/2048000
current iter: 1361000/2048000
current iter: 1362000/2048000
current iter: 1363000/2048000
current iter: 1364000/2048000
current iter: 1365000/2048000
current iter: 1366000/2048000
current iter: 1367000/2048000
current iter: 1368000/2048000
current iter: 1369000/2048000
current iter: 1370000/2048000
current iter: 1371000/2048000
current iter: 1372000/2048000
current iter: 1373000/2048000
current iter: 1374000/2048000
current iter: 1375000/2048000
current iter: 1376000/2048000
current iter: 1377000/2048000
current iter: 1378000/2048000
current iter: 1379000/2048000
current iter: 1380000/2048000
current iter: 1381000/2048000
current iter: 1382000/2048000
current iter: 1383000/2048000
current iter: 1384000/2048000
current iter: 1385000/2048000
current iter: 1386000/2048000
current iter: 1387000/2048000
current iter: 1388000/2048000
current iter: 1389000/2048000
current iter: 1390000/2048000
current iter: 1391000/2048000
current iter: 1392000/2048000
current iter: 1393000/2048000
current iter: 1394000/2048000
current iter: 1395000/2048000
current iter: 1396000/2048000
current iter: 1397000/2048000
current iter: 1398000/2048000
current iter: 1399000/2048000
current iter: 1400000/2048000
current iter: 1401000/2048000
current iter: 1402000/2048000
current iter: 1403000/2048000
current iter: 1404000/2048000
current iter: 1405000/2048000
current iter: 1406000/2048000
current iter: 1407000/2048000
current iter: 1408000/2048000
current iter: 1409000/2048000
current iter: 1410000/2048000
current iter: 1411000/2048000
current iter: 1412000/2048000
current iter: 1413000/2048000
current iter: 1414000/2048000
current iter: 1415000/2048000
current iter: 1416000/2048000
current iter: 1417000/2048000
current iter: 1418000/2048000
current iter: 1419000/2048000
current iter: 1420000/2048000
current iter: 1421000/2048000
current iter: 1422000/2048000
current iter: 1423000/2048000
current iter: 1424000/2048000
current iter: 1425000/2048000
current iter: 1426000/2048000
current iter: 1427000/2048000
current iter: 1428000/2048000
current iter: 1429000/2048000
current iter: 1430000/2048000
current iter: 1431000/2048000
current iter: 1432000/2048000
current iter: 1433000/2048000
current iter: 1434000/2048000
current iter: 1435000/2048000
current iter: 1436000/2048000
current iter: 1437000/2048000
current iter: 1438000/2048000
current iter: 1439000/2048000
current iter: 1440000/2048000
current iter: 1441000/2048000
current iter: 1442000/2048000
current iter: 1443000/2048000
current iter: 1444000/2048000
current iter: 1445000/2048000
current iter: 1446000/2048000
current iter: 1447000/2048000
current iter: 1448000/2048000
current iter: 1449000/2048000
current iter: 1450000/2048000
current iter: 1451000/2048000
current iter: 1452000/2048000
current iter: 1453000/2048000
current iter: 1454000/2048000
current iter: 1455000/2048000
current iter: 1456000/2048000
current iter: 1457000/2048000
current iter: 1458000/2048000
current iter: 1459000/2048000
current iter: 1460000/2048000
current iter: 1461000/2048000
current iter: 1462000/2048000
current iter: 1463000/2048000
current iter: 1464000/2048000
current iter: 1465000/2048000
current iter: 1466000/2048000
current iter: 1467000/2048000
current iter: 1468000/2048000
current iter: 1469000/2048000
current iter: 1470000/2048000
current iter: 1471000/2048000
current iter: 1472000/2048000
current iter: 1473000/2048000
current iter: 1474000/2048000
current iter: 1475000/2048000
current iter: 1476000/2048000
current iter: 1477000/2048000
current iter: 1478000/2048000
current iter: 1479000/2048000
current iter: 1480000/2048000
current iter: 1481000/2048000
current iter: 1482000/2048000
current iter: 1483000/2048000
current iter: 1484000/2048000
current iter: 1485000/2048000
current iter: 1486000/2048000
current iter: 1487000/2048000
current iter: 1488000/2048000
current iter: 1489000/2048000
current iter: 1490000/2048000
current iter: 1491000/2048000
current iter: 1492000/2048000
current iter: 1493000/2048000
current iter: 1494000/2048000
current iter: 1495000/2048000
current iter: 1496000/2048000
current iter: 1497000/2048000
current iter: 1498000/2048000
current iter: 1499000/2048000
current iter: 1500000/2048000
current iter: 1501000/2048000
current iter: 1502000/2048000
current iter: 1503000/2048000
current iter: 1504000/2048000
current iter: 1505000/2048000
current iter: 1506000/2048000
current iter: 1507000/2048000
current iter: 1508000/2048000
current iter: 1509000/2048000
current iter: 1510000/2048000
current iter: 1511000/2048000
current iter: 1512000/2048000
current iter: 1513000/2048000
current iter: 1514000/2048000
current iter: 1515000/2048000
current iter: 1516000/2048000
current iter: 1517000/2048000
current iter: 1518000/2048000
current iter: 1519000/2048000
current iter: 1520000/2048000
current iter: 1521000/2048000
current iter: 1522000/2048000
current iter: 1523000/2048000
current iter: 1524000/2048000
current iter: 1525000/2048000
current iter: 1526000/2048000
current iter: 1527000/2048000
current iter: 1528000/2048000
current iter: 1529000/2048000
current iter: 1530000/2048000
current iter: 1531000/2048000
current iter: 1532000/2048000
current iter: 1533000/2048000
current iter: 1534000/2048000
current iter: 1535000/2048000
current iter: 1536000/2048000
current iter: 1537000/2048000
current iter: 1538000/2048000
current iter: 1539000/2048000
current iter: 1540000/2048000
current iter: 1541000/2048000
current iter: 1542000/2048000
current iter: 1543000/2048000
current iter: 1544000/2048000
current iter: 1545000/2048000
current iter: 1546000/2048000
current iter: 1547000/2048000
current iter: 1548000/2048000
current iter: 1549000/2048000
current iter: 1550000/2048000
current iter: 1551000/2048000
current iter: 1552000/2048000
current iter: 1553000/2048000
current iter: 1554000/2048000
current iter: 1555000/2048000
current iter: 1556000/2048000
current iter: 1557000/2048000
current iter: 1558000/2048000
current iter: 1559000/2048000
current iter: 1560000/2048000
current iter: 1561000/2048000
current iter: 1562000/2048000
current iter: 1563000/2048000
current iter: 1564000/2048000
current iter: 1565000/2048000
current iter: 1566000/2048000
current iter: 1567000/2048000
current iter: 1568000/2048000
current iter: 1569000/2048000
current iter: 1570000/2048000
current iter: 1571000/2048000
current iter: 1572000/2048000
current iter: 1573000/2048000
current iter: 1574000/2048000
current iter: 1575000/2048000
current iter: 1576000/2048000
current iter: 1577000/2048000
current iter: 1578000/2048000
current iter: 1579000/2048000
current iter: 1580000/2048000
current iter: 1581000/2048000
current iter: 1582000/2048000
current iter: 1583000/2048000
current iter: 1584000/2048000
current iter: 1585000/2048000
current iter: 1586000/2048000
current iter: 1587000/2048000
current iter: 1588000/2048000
current iter: 1589000/2048000
current iter: 1590000/2048000
current iter: 1591000/2048000
current iter: 1592000/2048000
current iter: 1593000/2048000
current iter: 1594000/2048000
current iter: 1595000/2048000
current iter: 1596000/2048000
current iter: 1597000/2048000
current iter: 1598000/2048000
current iter: 1599000/2048000
current iter: 1600000/2048000
current iter: 1601000/2048000
current iter: 1602000/2048000
current iter: 1603000/2048000
current iter: 1604000/2048000
current iter: 1605000/2048000
current iter: 1606000/2048000
current iter: 1607000/2048000
current iter: 1608000/2048000
current iter: 1609000/2048000
current iter: 1610000/2048000
current iter: 1611000/2048000
current iter: 1612000/2048000
current iter: 1613000/2048000
current iter: 1614000/2048000
current iter: 1615000/2048000
current iter: 1616000/2048000
current iter: 1617000/2048000
current iter: 1618000/2048000
current iter: 1619000/2048000
current iter: 1620000/2048000
current iter: 1621000/2048000
current iter: 1622000/2048000
current iter: 1623000/2048000
current iter: 1624000/2048000
current iter: 1625000/2048000
current iter: 1626000/2048000
current iter: 1627000/2048000
current iter: 1628000/2048000
current iter: 1629000/2048000
current iter: 1630000/2048000
current iter: 1631000/2048000
current iter: 1632000/2048000
current iter: 1633000/2048000
current iter: 1634000/2048000
current iter: 1635000/2048000
current iter: 1636000/2048000
current iter: 1637000/2048000
current iter: 1638000/2048000
current iter: 1639000/2048000
current iter: 1640000/2048000
current iter: 1641000/2048000
current iter: 1642000/2048000
current iter: 1643000/2048000
current iter: 1644000/2048000
current iter: 1645000/2048000
current iter: 1646000/2048000
current iter: 1647000/2048000
current iter: 1648000/2048000
current iter: 1649000/2048000
current iter: 1650000/2048000
current iter: 1651000/2048000
current iter: 1652000/2048000
current iter: 1653000/2048000
current iter: 1654000/2048000
current iter: 1655000/2048000
current iter: 1656000/2048000
current iter: 1657000/2048000
current iter: 1658000/2048000
current iter: 1659000/2048000
current iter: 1660000/2048000
current iter: 1661000/2048000
current iter: 1662000/2048000
current iter: 1663000/2048000
current iter: 1664000/2048000
current iter: 1665000/2048000
current iter: 1666000/2048000
current iter: 1667000/2048000
current iter: 1668000/2048000
current iter: 1669000/2048000
current iter: 1670000/2048000
current iter: 1671000/2048000
current iter: 1672000/2048000
current iter: 1673000/2048000
current iter: 1674000/2048000
current iter: 1675000/2048000
current iter: 1676000/2048000
current iter: 1677000/2048000
current iter: 1678000/2048000
current iter: 1679000/2048000
current iter: 1680000/2048000
current iter: 1681000/2048000
current iter: 1682000/2048000
current iter: 1683000/2048000
current iter: 1684000/2048000
current iter: 1685000/2048000
current iter: 1686000/2048000
current iter: 1687000/2048000
current iter: 1688000/2048000
current iter: 1689000/2048000
current iter: 1690000/2048000
current iter: 1691000/2048000
current iter: 1692000/2048000
current iter: 1693000/2048000
current iter: 1694000/2048000
current iter: 1695000/2048000
current iter: 1696000/2048000
current iter: 1697000/2048000
current iter: 1698000/2048000
current iter: 1699000/2048000
current iter: 1700000/2048000
current iter: 1701000/2048000
current iter: 1702000/2048000
current iter: 1703000/2048000
current iter: 1704000/2048000
current iter: 1705000/2048000
current iter: 1706000/2048000
current iter: 1707000/2048000
current iter: 1708000/2048000
current iter: 1709000/2048000
current iter: 1710000/2048000
current iter: 1711000/2048000
current iter: 1712000/2048000
current iter: 1713000/2048000
current iter: 1714000/2048000
current iter: 1715000/2048000
current iter: 1716000/2048000
current iter: 1717000/2048000
current iter: 1718000/2048000
current iter: 1719000/2048000
current iter: 1720000/2048000
current iter: 1721000/2048000
current iter: 1722000/2048000
current iter: 1723000/2048000
current iter: 1724000/2048000
current iter: 1725000/2048000
current iter: 1726000/2048000
current iter: 1727000/2048000
current iter: 1728000/2048000
current iter: 1729000/2048000
current iter: 1730000/2048000
current iter: 1731000/2048000
current iter: 1732000/2048000
current iter: 1733000/2048000
current iter: 1734000/2048000
current iter: 1735000/2048000
current iter: 1736000/2048000
current iter: 1737000/2048000
current iter: 1738000/2048000
current iter: 1739000/2048000
current iter: 1740000/2048000
current iter: 1741000/2048000
current iter: 1742000/2048000
current iter: 1743000/2048000
current iter: 1744000/2048000
current iter: 1745000/2048000
current iter: 1746000/2048000
current iter: 1747000/2048000
current iter: 1748000/2048000
current iter: 1749000/2048000
current iter: 1750000/2048000
current iter: 1751000/2048000
current iter: 1752000/2048000
current iter: 1753000/2048000
current iter: 1754000/2048000
current iter: 1755000/2048000
current iter: 1756000/2048000
current iter: 1757000/2048000
current iter: 1758000/2048000
current iter: 1759000/2048000
current iter: 1760000/2048000
current iter: 1761000/2048000
current iter: 1762000/2048000
current iter: 1763000/2048000
current iter: 1764000/2048000
current iter: 1765000/2048000
current iter: 1766000/2048000
current iter: 1767000/2048000
current iter: 1768000/2048000
current iter: 1769000/2048000
current iter: 1770000/2048000
current iter: 1771000/2048000
current iter: 1772000/2048000
current iter: 1773000/2048000
current iter: 1774000/2048000
current iter: 1775000/2048000
current iter: 1776000/2048000
current iter: 1777000/2048000
current iter: 1778000/2048000
current iter: 1779000/2048000
current iter: 1780000/2048000
current iter: 1781000/2048000
current iter: 1782000/2048000
current iter: 1783000/2048000
current iter: 1784000/2048000
current iter: 1785000/2048000
current iter: 1786000/2048000
current iter: 1787000/2048000
current iter: 1788000/2048000
current iter: 1789000/2048000
current iter: 1790000/2048000
current iter: 1791000/2048000
current iter: 1792000/2048000
current iter: 1793000/2048000
current iter: 1794000/2048000
current iter: 1795000/2048000
current iter: 1796000/2048000
current iter: 1797000/2048000
current iter: 1798000/2048000
current iter: 1799000/2048000
current iter: 1800000/2048000
current iter: 1801000/2048000
current iter: 1802000/2048000
current iter: 1803000/2048000
current iter: 1804000/2048000
current iter: 1805000/2048000
current iter: 1806000/2048000
current iter: 1807000/2048000
current iter: 1808000/2048000
current iter: 1809000/2048000
current iter: 1810000/2048000
current iter: 1811000/2048000
current iter: 1812000/2048000
current iter: 1813000/2048000
current iter: 1814000/2048000
current iter: 1815000/2048000
current iter: 1816000/2048000
current iter: 1817000/2048000
current iter: 1818000/2048000
current iter: 1819000/2048000
current iter: 1820000/2048000
current iter: 1821000/2048000
current iter: 1822000/2048000
current iter: 1823000/2048000
current iter: 1824000/2048000
current iter: 1825000/2048000
current iter: 1826000/2048000
current iter: 1827000/2048000
current iter: 1828000/2048000
current iter: 1829000/2048000
current iter: 1830000/2048000
current iter: 1831000/2048000
current iter: 1832000/2048000
current iter: 1833000/2048000
current iter: 1834000/2048000
current iter: 1835000/2048000
current iter: 1836000/2048000
current iter: 1837000/2048000
current iter: 1838000/2048000
current iter: 1839000/2048000
current iter: 1840000/2048000
current iter: 1841000/2048000
current iter: 1842000/2048000
current iter: 1843000/2048000
current iter: 1844000/2048000
current iter: 1845000/2048000
current iter: 1846000/2048000
current iter: 1847000/2048000
current iter: 1848000/2048000
current iter: 1849000/2048000
current iter: 1850000/2048000
current iter: 1851000/2048000
current iter: 1852000/2048000
current iter: 1853000/2048000
current iter: 1854000/2048000
current iter: 1855000/2048000
current iter: 1856000/2048000
current iter: 1857000/2048000
current iter: 1858000/2048000
current iter: 1859000/2048000
current iter: 1860000/2048000
current iter: 1861000/2048000
current iter: 1862000/2048000
current iter: 1863000/2048000
current iter: 1864000/2048000
current iter: 1865000/2048000
current iter: 1866000/2048000
current iter: 1867000/2048000
current iter: 1868000/2048000
current iter: 1869000/2048000
current iter: 1870000/2048000
current iter: 1871000/2048000
current iter: 1872000/2048000
current iter: 1873000/2048000
current iter: 1874000/2048000
current iter: 1875000/2048000
current iter: 1876000/2048000
current iter: 1877000/2048000
current iter: 1878000/2048000
current iter: 1879000/2048000
current iter: 1880000/2048000
current iter: 1881000/2048000
current iter: 1882000/2048000
current iter: 1883000/2048000
current iter: 1884000/2048000
current iter: 1885000/2048000
current iter: 1886000/2048000
current iter: 1887000/2048000
current iter: 1888000/2048000
current iter: 1889000/2048000
current iter: 1890000/2048000
current iter: 1891000/2048000
current iter: 1892000/2048000
current iter: 1893000/2048000
current iter: 1894000/2048000
current iter: 1895000/2048000
current iter: 1896000/2048000
current iter: 1897000/2048000
current iter: 1898000/2048000
current iter: 1899000/2048000
current iter: 1900000/2048000
current iter: 1901000/2048000
current iter: 1902000/2048000
current iter: 1903000/2048000
current iter: 1904000/2048000
current iter: 1905000/2048000
current iter: 1906000/2048000
current iter: 1907000/2048000
current iter: 1908000/2048000
current iter: 1909000/2048000
current iter: 1910000/2048000
current iter: 1911000/2048000
current iter: 1912000/2048000
current iter: 1913000/2048000
current iter: 1914000/2048000
current iter: 1915000/2048000
current iter: 1916000/2048000
current iter: 1917000/2048000
current iter: 1918000/2048000
current iter: 1919000/2048000
current iter: 1920000/2048000
current iter: 1921000/2048000
current iter: 1922000/2048000
current iter: 1923000/2048000
current iter: 1924000/2048000
current iter: 1925000/2048000
current iter: 1926000/2048000
current iter: 1927000/2048000
current iter: 1928000/2048000
current iter: 1929000/2048000
current iter: 1930000/2048000
current iter: 1931000/2048000
current iter: 1932000/2048000
current iter: 1933000/2048000
current iter: 1934000/2048000
current iter: 1935000/2048000
current iter: 1936000/2048000
current iter: 1937000/2048000
current iter: 1938000/2048000
current iter: 1939000/2048000
current iter: 1940000/2048000
current iter: 1941000/2048000
current iter: 1942000/2048000
current iter: 1943000/2048000
current iter: 1944000/2048000
current iter: 1945000/2048000
current iter: 1946000/2048000
current iter: 1947000/2048000
current iter: 1948000/2048000
current iter: 1949000/2048000
current iter: 1950000/2048000
current iter: 1951000/2048000
current iter: 1952000/2048000
current iter: 1953000/2048000
current iter: 1954000/2048000
current iter: 1955000/2048000
current iter: 1956000/2048000
current iter: 1957000/2048000
current iter: 1958000/2048000
current iter: 1959000/2048000
current iter: 1960000/2048000
current iter: 1961000/2048000
current iter: 1962000/2048000
current iter: 1963000/2048000
current iter: 1964000/2048000
current iter: 1965000/2048000
current iter: 1966000/2048000
current iter: 1967000/2048000
current iter: 1968000/2048000
current iter: 1969000/2048000
current iter: 1970000/2048000
current iter: 1971000/2048000
current iter: 1972000/2048000
current iter: 1973000/2048000
current iter: 1974000/2048000
current iter: 1975000/2048000
current iter: 1976000/2048000
current iter: 1977000/2048000
current iter: 1978000/2048000
current iter: 1979000/2048000
current iter: 1980000/2048000
current iter: 1981000/2048000
current iter: 1982000/2048000
current iter: 1983000/2048000
current iter: 1984000/2048000
current iter: 1985000/2048000
current iter: 1986000/2048000
current iter: 1987000/2048000
current iter: 1988000/2048000
current iter: 1989000/2048000
current iter: 1990000/2048000
current iter: 1991000/2048000
current iter: 1992000/2048000
current iter: 1993000/2048000
current iter: 1994000/2048000
current iter: 1995000/2048000
current iter: 1996000/2048000
current iter: 1997000/2048000
current iter: 1998000/2048000
current iter: 1999000/2048000
current iter: 2000000/2048000
current iter: 2001000/2048000
current iter: 2002000/2048000
current iter: 2003000/2048000
current iter: 2004000/2048000
current iter: 2005000/2048000
current iter: 2006000/2048000
current iter: 2007000/2048000
current iter: 2008000/2048000
current iter: 2009000/2048000
current iter: 2010000/2048000
current iter: 2011000/2048000
current iter: 2012000/2048000
current iter: 2013000/2048000
current iter: 2014000/2048000
current iter: 2015000/2048000
current iter: 2016000/2048000
current iter: 2017000/2048000
current iter: 2018000/2048000
current iter: 2019000/2048000
current iter: 2020000/2048000
current iter: 2021000/2048000
current iter: 2022000/2048000
current iter: 2023000/2048000
current iter: 2024000/2048000
current iter: 2025000/2048000
current iter: 2026000/2048000
current iter: 2027000/2048000
current iter: 2028000/2048000
current iter: 2029000/2048000
current iter: 2030000/2048000
current iter: 2031000/2048000
current iter: 2032000/2048000
current iter: 2033000/2048000
current iter: 2034000/2048000
current iter: 2035000/2048000
current iter: 2036000/2048000
current iter: 2037000/2048000
current iter: 2038000/2048000
current iter: 2039000/2048000
current iter: 2040000/2048000
current iter: 2041000/2048000
current iter: 2042000/2048000
current iter: 2043000/2048000
current iter: 2044000/2048000
current iter: 2045000/2048000
current iter: 2046000/2048000
current iter: 2047000/2048000

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{SAMPLE\PYZus{}FILE\PYZus{}PATH} \PY{o}{=} \PY{n}{OUTPUT\PYZus{}PATH} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/12\PYZhy{}09\PYZus{}21\PYZhy{}52\PYZhy{}24\PYZus{}7D4325.sample}\PY{l+s+s2}{\PYZdq{}}
         
         \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{SAMPLE\PYZus{}FILE\PYZus{}PATH}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{wb}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)} \PY{k}{as} \PY{n}{file}\PY{p}{:}
             \PY{n}{pickle}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{n}{output}\PY{p}{,} \PY{n}{file}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{score} \PY{o}{=} \PY{n}{haydn\PYZus{}dataset}\PY{o}{.}\PY{n}{matrix\PYZus{}to\PYZus{}score}\PY{p}{(}\PY{n}{output}\PY{p}{)}
         
         \PY{n}{SAVING} \PY{o}{=} \PY{k+kc}{True}
         
         \PY{k}{if} \PY{n}{SAVING}\PY{p}{:}
             \PY{n}{file\PYZus{}name} \PY{o}{=} \PY{n}{RUN\PYZus{}ID} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZus{}}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{RUN\PYZus{}TIME} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.pgz}\PY{l+s+s2}{\PYZdq{}}
             \PY{n}{mkdir}\PY{p}{(}\PY{n}{SAMPLE\PYZus{}PATH}\PY{p}{)}
             \PY{n}{output\PYZus{}path} \PY{o}{=} \PY{n}{SAMPLE\PYZus{}PATH} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{file\PYZus{}name}
             \PY{n}{converter}\PY{o}{.}\PY{n}{freeze}\PY{p}{(}\PY{n}{score}\PY{p}{,} \PY{n}{fp}\PY{o}{=}\PY{n}{output\PYZus{}path}\PY{p}{)}
             
         \PY{c+c1}{\PYZsh{} converter.thaw(output\PYZus{}path)}
\end{Verbatim}



    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
